---
layout: post
title: "2.3 The Gaussian Distribution(3)"
modified:
categories: articles
excerpt:
tags: [PRML]
image:
  feature:
date: 2022-03-05T015:01:50-04:00
---

이 글은 [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.
{: .notice}

### 가우시안 분포의 최대 가능도


책에는 없지만, 앞의 내용을 복습할 겸 다변량 가우시안 분포부터 자세히 설명할 예정이다.
- 먼저, 다변량 가우시안 분포부터 살펴보자.
- 데이터 집합  \\(X = ( x_{1},\cdots,x_{D} )^{T}\\)이 주어졌으며, 관측값 \\( {X_{d}}\\)들이 다변량 가우시안 분포로 부터 독립적으로 추출되었다고 가정해 보자.
- 이때 원 분산의 매개변수들을 최대 가능도 방법을 이용하여 추정할 수 있다.

$$ X = \begin{pmatrix}
X_{1} \\
X_{2} \\
\vdots  \\
X_{d}
\end{pmatrix} \sim MVN(\mu,\Sigma)$$

* 여기서 \\(\mu \\)는 d x 1 벡터, \\( \Sigma\\)는 d x d행렬이 된다.

- 이러한 다변량 가우시안 분포의 pdf는 다음과 같이 쓸 수 있다.
\\[f(x_{i};\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}}\left| \Sigma \right|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \\] 
- 만약 우리가 n개의 샘플을 가지고 있다고 가정 할 경우에 이 것들의 가능도 함수는 다음과 같이 각 샘플들을 pdf에 집어넣은 것들을 곱한 꼴로 나타낼 수 있다.
\\[ L(x;\mu,\Sigma) = \prod_{i = 1}^{n}f(x_{i})= \prod_{i = 1}^{n}\left [ \frac{1}{(2\pi)^{\frac{d}{2}}\left|\Sigma \right|^{\frac{1}{2}}} exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \right ] \\]
- 양변에 로그 함수를 취하여 로그 가능도 함수를 만들면 다음과 같다.

$$ lnp(x \mid \mu,\Sigma) = \sum_{i=1}^{n}(-\frac{d}{2}log\pi - \frac{1}{2}log\left| \Sigma \right|-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x-\mu)) $$

$$ \qquad \qquad  \qquad \qquad \qquad \quad = -\frac{ND}{2}ln(2\pi) - \frac{N}{2}ln\left| \Sigma \right| -\frac{1}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{T}\Sigma^{-1}(x_{n}-\mu) \qquad 식(2.118) $$

최대 가능도(MLE)는 위의 로그 가능도 함수를 최대로 만드는 모수 \\(\mu \\) 와 \\(\Sigma \\)의 값을 구하는 것이다.
* 따라서 첫번째 항은 두 개의 모수와 관계 없으므로 생략할 수 있다.
* 그러면 이제, MLE를 구해보자
* 구하기에 앞서 벡터 편미분에 대한 기본적인 개념을 알아야한다.
    * 모수 \\( \mu \\)와 \\(\Sigma \\)로 각각 편미분하기 위하여 간단하게만 살펴보자.
    * a 와 b가 p x 1 벡터이고, A가 p x p 상수로 이루어진 행렬일 때, 다음의 미분식이 성립한다.
     
     1. \\( \frac{\partial a^{T}b}{\partial b} =a  \\)
     2. \\( \frac{\partial a^{T}b}{\partial a} =b \\)
     3. \\( \frac{\partial b^{T}Ab}{\partial B} =(A + A^{T})b \\)
     4. \\( \frac{\partial log \mid A \mid}{\partial A} =(A^{-1})^{T} \\)
     5. \\( \frac{\partial tr(AB)}{\partial A} = B^{T} \\)
    
이제, \\( \mu \\)벡터에 대하여 로그 가능도 함수를 편미분 해보자.
* \\( \mu\\)를 포함하고 있는 항만 살펴보면 된다.
* 이 항은 다음과 같은 함수 f의 합의 꼴로 볼 수 있다.
\\[ f(\mu) = (x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu) \\]


<figure>
    <a href="/PRML/43.jpeg" alt="image"><img src="/PRML/43.jpeg" alt="image"></a>
</figure>

* 마지막으로 위에서 구한 로그 가능도 함수의 편미분 벡터를 영벡터로 만드는 벡터 \\( \mu \\)의 값을 찾자.

<figure>
    <a href="/PRML/44.jpeg" alt="image"><img src="/PRML/44.jpeg" alt="image"></a>
</figure>

* 정리하면 평균에 대한 최대 가능도 추정값의 해는 다음과 같다.
\\[ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n} \qquad 식(2.121)\\]

이제, 로그 가능도 함수를 \\( \Sigma \\)행렬로 편미분해보자.

* 마찬가지로 \\( \Sigma \\)를 로그가능도에서 찾아보면 다음과 같다.
\\[ lnp(X \mid \mu,\Sigma) \propto -\frac{n}{2}log \mid \Sigma \mid -\frac{1}{2} \sum_{i=1}^{n}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu) \\]

* 행렬 미적분에서 2차 형식(Quadratic form)으로 나타내어진 행렬에 대해 미분하는 방법이 역랭렬로 나타내어진 것을 미분하는 것보다 훨씬 쉽기 때문에, \\( \Sigma^{-1} \\)로 미분하여 진행.

<figure>
    <a href="/PRML/45.jpeg" alt="image"><img src="/PRML/45.jpeg" alt="image"></a>
</figure>

* 그러면 이제 행렬 A로 미분해보자

<figure>
    <a href="/PRML/46.jpeg" alt="image"><img src="/PRML/46.jpeg" alt="image"></a>
</figure>

* 정리하면 다음과 같다.
\\[ \Sigma_{ML} = \frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})(x_{n}-\mu_{ML})^{T} \qquad 식(2.122)\\]

### 순차 추정

- 순차 추정 방식은 데이터 포인트들을 하나씩 처리하고 버리는 방식이다,
- 이는 다음과 같은 상황에서 유용하게 사용할 수 있다.
    - 관찰 데이터 집합이 매우 커서 한번에 계산이 불가능할 때
    - 실시간 처리를 해야하는 온라인 적용 사례

- MLE로 얻어진 \\( \mu_{ML}\\)식을 업데이트 되는 방식으로 바꿔보자.
    - \\( \mu_{ML}\\)에서 마지막 데이터 포인트 \\(X_{N}\\)따로 빼내어 보자
    \\[ \mu_{ML}^{(N)} = \frac{1}{N}\sum_{n=1}^{N}X_{n} = \frac{1}{N}X_{N} +\frac{1}{N}\sum_{n=1}^{ N-1}X_{n} = \frac{1}{N}X_{N} + \frac{N-1}{N}\mu_{ML}^{(N-1)} = \mu_{ML}^{(N-1)} + \frac{1}{N}(X_{N} - \mu_{ML}^(N-1)) \qquad  식(2.126)\\]
    
    - N-1개의 데이터로부터 추정된 \\( \mu_{ML}^{(N-1)}\\)와 N번째 관측된 데이터를 이용하여 \\( \mu_{ML}^{(N)}\\)을 구한다.
    - N의 값이 증가할수록 새로 관측되는 데이터의 기여도가 점점 작아지게 된다.
    - 한번에 계산을 처리하는 배치 방식으로부터 식을 유도했기 때문에 실제 결과는 동일하게 된다.
    - 이런 방식은 매우 유용하지만, 배치 방식의 식에서 업데이트 방식의 식을 항상 유도할 수 있는 것은 아니다.
    - 따라서 순차적인 학습에 대한 더 일반적인 방식인 로빈스 몬로(Robbins & Monro) 알고리즘을 알아보자
    
#### Robins & Monro 알고리즘

- 두 개의 확률 변수 z와 \\(\theta \\)가 주어졌을 때 이 분포의 결합 분포는 \\( p(z,\theta) \\)이다.
- \\(\theta \\)가 주어졌을 때의 z의 조건부 기댓값으로 결정 함수 \\( f(\theta) \\)를 정의할 수 있다.
\\[ f(\theta) = E[z \mid \theta ] = \int zp (z \mid \theta ) dz \\]
- 이러한 함수를 회귀(Regression) 함수라고 한다.
- 우리의 목표는 \\( f(\theta^{\ast}) = 0 \\) 을 만족하는 \\( \theta ^{\ast}\\)을 찾는 것이다.

<figure>
    <a href="/PRML/47.png" alt="image"><img src="/PRML/47.png" alt="image"></a>
</figure>

- 여기서 관찰 데이터는 파란색 점인 z이다/
- z와 \\( \theta \\)의 관측 데이터가 많다면 함수를 직접 모델하여 근을 추정할 수 있다.
- 하지만,앞서 설명 처럼 z값을 한번에 하나씩 관측하고 그에 대한 \\( \theta ^{\ast}\\)의 순차 추정값을 매번 업데이트 해야하는 상황을 가정해보자.
- 이를 위해 몇 가지 가정을 한다.
    - z의 조건부 분산은 유한한 값을 가진다.
    \\[ E[(z - f)^{2} \mid \theta ] < \infty \qquad 식(2.128) \\]
    - 임의의 \\( \theta \\) 에 대해 \\( \theta > \theta ^{\ast}\\)인 경우 \\( f(\theta) > 0 \\)
    - 임의의 \\( \theta \\) 에 대해 \\( \theta < \theta ^{\ast}\\)인 경우 \\( f(\theta) < 0 \\)
    - 이는 위의 그림을 보면 쉽게 이해할 수 있다.
- 이러한 상황에서 Ronins & Monro에 따르면 \\( \theta ^{\ast}\\) 를 다음과 같이 순차적으로 구할 수 있다.
\\[ \theta^{(N)} = \theta^{(N-1)} - a_{N-1}z(\theta^{(N-1)}) \qquad 식(2.129) \\]
- 이 식을 이용하여 순차적으로 들어오는 데이터를 넣어 \\( \theta ^{\ast}\\) 값을 추정할 수 있다.
- 여기서 \\( z(\theta (N)) \\)은 N번째의 \\(\theta \\)값이 들어왔을 때의 출력값을 의미한다.
- 계수 \\(a_{n}\\)은 연속적인 양의 실수이며 다음과 같은 조건을 만족한다.
\\[ \displaystyle \lim_{ N \to \infty} a_{N} = 0 \qquad 식(2.130)  \\] 
\\[ \sum_{N=1}^{\infty} a_{N} = \infty \qquad 식(2.131)\\]
\\[ \sum_{N=1}^{\infty} a_{N}^{2} < \infty \qquad 식(2.132)\\]
- 위의 식이 의미하는 바는 다음과 같다.
    - 위의 세 식은 추정이 반복될수록 (N이 커질수록) 다음의 특성을 갖는다.
        - 식(2.130) : \\( \theta \\)가 특정 값에 수렴
        - 식(2.131) : \\( f(\theta) \\)를 찾기도 전에 임의 값에 수렴하지 않도록 누적된 오차가 제한된 분산을 가지게 된다.
        - 식(2.132) : 축적되는 노이즈가 유한다하는 가정에 의해 수렴된 상태를 깨지 않는다.
이제 가우시안 모델의 MLE를 구하는 과정에 적용해보자
- 가우시안 모델에서 \\( \theta_{ML}\\)의 값은 음의 로그 가능도 함수를 한번 미분하여 얻을 수 있다.

$$ \frac{\partial}{\partial\theta}\left.\left\{-\frac{1}{N}\sum_{n=1}^{N}\ln p(x_n|\theta)\right\}\right|_{\theta_{MLE}}=0 \qquad 식(2.133) $$

- 미분과 합산을 교환하고, \\( \infty  \to \infty  \\)을 취하면 다음을 얻게 된다.

$$-\lim_{n\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}\frac{\partial}{\partial\theta}\ln p(x_n|\theta) = E_x\left[-\frac{\partial}{\partial\theta}\ln p(x|\theta)\right] \qquad 식 (2.134)$$

- Robbins & Monro 식과 동일해졌음을 알 수 있다.
- 이제 Robbins & Monro 알고리즘을 적용해보자.

$$\theta^{(N)} = \theta^{(N-1)} - a_{N-1}\frac{\partial}{\partial\theta^{(N-1)}}\left[-\ln p(x_N|\theta^{(N-1)})\right] \qquad 식(2.135)$$

- 우리는 \\( \ln p(x\_n\|\theta) \\) 는 \\( \ln p(x\_n\|\theta)=\frac{1}{2\sigma^2}(x_n-\theta)^2 \\) 임을 앞서 배웠다.
- 이를 식에 대입해보자.

$$z=\frac{\partial}{\partial\mu_{ML}}[-\ln p(x|\mu_{ML}, \sigma^2)]=-\frac{1}{\sigma^2}(x-\mu_{ML}) \qquad 식(2.136)$$

- 우리에게 필요한 식은 \\( E[z\|\theta] \\) 이다.
- 따라서 모든 \\( x \\) 에 대해 평균 공식을 대입하면 \\( -\frac{1}{\sigma^2}(\mu-\mu_{ML}) \\) 을 얻는다.
    - 이는 \\( \frac{1}{N}\sum x_i=\mu \\) 이기 때문이다.

- 사실 \\( E[z\|\theta] \\) 는 회귀 식이므로 \\( z \\) 에 대해 정규분포로 표현이 가능하다.
    - 이 때 이 정규 분포의 평균 값은 실제 회귀 함수가 되며, 따라서 \\( -\frac{1}{\sigma^2}(\mu-\mu_{ML}) \\)
- 정규 분포의 평균 값에 대한 연결선은 직선의 함수가 된다. 그림에서는 다음과 같이 붉은 선으료 표현된다.

<figure>
    <a href="/PRML/48.png" alt="image"><img src="/PRML/48.png" alt="image"></a>
</figure>

- 이 때 이 값이 0 을 만족하는 값이 우리가 찾고자 하는 \\( \mu_{ML} \\) 의 해가 된다.
- 여기서 \\( a\_N=\sigma^2/N \\) 으로 놓고 식을 전개하면 배치 방식의 전개와 동일한 식을 얻을 수 있다.

### 가우시안 분포에서의 베이지안 추론
