<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">MinPy</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml" />
<link rel="alternate" type="text/html" href="http://localhost:4000" />
<updated>2022-05-26T15:09:35+09:00</updated>
<id>http://localhost:4000/</id>
<author>
  <name>Cho Min Hee</name>
  <uri>http://localhost:4000/</uri>
  <email>aezjk56@gmail.com</email>
</author>


<entry>
  <title type="html"><![CDATA[2.3 The Gaussian Distribution(4)]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-2.3-The-Gaussian-Distribution(4)/" />
  <id>http://localhost:4000/articles/[PRML]2.3 The Gaussian Distribution(4)</id>
  <published>2022-03-07T04:01:50+09:00</published>
  <updated>2022-03-07T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/8.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/8.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-2.3-The-Gaussian-Distribution(4)/&quot;&gt;2.3 The Gaussian Distribution(4)&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 07, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[적률과 적률 생성 함수]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/blog/%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA-%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF-%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC-%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE/" />
  <id>http://localhost:4000/blog/적률과 적률 생성 함수</id>
  <published>2022-03-06T07:08:50+09:00</published>
  <updated>2022-03-06T07:08:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;적률(moment)을 이해하기 위해, 물리학에서 말하는 moment를 먼저 살펴보자&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;물리학에서 말하는 ‘moment’는 어떤 물리량과 거리의 곱을 말한다.&lt;/li&gt;
  &lt;li&gt;여기서 거리는 ‘어떤 기준점에서 물리량 사이의 거리’를 뜻한다.&lt;/li&gt;
  &lt;li&gt;이 ‘거리’는 물리량의 분포를 보여준다.&lt;/li&gt;
  &lt;li&gt;즉, 통계학에서의 적률(moment)은 이 ‘분포’와 관련되어 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그렇다면 통계학에서 적률의 정의를 살펴보자&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;양수 n에 대해 확률변수 \(x^{n}\)의 기댓값 \(E(x^{n})\)을 확률변수 x의 원점에 대한 n차 적률이라 한다.
\[ \mu_{n} = E(x^{n})\]
\(E(x^{n}) = \left\{\begin{matrix}
discrete \; random \;  variable : &amp;amp;  \sum_{x}x^{n}f(x)\\
continuous \; random \; variable : &amp;amp; \int_{-\infty }^{\infty}x^{n}f(x)dx \\
\end{matrix}\right.\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;f(x)는 확률질량함수 또는 확률밀도함수&lt;/li&gt;
  &lt;li&gt;위 정의에서 n = 1일 경우 우리가 알고있는 확률변수 x의 기댓값이 된다.&lt;/li&gt;
  &lt;li&gt;즉, 확률 변수의 ‘적률(moment)’이 확률 분포의 특징을 설명해 주는 역할이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;적률-생성-함수&quot;&gt;적률 생성 함수&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;적률을 쉽게 구하기 위한 함수 즉, 적률을 생성하는 함수&lt;/li&gt;
  &lt;li&gt;확률 변수 X의 적률 생성 함수\( M_{x}(t) = E(e^{tx})\)의 정의는 다음과 같다.
\(M_{x}(t) = E(e^{tx}) = \left\{\begin{matrix}
discrete \; random \; variable&amp;amp;  \sum_{x}e^{tx}f(x)\\
continuous \; random \; variable &amp;amp; \int_{-\infty }^{\infty} e^{tx}f(x) dx\\
\end{matrix}\right.\)&lt;/li&gt;
  &lt;li&gt;또한, \(M_{x}(0) = 1\)이다.&lt;/li&gt;
  &lt;li&gt;정의에는 적분이 나왔지만, 적률 생성 함수로 적률을 구할 때 다음을 이용하면 적분 없이 구할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;적률 생성 함수 \(M_{x}(t)\)로 n차 적률을 구한다면,&lt;/li&gt;
      &lt;li&gt;\(\frac{d^{n}}{dt^{n}}M_{x}(t) = M_{x}^{n}(0) = E(x^{n}) = \mu_{n} \)&lt;/li&gt;
      &lt;li&gt;즉, n차 적률을 구하려면 적률 생성 함수를 t = 0에서 n번 미분&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;적률 생성 함수의 특징을 살펴보고 마무리하자.
    &lt;ul&gt;
      &lt;li&gt;확률 변수 X와 Y가 같은 적률 생성 함수를 가지면 즉, 모든 t에 대해 \(M_{x}(t) = M_{y}(t)\)이면 두 확률변수는 같은 확률 분포를 가진다.&lt;/li&gt;
      &lt;li&gt;서로 독립인 확률 변수 \(X_{1},X_{2},\cdots,X_{n}\)의 적률 생서 함수가 각각 \( M_{X_{1}}(t),M_{X_{2}}(t),\cdots,M_{X_{n}}(t)\)일 때, 확률 변수 \(Y = X_{1}+X_{2}+\cdots+X_{n}\)의 적률 생성 함수는 \(M_{X_{Y}}(t) =  M_{X_{1}}(t) \cdot M_{X_{2}}(t) \cdots M_{X_{n}}(t) = \prod_{i=1}^{n}M_{X_{i}}(t)\)이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/blog/%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA-%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF-%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC-%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE/&quot;&gt;적률과 적률 생성 함수&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 06, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[2.3 The Gaussian Distribution(3)]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-2.3-The-Gaussian-Distribution(3)/" />
  <id>http://localhost:4000/articles/[PRML]2.3 The Gaussian Distribution(3)</id>
  <published>2022-03-06T04:01:50+09:00</published>
  <updated>2022-03-06T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;가우시안-분포의-최대-가능도&quot;&gt;가우시안 분포의 최대 가능도&lt;/h3&gt;

&lt;p&gt;책에는 없지만, 앞의 내용을 복습할 겸 다변량 가우시안 분포부터 자세히 설명할 예정이다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;먼저, 다변량 가우시안 분포부터 살펴보자.&lt;/li&gt;
  &lt;li&gt;데이터 집합  \(X = ( x_{1},\cdots,x_{D} )^{T}\)이 주어졌으며, 관측값 \( {X_{d}}\)들이 다변량 가우시안 분포로 부터 독립적으로 추출되었다고 가정해 보자.&lt;/li&gt;
  &lt;li&gt;이때 원 분산의 매개변수들을 최대 가능도 방법을 이용하여 추정할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

\[X = \begin{pmatrix}
X_{1} \\
X_{2} \\
\vdots  \\
X_{d}
\end{pmatrix} \sim MVN(\mu,\Sigma)\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;여기서 \(\mu \)는 d x 1 벡터, \( \Sigma\)는 d x d행렬이 된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;이러한 다변량 가우시안 분포의 pdf는 다음과 같이 쓸 수 있다.
\[f(x_{i};\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}}\left| \Sigma \right|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \]&lt;/li&gt;
  &lt;li&gt;만약 우리가 n개의 샘플을 가지고 있다고 가정 할 경우에 이 것들의 가능도 함수는 다음과 같이 각 샘플들을 pdf에 집어넣은 것들을 곱한 꼴로 나타낼 수 있다.
\[ L(x;\mu,\Sigma) = \prod_{i = 1}^{n}f(x_{i})= \prod_{i = 1}^{n}\left [ \frac{1}{(2\pi)^{\frac{d}{2}}\left|\Sigma \right|^{\frac{1}{2}}} exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \right ] \]&lt;/li&gt;
  &lt;li&gt;양변에 로그 함수를 취하여 로그 가능도 함수를 만들면 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

\[lnp(x \mid \mu,\Sigma) = \sum_{i=1}^{n}(-\frac{d}{2}log\pi - \frac{1}{2}log\left| \Sigma \right|-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x-\mu))\]

\[\qquad \qquad  \qquad \qquad \qquad \quad = -\frac{ND}{2}ln(2\pi) - \frac{N}{2}ln\left| \Sigma \right| -\frac{1}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{T}\Sigma^{-1}(x_{n}-\mu) \qquad 식(2.118)\]

&lt;p&gt;최대 가능도(MLE)는 위의 로그 가능도 함수를 최대로 만드는 모수 \(\mu \) 와 \(\Sigma \)의 값을 구하는 것이다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;따라서 첫번째 항은 두 개의 모수와 관계 없으므로 생략할 수 있다.&lt;/li&gt;
  &lt;li&gt;그러면 이제, MLE를 구해보자&lt;/li&gt;
  &lt;li&gt;구하기에 앞서 벡터 편미분에 대한 기본적인 개념을 알아야한다.
    &lt;ul&gt;
      &lt;li&gt;모수 \( \mu \)와 \(\Sigma \)로 각각 편미분하기 위하여 간단하게만 살펴보자.&lt;/li&gt;
      &lt;li&gt;a 와 b가 p x 1 벡터이고, A가 p x p 상수로 이루어진 행렬일 때, 다음의 미분식이 성립한다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;\( \frac{\partial a^{T}b}{\partial b} =a  \)&lt;/li&gt;
      &lt;li&gt;\( \frac{\partial a^{T}b}{\partial a} =b \)&lt;/li&gt;
      &lt;li&gt;\( \frac{\partial b^{T}Ab}{\partial B} =(A + A^{T})b \)&lt;/li&gt;
      &lt;li&gt;\( \frac{\partial log \mid A \mid}{\partial A} =(A^{-1})^{T} \)&lt;/li&gt;
      &lt;li&gt;\( \frac{\partial tr(AB)}{\partial A} = B^{T} \)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이제, \( \mu \)벡터에 대하여 로그 가능도 함수를 편미분 해보자.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\( \mu\)를 포함하고 있는 항만 살펴보면 된다.&lt;/li&gt;
  &lt;li&gt;이 항은 다음과 같은 함수 f의 합의 꼴로 볼 수 있다.
\[ f(\mu) = (x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu) \]&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/43.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/43.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;마지막으로 위에서 구한 로그 가능도 함수의 편미분 벡터를 영벡터로 만드는 벡터 \( \mu \)의 값을 찾자.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/44.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/44.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;정리하면 평균에 대한 최대 가능도 추정값의 해는 다음과 같다.
\[ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n} \qquad 식(2.121)\]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이제, 로그 가능도 함수를 \( \Sigma \)행렬로 편미분해보자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;마찬가지로 \( \Sigma \)를 로그가능도에서 찾아보면 다음과 같다.
\[ lnp(X \mid \mu,\Sigma) \propto -\frac{n}{2}log \mid \Sigma \mid -\frac{1}{2} \sum_{i=1}^{n}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu) \]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;행렬 미적분에서 2차 형식(Quadratic form)으로 나타내어진 행렬에 대해 미분하는 방법이 역랭렬로 나타내어진 것을 미분하는 것보다 훨씬 쉽기 때문에, \( \Sigma^{-1} \)로 미분하여 진행.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/45.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/45.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;그러면 이제 행렬 A로 미분해보자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/46.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/46.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;정리하면 다음과 같다.
\[ \Sigma_{ML} = \frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})(x_{n}-\mu_{ML})^{T} \qquad 식(2.122)\]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;순차-추정&quot;&gt;순차 추정&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;순차 추정 방식은 데이터 포인트들을 하나씩 처리하고 버리는 방식이다,&lt;/li&gt;
  &lt;li&gt;이는 다음과 같은 상황에서 유용하게 사용할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;관찰 데이터 집합이 매우 커서 한번에 계산이 불가능할 때&lt;/li&gt;
      &lt;li&gt;실시간 처리를 해야하는 온라인 적용 사례&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MLE로 얻어진 \( \mu_{ML}\)식을 업데이트 되는 방식으로 바꿔보자.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;\( \mu_{ML}\)에서 마지막 데이터 포인트 \(X_{N}\)따로 빼내어 보자
  \[ \mu_{ML}^{(N)} = \frac{1}{N}\sum_{n=1}^{N}X_{n} = \frac{1}{N}X_{N} +\frac{1}{N}\sum_{n=1}^{ N-1}X_{n} = \frac{1}{N}X_{N} + \frac{N-1}{N}\mu_{ML}^{(N-1)} = \mu_{ML}^{(N-1)} + \frac{1}{N}(X_{N} - \mu_{ML}^(N-1)) \qquad  식(2.126)\]&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;N-1개의 데이터로부터 추정된 \( \mu_{ML}^{(N-1)}\)와 N번째 관측된 데이터를 이용하여 \( \mu_{ML}^{(N)}\)을 구한다.&lt;/li&gt;
      &lt;li&gt;N의 값이 증가할수록 새로 관측되는 데이터의 기여도가 점점 작아지게 된다.&lt;/li&gt;
      &lt;li&gt;한번에 계산을 처리하는 배치 방식으로부터 식을 유도했기 때문에 실제 결과는 동일하게 된다.&lt;/li&gt;
      &lt;li&gt;이런 방식은 매우 유용하지만, 배치 방식의 식에서 업데이트 방식의 식을 항상 유도할 수 있는 것은 아니다.&lt;/li&gt;
      &lt;li&gt;따라서 순차적인 학습에 대한 더 일반적인 방식인 로빈스 몬로(Robbins &amp;amp; Monro) 알고리즘을 알아보자&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-2.3-The-Gaussian-Distribution(3)/&quot;&gt;2.3 The Gaussian Distribution(3)&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 06, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[2.3 The Gaussian Distribution(2)]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-2.3-The-Gaussian-Distribution(2)/" />
  <id>http://localhost:4000/articles/[PRML]2.3 The Gaussian Distribution(2)</id>
  <published>2022-03-05T04:01:50+09:00</published>
  <updated>2022-03-05T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;h4 id=&quot;주변-가우시안-분포&quot;&gt;주변 가우시안 분포&lt;/h4&gt;

&lt;p&gt;이제 두 번째, 각 변수 집합의 주변 분포 역시 가우시안 분포를 보이는 것을 확인해보자.&lt;/p&gt;

&lt;p&gt;먼저,&lt;b&gt;주변 확률 분포&lt;/b&gt;가 무엇인지 확인하고 넘어가자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;결합 확률 분포에서 한 쪽의 변수만 보는 것.&lt;/li&gt;
  &lt;li&gt;이 때 나머지 변수는 합산하여 사라지게 되는데 이산 변수는 모든 확률 값의 합으로, 연속 변수의 경우 적분을 통해 진행된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이제 다음의 식으로 주어지는 주변 분포에 대해 살펴보자.&lt;/p&gt;

\[p(\textbf{x}_{a}) = \int p(\textbf{x}_{a},\textbf{x}_{b}) \; d\textbf{x}_{b} \qquad 식(2.83)\]

&lt;ul&gt;
  &lt;li&gt;앞에서와 같이 이차식에서 평균과 공분산을 구하는 전략을 사용할 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;자 그러면, 가우시안 분포에서 이차 형식을 다시 살펴보자.&lt;/p&gt;

&lt;p&gt;\[ -\frac{1}{2}(x -\mu)^{t}\Sigma^{-1}(x -\mu) =  \]
\[-\frac{1}{2}(x_{a}-\mu_{a})^{T}\Lambda_{aa}(x_{a}-\mu_{a})-\frac{1}{2}(x_{a}-\mu_{a})^{T}\Lambda_{ab}(x_{b}-\mu_{b}) \]
\[\qquad \qquad \quad -\frac{1}{2}(x_{b}-\mu_{b})^{T}\Lambda_{ba}(x_{a}-\mu_{a})-\frac{1}{2}(x_{b}-\mu_{b})^{T}\Lambda_{bb}(x_{b}-\mu_{b}) \qquad 식(2.70) \]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;여기서 우리는 \(x_{b}\)에 연관된 항들을 적분시켜서 없애는 것이 우리의 목표다.&lt;/li&gt;
  &lt;li&gt;이를 위해서 \(x_{b} \)에 연관된 항들을 일단 먼저 고려하여 완전제곱식을 적용해야 한다.&lt;/li&gt;
  &lt;li&gt;완전제곱식 역시 다시 살펴보자.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ -\frac{1}{2}(x -\mu)^{T}\Sigma^{-1}(x -\mu) = -\frac{1}{2}x^{T}\Sigma{-1}x + x^{T}\Sigma{-1}\mu + const \qquad 식(2.71)\]&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;이를 이용한 전략은 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/36.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/36.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;자세히 설명하면,
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;\(f(x_{b},x_{a})\) : 원래 지수부를 \( (x_{a},x_{b}) \) 부분 집합을 통해 전개한 식 중에서 \(x_{b} \)을 포함한 모든 항들을 모은 식이다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;\( g(x_{a}) \) : \(f(x_{b},x_{a})\)에 포함된 항등을 제외한 항들 중 \(x_{a}\)를 포함한 모든 항들을 모은 식이다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;const : 그 외 나머지 항들을 모은 식이다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;\(\tau + g(x_{a})\)를 \(x_{a}\)의 완전제곱식으로 만들면 \(x_{a}\)의 평균벡터와 공분산 행렬을 구할 수 있다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이제 식을 펼쳐보자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/37.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/37.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;완전제곱식을 이용해서 \(f(x_{b},x_{a})\)에 대해 전개하면 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

\[-\frac{1}{2}\textbf{x}_{b}^{T}\Lambda_{bb}\textbf{x}_{b} + \textbf_{b}^{T}m = -\frac{1}{2}(\textbf{x}_{b} - \Lambda_{bb}^{-1}m)^{T}\Lambda_{bb}(\textbf{x}_{b}-\Lambda_{bb}^{-1}m) +\frac{1}{2}m^{T}\Lambda_{aa}^{-1}m \qquad 식(2.84)\]

&lt;ul&gt;
  &lt;li&gt;여기서 m은 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

\[m = \Lambda_{bb}\mu_{b} - \Lambda_{ba}(\textbf{x}_{a} - \mu_{a}) \qquad 식(2.85)\]

&lt;ul&gt;
  &lt;li&gt;그러면 \(\tau \)는 다음과 같음을 알 수 있다.&lt;/li&gt;
&lt;/ul&gt;

\[\tau = \frac{1}{2}m^{T}\Lambda_{bb}^{-1}m\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;그러면
\[ \int exp(f(x_{b},x_{a}) - \tau)dx_{b} = \int exp(-\frac{1}{2}(x_{b}-\Lambda_{bb}^{-1}m)^{T}\Lambda_{bb}(x_{b}-\Lambda_{bb}^{-1}m))dx_{b} \qquad 식(2.86)\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;이 값은 공분산 \( \Lambda_{aa}\)에만 종속되고 \(x_{a}\)에 독립적이므로 \(\alpha \beta exp(\tau + g(x_{a} )+ const )\)의 지수부에만 집중하면 된다.&lt;/li&gt;
  &lt;li&gt;그러면 \(\tau + g(x_{a}) + const\)를 살펴보자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ \tau + g(x_{a}) + const = \frac{1}{2}m^{T}\Lambda_{bb}^{-1}m - \frac{1}{2}x_{a}^{TT}\Lambda_{aa}x_{a}+x_{a}^{T}(\Lambda_{aa}\mu_{a} + \Lambda_{ab}\mu_{b}) + const \]
\[\qquad \qquad \qquad \qquad \;\; =-\frac{1}{2}x_{a}^{T}(\Lambda_{aa} - \Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})x_{a} + x_{a}^{T}(\Lambda_{aa} - \Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})\mu_{a} + const \qquad 식(2.87) \]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;따라서 공분산은 다음과 같다(이차식을 살펴보자).
\[\Sigma_{a} = (\Lambda_{aa} - \Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})^{-1} \qquad 식(2.88)\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;평균 벡터는 다음과 같다(1차식을 살펴보자).
\[ \Sigma_{a}(\Lambda_{aa} - \Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})\mu_{a} = \mu_{a} \qquad 식(2.89)\]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;*공분산의 형태가 복잡하게 보이지만, 슈어 보수행렬(Schur complement)를 사용하면 다음을 알 수 있다.
\[ (\Lambda_{aa} - \Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})^{-1} = \Sigma_{aa} \]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;따라서 정리하면&lt;/li&gt;
&lt;/ul&gt;

\[E[x_{a}] = \mu_{a}\]

\[cov[x_{a}] = \Sigma_{aa}\]

&lt;ul&gt;
  &lt;li&gt;이 결과는 우리의 직돤과도 일치한다.&lt;/li&gt;
  &lt;li&gt;두 개의 변수에 대한 다변량 가우시안 분포의 조건부 분포와 주변 분포의 예시를 마지막으로 살펴보자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/35.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/35.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;가우시안-변수에-대한-베이지안-정리&quot;&gt;가우시안 변수에 대한 베이지안 정리&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;앞서 배운 내용을 정리하면
    &lt;ul&gt;
      &lt;li&gt;가우시안 분포 p(x)에 대해 벡터 집합 \( x = (x_{a} , x_{b}) \)로 나눈 후, 조건부 분포 \( p(x_{a} \mid x_{b}) \)와 주변 분포 \(p(x_{a})\) 역시 가우시안 분포임을 확인하였다.&lt;/li&gt;
      &lt;li&gt;이때, 조건부 분포 \( p(x_{a} \mid x_{b}) \)의 평균이 \(x_{b}\)에 대해서 선형이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이번에는 가우시안 주변 확률 분포인 p(x)와 가우시안 조건부 분포 \(p(y \mid x) \)에 대해 살펴보자
    &lt;ul&gt;
      &lt;li&gt;여기서, \(p(y \mid x)\)의 평균이 x에 대한 선형 함수이며, 공분산 x에 대해 독립적이다.&lt;/li&gt;
      &lt;li&gt;이는 바로 선형 가우시안 모델의 한 예이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이때 주변 분포와 조건부 분포를 다음과 같이 정의하자.
\[ p(x) = N(x \mid \mu, \Lambda^{-1} ) \qquad 식(2.99)\]
\[ p( y \mid x) = N(y \mid Ax + b, L^{-1}) \qquad 식(2.100)\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;식에 대해 설명하면,
    &lt;ul&gt;
      &lt;li&gt;\(\mu \), A, b : 평균을 조절하는 매개변수&lt;/li&gt;
      &lt;li&gt;\(\Lambda^{-1},L^{-1}\) : 정밀도 행렬&lt;/li&gt;
      &lt;li&gt;만약 x가 M차원, y가 D차원이면 행령 A의 크기는 D x M이 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;x와 y의 결한 분포에 대한 표현식을 살펴보자
\[ z = \binom{x}{y} \qquad 식(2.101)\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;결론부터 말하자면, p(x)와 \(p(y \mid x)\)룰 이용하여 p(y),\(p(x \mid y)\)를 구하는 것이다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;즉, \(p(z) = p(x)p(y \mid x) \)인 식을 \(p(x \mid y)p(y)\)의 식으로 전개하는 것을 베이즈론을 활용하여 증명하고자 하는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그리고 이어서, 결합 분포에 로그를 씌우면 다음과 같다.
\[ ln(z) = lnp(x) + lnp(y \mid x) \]
\[ = -\frac{1}{2}(x - \mu)^{T}\Lambda(x - \mu) - \frac{1}{2}(y -Ax - b)^{T}L(y-Ax-b) + const \qquad 식(2.102)\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;앞에서와 같이, 이차식 형태를 띠는 것을 확인할 수 있다. 따라서 p(z)는 가우시안 분포다.&lt;/li&gt;
  &lt;li&gt;또한, 앞에서와 같이 정밀도를 찾기 위해 이차항만 고려해보자.&lt;/li&gt;
&lt;/ul&gt;

\[-\frac{1}{2}x^{T}(\Lambda + A^{T}LA)x -\frac{1}{2}y^{T}Ly + \frac{1}{2}y^{T}LAx + \frac{1}{2}x^{T}A^{T}Ly\]

\[= -\frac{1}{2}\binom{x}{y}^{T}\begin{pmatrix}
\Lambda + A^{T}LA &amp;amp; -A^{T}L \\
-LA &amp;amp; L \\
\end{pmatrix}\binom{x}{y} = -\frac{1}{2}z^{T}Rz \qquad 식(2.103)\]

&lt;ul&gt;
  &lt;li&gt;따라서 z는 다음과 같은 형태의 정밀 행렬을 가지게 된다.&lt;/li&gt;
&lt;/ul&gt;

\[R = \begin{pmatrix}
\Lambda + A^{T}LA &amp;amp; -A^{T}L \\
-LA &amp;amp; L \\
\end{pmatrix}  \qquad 식(2.104)\]

&lt;ul&gt;
  &lt;li&gt;정밀 행렬의 역행렬인 공분산은 다음과 같다.(앞에서 설명한 식(2.76) 참고)&lt;/li&gt;
&lt;/ul&gt;

\[cov[z] = R^{-1} = \begin{pmatrix}
\Lambda^{-1} &amp;amp; \Lambda^{-1}A^{T} \\
A\Lambda^{-1} &amp;amp; L^{-1}+ A\Lambda^{-1}A^{T} \\
\end{pmatrix} \qquad 식(2.105)\]

&lt;ul&gt;
  &lt;li&gt;이제, 평균을 구하기 위해 식(2.102)의 일차항만 고려해 보자
\[ x^{T}\Lambda\mu - x^{T}A^{T}Lb + y^{T}Lb = \binom{x}{y}^{T} \binom{\Lambda\mu - A^{T}Lb}{Lb} \qquad 식(2.106)\]&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;앞에서 설명한 식(2.71)을 이용하여 z의 평균을 구하면 다음과 같다.
\[ E(z) = R^{-1}\binom{\Lambda\mu - A^{T}Lb}{Lb} \qquad 식(2.107)\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;여기에 식(2.105)를 적용하면 다음과 같다.
\[ E(z) = \binom{\mu}{A\mu + b} \qquad 식(2.108)\]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;자, 그러면 이를 토대로 p(y)와 \( p(x \mid y)\)의 평균과 공분산을 구할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;먼저, p(y)의 평균과 공분산은 식(2.92)와 식(2.93)을 통해 구할 수 있다.
\[ E(y) = A\mu + b \qquad 식(2.109)\]
\[ cov(y) = L^{-1} + A\Lambda^{-1}A^{T} \qquad 식(2.110) \]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;여기서 A=I인 경우 두 가우시안의 관계가 convolution 관계이다.
    &lt;ul&gt;
      &lt;li&gt;여기서 convolution 은 두 개의 가우시안 함수가 서로 오버랩되는 영역을 나타내는 식이라고 생각하면 된다.&lt;/li&gt;
      &lt;li&gt;이렇게 오버랩되는 영역도 마찬가지로 가우시안 분포를 따르게 된다.&lt;/li&gt;
      &lt;li&gt;이 때 생성되는 분포의 평균는 각각의 분포의 평균의 합의 평균이 되고 분산은 각각의 분포의 분산의 합의 평균이 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이제, 조건부 분포 \( p(x \mid y)\)에 대한 표현식을 구해 보자.&lt;/li&gt;
  &lt;li&gt;앞에서 구한 식(2.73)과 식(2.105)를 이용하여 평균과 공분산을 쉽게 구할 수 있다.
\[ E(x \mid y) = (\Lambda + A^{T}LA)^{-1}{A^{T}L(y-b)+\Lambda\mu} \qquad 식(2.111)\]
\[ cov(x \mid y) = (\Lambda + A^{T}LA)^{-1} \qquad 식(2.112)\]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;마지막으로 정리하면서 마무리하자.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;다음과 같은 가우시안 확률 분포가 주어졌을 때,
\[ p(x) = N(x \mid \mu, \Lambda^{-1} ) \qquad 식(2.113)\]
\[ p( y \mid x) = N(y \mid Ax + b, L^{-1}) \qquad 식(2.114)\]&lt;/li&gt;
  &lt;li&gt;주변 확률 분포와 조건부 확률 분포는 다음과 같다.
\[ p(y) = N(y \mid A\mu + b, L^{-1}+A\Lambda^{-1}A^{T}) \qquad 식(2.115)\]&lt;/li&gt;
&lt;/ul&gt;

\[p(x \mid y) = N(x \mid \Sigma \left\{ A^{T}L(y -b) + \Lambda\mu \right\},\Sigma) \qquad 식(2.116)\]

&lt;ul&gt;
  &lt;li&gt;여기서 \(\Sigma \)는 다음과 같이 정의된다.
\[ \Sigma = (\Lambda + A^{T}LA)^{-1} \qquad 식(2.117)\]&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-2.3-The-Gaussian-Distribution(2)/&quot;&gt;2.3 The Gaussian Distribution(2)&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 05, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[디리클레 분포]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/blog/%E1%84%83%E1%85%B5%E1%84%85%E1%85%B5%E1%84%8F%E1%85%B3%E1%86%AF%E1%84%85%E1%85%A6-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%91%E1%85%A9/" />
  <id>http://localhost:4000/blog/디리클레 분포</id>
  <published>2022-03-04T07:08:50+09:00</published>
  <updated>2022-03-04T07:08:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;a href=&quot;https://jjomaeng.github.io/blog/베타-분포/&quot;&gt;베타 분포&lt;/a&gt;의 확장 버전인 디리클레 분포(Dirichlet distribution)를 살펴보자&lt;/p&gt;

&lt;h3 id=&quot;디리클레-분포dirichlet-distribution&quot;&gt;디리클레 분포(Dirichlet distribution)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;디리클레 분포는 2가지 경우만을 다루는 베타분포를 k가지 경우를 다루도록 확장한 버전이다.&lt;/li&gt;
  &lt;li&gt;디리클레분포란 𝑘차원의 실수 벡터 중 벡터의 요소가 양수이며 모든 요소를 더한 값이 1인 경우에 확률값이 정의되는 연속확률분포이다.&lt;/li&gt;
  &lt;li&gt;2이상의 자연수 𝑘와 양의 상수 α1,…,α𝑘에 대하여 디리클레분포의 확률밀도함수는 다음과 같이 정의된다.
    &lt;ul&gt;
      &lt;li&gt;실수값 \(x_{1},\cdots,x_{k}\)가 모두 양의 실수이며 \(\sum_{i = 1 }^{k}x_{i} = 1\)을 만족할 때,&lt;/li&gt;
      &lt;li&gt;\( f(x_{1},\cdots,x_{k};,\alpha_{1},\cdots,\alpha_{k}) = \frac{1}{\beta(\alpha)}\prod_{i = 1}^{k}x_{i}^{\alpha_{i}-1}\)의 값을 가지며,&lt;/li&gt;
      &lt;li&gt;그 외의 경우는 0의 값을 가진다&lt;/li&gt;
      &lt;li&gt;이때 \(\alpha = (\alpha_{1},\cdots,\alpha_{k})\)이며, \(\beta(\alpha)\)는 정규화 상수로서 다음의 값을 가진다.&lt;/li&gt;
      &lt;li&gt;\(\beta(\alpha) = \frac{\prod_{i = 1}^{k}\Gamma(\alpha_{i}}{\Gamma(\sum_{i = 1}^{k}\alpha_{i})}\)&lt;/li&gt;
      &lt;li&gt;디리클레 분포에서 k = 2인 경우가 베타 분포가 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;다항-분포의-결레-사전-분포&quot;&gt;다항 분포의 결레 사전 분포&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;이항 분포의 켤레 사전 분포로 베타 분포를 봤듯이 다항 분포와 디리클레 분포는 동일한 분포를 따른다.&lt;/li&gt;
  &lt;li&gt;즉, 디리클레 분포 \(\theta \sim Dir(\alpha)\)와 그에 대한 다항 분포 \(X \mid \theta \sim Multinomial(\theta )\)에 대하여, X가 주어졌을 때 \(\theta \)의 사후 확률 \(\theta \mid X\)는 다음과 같이 나타낼 수 있다.
\[ \theta \mid X  \sim Dir(\alpha + X)\]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;켤레-사전-분포conjugate-prior-distribution&quot;&gt;켤레 사전 분포(Conjugate prior distribution)&lt;/h3&gt;

&lt;p&gt;마지막으로, 앞서 배운 이항 분포와 함께 켤레 사전 분포의 관계를 표로 정리해보자&lt;/p&gt;

&lt;table rules=&quot;groups&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;우도&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;켤레사전분포&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;사후확률분포&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;파라미터의 의미&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;이항분포&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;베타분포 \(Beta(\alpha,\beta)\)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;베타분포 \(Beta({}\alpha ‘,{}\beta ‘)\)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;성공횟수 : α−1, 실패횟수 : β−1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;다항분포&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;디리클레분포 \(Dir(\alpha)\)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;디리클레분포 \(Dir({}\alpha ‘)\)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;𝑖번째 범주가 나타날 횟수 : \(α_{𝑖}\)−1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/blog/%E1%84%83%E1%85%B5%E1%84%85%E1%85%B5%E1%84%8F%E1%85%B3%E1%86%AF%E1%84%85%E1%85%A6-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%91%E1%85%A9/&quot;&gt;디리클레 분포&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 04, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[2.3 The Gaussian Distribution]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-2.3-The-Gaussian-Distribution/" />
  <id>http://localhost:4000/articles/[PRML]2.3 The Gaussian Distribution</id>
  <published>2022-03-04T04:01:50+09:00</published>
  <updated>2022-03-04T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;p&gt;정규 분포라고도 알려져 있는 가우시안 분포는 연속 변수를 모델하는 분포로 매우 널리 활용되고 있다.
단일 변수 x에 대한 가우시안 분포는 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ N(x \mid \mu,\sigma^{2}) = \frac{1}{(2\pi \sigma^{2})^{\frac{1}{2}}}exp(-\frac{1}{2\sigma^{2}}(x-\mu)^{2}) \qquad 식(2.42)\]&lt;/p&gt;

&lt;p&gt;여기서 \(\mu \)평균을, \(\sigma^{2}\)는 분산을 나타낸다. D차원 벡터 x에 대한 다변량 가우시안 분포는 다음의 형태를 띤다.&lt;/p&gt;

&lt;p&gt;\[ N(x \mid \mu,\Sigma) = \frac{1}{(2\pi)^{\frac{1}{2}}}\frac{1}{\mid \Sigma \mid^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)) \qquad 식(2.43)\]&lt;/p&gt;

&lt;p&gt;여기서 \(\mu \)는 D차원 평균를, \(\sigma^{2}\)는 D x D 공분산 행렬을,\(\Sigma \)는 \(\Sigma \)의 행렬식을 의미한다.&lt;/p&gt;

&lt;p&gt;가우시안 분포는 여러 다양한 상황에서 여러 가지 다른 용도로 활용될 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;단일 실변수에 대해서 엔트로피를 극대화하는 분포가 가우시안 분포이다. 이 성질은 다변량 가우시안 분포의 경우에도 동일하게 적용된다.&lt;/li&gt;
  &lt;li&gt;라플라스의 중심 극한 정리
    &lt;ul&gt;
      &lt;li&gt;여러개의 확률 변수들의 합에 해당하는 확률 변수는 몇몇 조건하에서 합해지는 확률 변수의 숫자가 증가함에 따라서 점점 가우시안 분포가 되어간다.&lt;/li&gt;
      &lt;li&gt;표본 평균(sample mean)들이 이루는 분포는 샘플 크기가 큰 경우 모집단의 원래 분포와 상관없이 가우시안 분포를 따른다.&lt;/li&gt;
      &lt;li&gt;동일한 확률 분포를 따르는 N개의 독립 확률 변수의 평균 값은 N이 충분이 크다면 가우시안 분포를 따른다.&lt;/li&gt;
      &lt;li&gt;N개의 확률 변수가 어떤 확률 분포를 따르든지 상관없이 N이 충분이 크다면 그 합은 가우시안 분포를 따른다.&lt;/li&gt;
      &lt;li&gt;표본의 값을 랜덤 변수로 놓는 것이 아니라 표본의 평균 값을 랜덤 변수로 놓는 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/27.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/27.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;
&lt;p&gt;균일하게 분포된 N개의 값의 평균에 대한 히스토그램을 다양한 N값에 대해 그러보면, N값이 증가함에 따라서 분포는 가우시안 형태를 띠게 된다.
마찬가지로 N개의 이진 확률 변수 x의 관찰값의 합인 m에 대한 이항 분포 역시 N -&amp;gt; ∞이 됨에 따라서 가우시안의 형태를 띤다.
가우시안 분포를 자세히 살펴보자.&lt;/p&gt;

&lt;p&gt;우선 첫 번째로 가우시안 분포의 기하학적 형태를 살펴보자.&lt;/p&gt;

&lt;p&gt;x에 대한 가우시안 분포의 함수적 종속성은 식(2.43)의 지수부에 나타난다.&lt;/p&gt;

&lt;p&gt;\[ \Delta^{2} = (x - \mu )^{T} \Sigma^{-1}(x-\mu) \qquad 식(2.44) \]&lt;/p&gt;

&lt;p&gt;여기서 \(\Delta \)값은  μ로부터 x까지의 마할라노비스 거리(Mahalanobis distance)라고 한다.
마할라노비스 거리는 \(\Sigma \)가 항등 행렬(I)일 경우 유클리디안 거리가 된다. 이 값은 평균과의 거리가 표준편차의 몇 배인지를 나타내는 값으로 평균과의 거리를 측정할 때 분산도를 고려한다는 의미가 된다. 이 이차식의 상수가 되는 x 공간의 표면에서는 가우시안 분포 역시 상수가 된다.&lt;/p&gt;

&lt;p&gt;여기서 중요한 점은 공분산 행렬 \(\Sigma\)가 대칭 행렬이라는 점이다.&lt;/p&gt;

&lt;p&gt;공분산 행렬의 고유벡터, 고윳값을 정의한 식은 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ \Sigma u_{i} = \lambda_{i}u_{i} \qquad 식(2.45)\]&lt;/p&gt;

&lt;p&gt;이로 인해 지수(exponent) 연산에 의해서 비대칭적인 요소가 모두 사라진다.( 종모양의 대칭적인 요소가 된다.)&lt;/p&gt;

&lt;p&gt;여기서 i = 1,…,D다. \(\Sigma \)가 실수 대칭 행렬이므로 고윳값 역시 실수이다.
또한 공분산 행렬 \(\Sigma \)가 대칭행렬이기 때문에 고유벡터는 모두 직교한다.
따라서 다음을 만족한다.&lt;/p&gt;

&lt;p&gt;\[ U_{i}^{T}U_{j} = I_{i,j} \qquad 식(2.46)\]&lt;/p&gt;

&lt;p&gt;이때 고유 벡터를 이용해서 공분산 행렬 \(\Sigma \)를 전개할 수 있으며, 이는 다음의 형태를 띠게 된다.&lt;/p&gt;

&lt;p&gt;\[ \Sigma = \sum_{i = 1}^{D} \lambda_{i} u_{i}u_{i}^{T} \qquad 식(2.48)\]&lt;/p&gt;

&lt;p&gt;여기서 공분산 행렬이 직교 행렬이라는 것을 알 수 있으며 (고유벡터로 이루어져 있으니까) 직교 행렬의 성질은 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;nxn 직교 행렬이면, n개의 서로 직교하는 벡터들로 이루어진 행렬 그 벡터들의 길이가 1이다.&lt;/li&gt;
  &lt;li&gt;직교 행렬의 행렬식은 -1 혹은 1 이다.&lt;/li&gt;
  &lt;li&gt;역행렬과 전치 행렬이 같다 \( \;A^{T} = A^{-1}\quad (A는 직교행렬) \)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서 공분산 행렬의 역행렬 \( \Sigma^{-1} \)을 다음과 같이 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \Sigma^{-1} = \sum_{i = 1}^{D} \frac{1}{\lambda_{i}} u_{i}u_{i}^{T} \qquad 식(2.49)\]&lt;/p&gt;

&lt;p&gt;식(2.49)를 식(2.44)에 대입하면 이차식이 다음의 형태를 띤다.
\[ \Delta^{2} = \sum_{i = 1}^{D} \frac{y_{i}^{2}}{\lambda_{i}} \qquad 식(2.50)\]&lt;/p&gt;

&lt;p&gt;여기서 \(y_{i}\)는 다음처럼 정의된다.&lt;/p&gt;

&lt;p&gt;\[ y_{i} = u_{i}^{T}(x-\mu) \qquad 식(2.51)\]&lt;/p&gt;

&lt;p&gt;\({y_{i}} \)를 정규직교 벡터 \(u_{i}\)들로 정의되는 새로운 좌표계라고 해석할 수 있다. 원래의 \(x_{i}\)좌표계로부터 \(\mu \)이동되고 고유 벡터를 축으로 회전된 것이다.&lt;/p&gt;

&lt;p&gt;벡터 \(\textbf{Y}= (y_{1},….y_{D})^{T}\)이라 하면 다음 식을 얻게 된다.
\[ \textbf{Y} = \textbf{U}(x-\mu) \qquad 식(2.52) \]&lt;/p&gt;

&lt;p&gt;직교 행렬의 성질에 따라 \(\textbf{U}\textbf{U}^{T} = \textbf{I}\)를 만족한다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;이차식, 즉 가우시안 밀도 함수는 식(2.50)이 상수인 표면에 대해서 상수일 것이다. 만약 모든 고윳값 \( \lambda \)들이 양의 값을 가진다면, 이 표면은 타원형을 띤다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/28.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/28.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;그림에 대해 설명하자면, 타원형의 중심은 \(\mu \)에 위치하며, 이 타원형의 축은 \(\mu_{i} \)상에 위치하게 된다.
그리고 각각의 축 방향에 대한 척도 인자는 \( \lambda_{i}^{\frac{1}{2}}\)로 주어진다.&lt;/p&gt;

&lt;p&gt;가우시안 분포를 더 잘 정의하기 위해서는 공분산 행렬의 모든 고윳값 \(\lambda_{i}\)들이 순양숫값을ㄹ 가질 필요가 있다.
이 순양수의 값을 가지는 행렬을 &lt;b&gt;양의 정부호(positive definite)&lt;/b&gt;행렬이라 한다. 만약 모든 고윳값이 0 또는 0보다 큰 값을 가질 경우에 공분산 행렬을 &lt;b&gt;양의 준정부호(positive semidefinite)&lt;/b&gt;의 성징을 가졌다고 한다.&lt;/p&gt;

&lt;p&gt;야코비언 행렬을 먼저 설명하면, 공간의 선형 변환시 발생되는 부피의 변화율을 확률식에 반영하면 다음과 같다.
\[\int_{x}f(x)dx = \int_{y}f(y) \left| J \right| dy\]&lt;/p&gt;

&lt;p&gt;이제 \( y_{i}\)로 정의되는 새로운 좌표 체계상에서의 가우시안 분포의 형태에 대해서 살펴보면, x좌표계에서 y좌표계로 변환되는 과정에서 야코비안 행렬 J를 가지게 된다.&lt;/p&gt;

&lt;p&gt;J의 각 원소는 다음과 같이 주어진다.&lt;/p&gt;

&lt;p&gt;\[ J_{i,j} = \frac{\partial x_{i}}{\partial y_{j}} =\frac{\partial}{\partial y_{j}} (U_{j,i}y_{i} + \mu) = U_{j,i} \qquad 식(2.53)\]&lt;/p&gt;

&lt;p&gt;여기서 \(U_{j,i}\)는 행렬 \(U^{T} \)의 원소에 해당한다. 행렬 U의 정규직교성을 바탕으로 야코비안 행렬의 행렬식 제곱이 다음과 같음을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \mid J \mid^{2} = \mid U^{T}\mid^{2} = \mid U^{T} \mid \mid U \mid= \mid U^{T}U \mid = \mid I \mid = 1 \qquad 식(2.54) \]&lt;/p&gt;

&lt;p&gt;따라서 \(\mid J \mid  = 1\)이다.&lt;/p&gt;

&lt;p&gt;또한, 공분산 행렬의 행렬식 \( \mid \Sigma \mid \)는 고윳값의 곱으로 표현할 수 있다. 따라서 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ \mid \Sigma \mid^{\frac{1}{2}} = \prod_{j = 1}^{D}\lambda_{j}^{\frac{1}{2}} \qquad 식(2.55) \]&lt;/p&gt;

&lt;p&gt;따라서 \(y_{j}\)좌표계에서 가우시안 분포는 다음의 형태를 가지게 된다. 이는 x축에서 y축의 전환이다.&lt;/p&gt;

&lt;p&gt;\[ p(y) = p(x)\mid J \mid =  \prod_{j = 1}^{D}\frac{1}{(2\pi \lambda_{j})^{\frac{1}{2}}}exp(-\frac{y_{j}^{2}}{2\lambda_{j}}) \qquad 식(2.56)  \]&lt;/p&gt;

&lt;p&gt;이는 D개의 단변량 정규 분포가 독립적이기 때문에 곱한 형태로 이루어진 것을 확인할 수 있다.
따라서 고유 벡터를 이용해서 이동되고 회전된 새로운 좌표축을 표현하고, 얻은 식은 결국 차원간 서로 독립적인 정규 분포를 만들어낸다.&lt;/p&gt;

&lt;p&gt;y 좌표계상에서 분포의 적분은 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ \int p(y) dy = \prod_{j = 1}^{D}\int_{-\infty}^{\infty}\frac{1}{(2\pi\lambda_{j})^{\frac{1}{2}}}exp(-\frac{y_{j}^{2}}{2\lambda_{j}})dy_{i} = 1 \qquad 식(2.57)\]&lt;/p&gt;

&lt;p&gt;식(2.57)은 식(2.43)의 다변량 가우시안 분포가 정규화되었다는 것을 증명해준다.&lt;/p&gt;

&lt;p&gt;가우시안 분포의 모멘트값들을 살펴봄으로써 매개변수 \(\mu , \Sigma\)를 어떻게 해석할 수 있는지 알아보도록 하자.(&lt;a href=&quot;https://jjomaeng.github.io/blog/적률과-적률-생성-함수/&quot;&gt;모멘트에 대한 자세한 설명은 따로 작성하였습니다.&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;가우시안 분포에서의 x의 기댓값은 다음으로 주어진다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/29.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/29.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;위의 식은 z에 의해 좌우 대칭인 함수가 만들어진다. 여기에 \((z+\mu) \)식이 추가되므로 \(\mu\)만큼 평행이동한 함수가 되어 중심이 \(\mu\)이고 좌우 대칭인 정규 함수가 만들어진다.&lt;/p&gt;

&lt;p&gt;이제는 가우시안 분포의 이차 모멘트값을 살펴보도록 하자.&lt;/p&gt;

&lt;p&gt;\[ E(\textbf{x}\textbf{x}^{T}) = \frac{1}{(2\pi^{\frac{D}{2}})}\frac{1}{\mid \Sigma \mid^{\frac{1}{2}}} \int(-\frac{1}{2}(\textbf{x}-\mu)^{T}\Sigma^{-1}(\textbf{x}-\mu))\textbf{x}\textbf{x}^{T}d\textbf{x}\]&lt;/p&gt;

&lt;p&gt;\[= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{\mid \Sigma \mid^{\frac{1}{2}}} \int exp(-\frac{1}{2}\textbf{z}^{T}\Sigma^{-1}\textbf{z})(\textbf{z} + \mu)(\textbf{z} + \mu)^{T} d\textbf{z} \]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\textbf{z} = \textbf{x} - \mu\) 적용&lt;/li&gt;
  &lt;li&gt;\((\textbf{z} + \mu)(\textbf{z} + \mu)^{T} \)를 전개해보자.&lt;/li&gt;
  &lt;li&gt;\(\mu\textbf{z}^{T},\textbf{z}\mu^{T} \)의 교차항들은 대칭성에 의해 사라짐&lt;/li&gt;
  &lt;li&gt;\(\mu\mu^{T}\)는 상수로 밖으로 빼낼 수 있으며 정규화 된 가우시안 분포의 적분 값은 1&lt;/li&gt;
  &lt;li&gt;\(\textbf{z}\textbf{z}^{T}\)값을 자세히 살펴보자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/31.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/31.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;결과적으로 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ E(\textbf{x}\textbf{x}^{T}) = \mu\mu^{T} + \Sigma \qquad 식(2.62)\]&lt;/p&gt;

&lt;p&gt;x의 공분산은 다음과 같이 구할 수 있다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/30.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/30.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;가우시안 분포는 밀도 모델로써 널리 활용되지만, 한계점을 가졌다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;매개변수 개수의 증가
    &lt;ul&gt;
      &lt;li&gt;D차원의 데이터에서 D(D+3)/2개 만큼의 매개변수를 가지게 된다.&lt;/li&gt;
      &lt;li&gt;즉, 매개변수 D에 대해 2차로 증가&lt;/li&gt;
      &lt;li&gt;그 결과 큰 D값에 대해서는 계산이 매우 느려질 수 있다.&lt;/li&gt;
      &lt;li&gt;해결 방법
        &lt;ul&gt;
          &lt;li&gt;제한된 형태의 공분산 행렬을 사용
            &lt;ul&gt;
              &lt;li&gt;대각 행렬 공분산 사용 : 2D개의 매개변수만 고려&lt;/li&gt;
              &lt;li&gt;동방성 (항등 행렬 상수배) 공분산 사용 : D+1개의 매개변수만 고려&lt;/li&gt;
              &lt;li&gt;확률 밀도의 형태를 제약시키며, 그에 따라 모델에 데이터 상의 흥미로운 상관관계를 표현하는 것을 방해할 수 있다.&lt;/li&gt;
              &lt;li&gt;그림으로 살펴보면 다음과 같다&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/32.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/32.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        * (a) 일반적인 공분산 행렬 경우
        * (b) 공분산이 대각 행렬인 경우, 좌표축에 따라 타원형 형태를 띤다
        * (c) 공분산 행렬이 항등 행렬의 상수배인 경우, 경로가 동심원 형태를 띤다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;가우시안 분포가 단봉 분포 형태
    &lt;ul&gt;
      &lt;li&gt;다봉 분포에 대해 적절한 근사치를 제공할 수 없다.&lt;/li&gt;
      &lt;li&gt;해결 방법
        &lt;ul&gt;
          &lt;li&gt;잠재 변수(latent variable)사용&lt;/li&gt;
          &lt;li&gt;계층적 모델 집합 구성 이용 (8장 확률적 그래프 모델에서 다룸)
            &lt;ul&gt;
              &lt;li&gt;마르코프 무작위장&lt;/li&gt;
              &lt;li&gt;선형 동적 시스템&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;조건부-가우시안-분포&quot;&gt;조건부 가우시안 분포&lt;/h4&gt;

&lt;p&gt;다차원 가우시안 확률 분포의 특징을 살펴보자
결론부터 설명하자면,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;두 변수 집합이 결합적으로 가우시안 분포를 보인다면 하나의 변수 집합에 대한 다른 변수 집합의 조건부 분포 역시 가우시안 분포를 보인다.&lt;/li&gt;
  &lt;li&gt;각 변수 집합의 주변 분포 역시 가우시안 분포를 보인다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;먼저, 첫 번째의 조건부 분포의 경우를 살펴보자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;D차원의 벡터 x가 \(N(x \mid \mu,\Sigma)\)의 가우시안 분포를 가진다고 가정하고 x를 두 개의 부분집합 \(x_{a},x_{b}\)로 분리해보자&lt;/li&gt;
  &lt;li&gt;\(x_{a}\)가 x의 첫 M 원소에, \(x_{b}\)가 나머지 D-M개의 원소에 해당한다고 가정할 수 있다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그러면 다음과 같이 표현할 수 있다.
\[ X = \binom{X_{a}}{X_{b}} \qquad 식(2.65)\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이에 해당하는 각각 부분 집합의 평균값 벡터 \(\mu \)를 다음과 같이 정의할 수 있다.
\[\mu = \binom{\mu_{a}}{\mu_{b}}\qquad 식(2.66) \]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;공분산 행렬 \(\Sigma \)는 다음처럼 주어진다.&lt;/li&gt;
&lt;/ul&gt;

\[\Sigma =  \begin{pmatrix}
    \Sigma_{aa} &amp;amp; \Sigma_{ab} \\
    \Sigma_{ba} &amp;amp; \Sigma_{bb} \\
    \end{pmatrix} \qquad 식(2.67)\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;여기서 \(\Sigma^{-1} = \Sigma \) (공분산 행렬이 대칭행렬이니까)는 \(\Sigma_{aa}\)와 \(\Sigma_{bb}\)가 대칭 행렬이라는 것을 암시한다.이 경우 \(\Sigma_{ba} = \Sigma_{ab}^{T}\)다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;수식의 편의를 위해 공분산 행렬의 역행렬 (정밀도 행렬)을 다음처럼 정의한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ \Lambda \equiv \Sigma^{-1} \qquad 식(2.68)\]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;분할된 형태의 정밀 행렬은 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

\[\Lambda = \begin{pmatrix}
\Lambda_{aa} &amp;amp;\lambda_{ab}  \\
\Lambda_{ba} &amp;amp;\lambda_{bb}  \\
\end{pmatrix} \qquad 식(2.69)\]

&lt;ul&gt;
  &lt;li&gt;마찬가지로 \(\Lambda_{ab}^{T} = \Lambda_{ba} \)가 성립한다&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\Lambda_{aa}\)가 \(\Sigma_{aa}\)의 역행렬이 아님을 주의하자!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;우리가 찾고자 하는 것은 조건부 분포 \(p(x_{a} \mid x_{b})\)이다.
    &lt;ul&gt;
      &lt;li&gt;확률 곱의 법칙에 따라서 결합 분포 \(p(x)  = p(x_{a},x_{b}) \)로부터 계산할 수 있다.&lt;/li&gt;
      &lt;li&gt;\(x_{b}\)를 관측된 값으로 고정하고 그때의 \(x_{a}\)애 해당하는 확률분포를 구할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 식을 다시 살펴보자
\[ -\frac{1}{2}(x -\mu)^{t}\Sigma^{-1}(x -\mu) \]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;식(2.65),식(2.66),식(2.69)의 식을 이용하여 전개하면 다음과 같다.&lt;/p&gt;

    &lt;p&gt;\[ -\frac{1}{2}(x -\mu)^{t}\Sigma^{-1}(x -\mu) =  \]
  \[-\frac{1}{2}(x_{a}-\mu_{a})^{T}\Lambda_{aa}(x_{a}-\mu_{a})-\frac{1}{2}(x_{a}-\mu_{a})^{T}\Lambda_{ab}(x_{b}-\mu_{b}) \]
  \[\qquad \qquad \quad -\frac{1}{2}(x_{b}-\mu_{b})^{T}\Lambda_{ba}(x_{a}-\mu_{a})-\frac{1}{2}(x_{b}-\mu_{b})^{T}\Lambda_{bb}(x_{b}-\mu_{b}) \qquad 식(2.70) \]&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;\(x_{b}\)가 고정이니까 이 식은 \(x_{a}\)에 대해 이차식이다.&lt;/li&gt;
      &lt;li&gt;즉, 조건부 가우시안 분포 \(p(x_{a} \mid x_{b})\)의 이차식이다.&lt;/li&gt;
      &lt;li&gt;이 식을 통해 \(p(x_{a} \mid x_{b})\)는 가우시안 분포임을 알 수 있다.&lt;/li&gt;
      &lt;li&gt;가우시안 분포는 평균과 분산에 의해 결정되므로 이걸 구해보자.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;완전제곱식
    &lt;ul&gt;
      &lt;li&gt;완전제곱식을 쉬운 예시로 설명하면, \(x^{2} + 4x + 1\)의 형태를 \((x+2)^{2} - 3\)의 제곱의 형태, 즉 이차 형식으로 변환하는 것&lt;/li&gt;
      &lt;li&gt;위의 식을 완전제곱식으로 변경하면 다음과 같다.&lt;/li&gt;
    &lt;/ul&gt;
    &lt;figure&gt;
  &lt;a href=&quot;/PRML/33.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/33.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
  &lt;/figure&gt;

    &lt;ul&gt;
      &lt;li&gt;여기서 const는 x에 대한 독립적인 항들을 의미한다는 것을 알 수 있다.&lt;/li&gt;
      &lt;li&gt;x의 이차항에 해당하는 계수들의 행렬과 공분산 행렬의 역행렬 \(\Sigma^{-1}\)이 같다.&lt;/li&gt;
      &lt;li&gt;x의 일차함의 계수들과 \(\Sigma^{-1}\mu\)가 같다.&lt;/li&gt;
      &lt;li&gt;이를 식(2.70)에 적용해서 \(\mu \)를 구해보자!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;식(2.70)의 평균은 \( \mu_{a \mid b}\), 공분산은 \(\Sigma_{a \mid b}\)이다.&lt;/li&gt;
  &lt;li&gt;식(2.70)dmf \(x_{a}\)에 대해 전개하여 \(x_{a}\)의 이차식에 해당하는 항만 골라내면 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

\[-\frac{1}{2}\textbf{x}_{a}^{T}\Lambda_{aa}\textbf{x}_{a} \qquad 식(2.72)\]

&lt;ul&gt;
  &lt;li&gt;따라서 공분산은 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[\Sigma_{a \mid b} = \Lambda_{aa}^{-1} \qquad 식(2.73)\]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(x_{a}\)에 대해 일차식에 해당하는 항만 추려내면 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

\[\textbf{x}_{a}^{T} \begin{Bmatrix} \Lambda_{aa} \mu_{a} - \Lambda_{ab}(\textbf{x}_{b} - \mu_{b}) \end{Bmatrix} \qquad 식(2.74)\]

&lt;ul&gt;
  &lt;li&gt;식 (2.71)에서의 우리의 논의에 따라 이 표현식에서 \(x_{a}\)의 계수는 \(\Sigma_{a \mid b}^{T} \mu_{a \mid b} \)와 동일해야 한다. 따라서 다음을 구할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
&lt;a href=&quot;/PRML/34.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/34.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;식(2.73)과 위의 식은 조건부 분포 \(p(x_{a},x_{b})\)의 분할 정밀 행렬에 대한 식으로 표현되었다. 해당 결과값은 분할 공분산 행렬의 식으로도 표현 가능하다.&lt;/li&gt;
  &lt;li&gt;이를 위해서는 분할 행렬의 역행렬에 대한 다음의 성징을 활용해야 한다.&lt;/li&gt;
&lt;/ul&gt;

\[\begin{pmatrix}
 A&amp;amp; B \\
 C&amp;amp;  D\\
\end{pmatrix}^{-1} = \begin{pmatrix}
M &amp;amp; -MBD^{-1} \\
-D^{-1}CM &amp;amp; D^{-1}CMBD^{-1} \\
\end{pmatrix} \qquad 식(2.76)\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;여기서 M은 다음과 같이 정의되었다.
\[ M = (A - BD^{-1}C)^{-1} \qquad 식(2.77)\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;여기서 \(M^{-1}\)은 슈어 보수행렬(Schur complement)라고 불리는 행렬을 의미한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 식을 다음 정의에 대입해서 풀 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[\begin{pmatrix}
 \Sigma_{aa}&amp;amp; \Sigma_{ab} \\
 \Sigma_{ba}&amp;amp; \Sigma_{bb}\\
\end{pmatrix}^{-1} = \begin{pmatrix}
\Lambda_{aa} &amp;amp; \Lambda_{ab} \\
\Lambda_{ba}&amp;amp; \Lambda_{bb} \\
\end{pmatrix} \qquad 식(2.78)\]

&lt;p&gt;여기에 식(2.76)을 적용하면 다음을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;\[\Lambda_{aa} = (\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1} \qquad 식(2.79)  \]&lt;/p&gt;

&lt;p&gt;\[\Lambda_{ab} = -(\Sigma_{aa} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\Sigma_{ab}\Sigma_{bb}^{-1} \qquad 식(2.80)  \]&lt;/p&gt;

&lt;p&gt;이 식들을 바탕으로 조건부 분포 \(p(x_{a} \mid x_{b})\)의 평균과 공분산에 대한 식을 최종적으로 공분산으로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \mu_{a \mid b}  = \mu_{a} + \Sigma_{aa}\Sigma_{bb}^{-1}(x_{b} - \mu_{b}) \qquad 식(2.81)\]
\[\Sigma_{a \mid b} = \Sigma_{aa} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba} \qquad 식(2.82)\]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;식에 대한 설명을하면,
    &lt;ul&gt;
      &lt;li&gt;평균은 \(x_{b}\)에 대한 일차식며,&lt;/li&gt;
      &lt;li&gt;공분산은 \(x_{b}\)에 대해 독립적임을 확인할 수 있다.&lt;/li&gt;
      &lt;li&gt;이것이 선형 가우시안 모델의 예시다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;조건부 분포 \(p(x_{a} \mid x_{b})\)에 대한 평균과 공분산 값은 \(x_{a} \)와 \(x_{b} \)에 대한 공분산 행렬이 아닌 부분 정확도 행렬로 표현하는게 더 간결하다는 것을 알 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결론적으로,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(p(x_{a} \mid x_{b})\)가 가우시안 분포를 따르는 경우, \(p(x_{a} \mid x_{b} )\)도 가우시안 분포를 따르게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;주변 가우시안 분포에 대한 내용부터 다음에 이어 설명하겠습니다.&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-2.3-The-Gaussian-Distribution/&quot;&gt;2.3 The Gaussian Distribution&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 04, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[이항 분포와 다항 분포]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/blog/%E1%84%8B%E1%85%B5%E1%84%92%E1%85%A1%E1%86%BC-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%91%E1%85%A9%E1%84%8B%E1%85%AA-%E1%84%83%E1%85%A1%E1%84%92%E1%85%A1%E1%86%BC-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%91%E1%85%A9/" />
  <id>http://localhost:4000/blog/이항 분포와 다항 분포</id>
  <published>2022-03-03T07:08:50+09:00</published>
  <updated>2022-03-03T07:08:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;앞서 우리는 &lt;a href=&quot;https://jjomaeng.github.io/articles/PRML-2.1-Binary-Variables/&quot;&gt;베르누이 분포(Bernoulli Distribution)&lt;/a&gt;에 대해 살펴봤다&lt;/p&gt;

&lt;p&gt;들어가기에 앞서 베르누이 과정을 정리하면,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;베르누이 과정은 결과가 오직 두가지인 확률 변수의 과정이다.&lt;/li&gt;
  &lt;li&gt;쉬운 예시로 동전 던지기를 생각하면 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;이항-분포binomial-distribution&quot;&gt;이항 분포(binomial distribution)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;이항 분포는 베르누이 과정을 따르는 확률 분포이다.&lt;/li&gt;
  &lt;li&gt;성공 확률이 p인 베르누이 시행을 n번 반복 시행할 때 성공 횟수를 나타내는 확률 변수 x의 분포를 이항 분포라고 한다.&lt;/li&gt;
  &lt;li&gt;즉, 결과가 오직 두 개인 시행을 여러번 시도할 때 사용한다고 볼 수 있다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이항 분포의 확률 질량 함수는 다음과 같다.
\[ f(x) = b(x,n,p) = \binom{n}{x}p^{x}q^{n-x}\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;식에 대해 자세히 설명하면,
    &lt;ul&gt;
      &lt;li&gt;x : 성공 횟수&lt;/li&gt;
      &lt;li&gt;n : 전체 시행(반복) 횟수&lt;/li&gt;
      &lt;li&gt;p : 성공할 확률&lt;/li&gt;
      &lt;li&gt;q : 실패할 확률 = (1 - p)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;또한 전체 확률은 항상 1이어야 하기 때문에 x에 0부터 n까지 넣어서 모두 더하면 1이 된다.
\[ \sum_{0}^{n}b(x;n,p) = 1\]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;이항-분포의-기댓값과-분산&quot;&gt;이항 분포의 기댓값과 분산&lt;/h3&gt;
&lt;p&gt;\[ E(x) = np\]
\[ Var(x) = np(1-p)\]&lt;/p&gt;

&lt;h3 id=&quot;다항-분포multinomial-distribution&quot;&gt;다항 분포(Multinomial Distribution)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;다항 분포는 여러 개의 값을 가질 수 있는 독립 확률 변수에 대한 확률 분포를 가리킨다.&lt;/li&gt;
  &lt;li&gt;즉, 이항 분포에서 도출 가능한 결과가 3개 이상인 분포이다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;어떤 시행에서 𝑘가지의 값이 나타날 수 있고, 그 값이 나타날 확률을 각각 \(𝑝_{1},𝑝_{2},…,𝑝_{𝑘} \)라고 할 때 𝑛번의 시행에서 𝑖번째 값이 \(𝑥_{𝑖} \)회 나타날 확률, 즉 다항분포의 확률질량함수는 아래와 같다.
\[ p(x_{1},x_{2},….,x_{k};n,p_{1},…,p_{k}) = \frac{n!}{x_{1}!,x_{2}!,….,x_{k}!}p_{1}^{x_{1}}p_{2}^{x_{2}}\cdots p_{k}^{x_{k}} \]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;여러 번의 시행에서 결고가 몇 번 나왔는지를 나타내는 x를 모두 더한 값은 n을 만족해야 한다.
\[ \sum_{i=1}^{k}x_{i} = n\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;또한, 여러 개의 결과의 확률을 모두 더하면 1을 만족해야 한다.
\[ \sum_{i=1}^{k}p_{i} = 1\]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;이항-분포와-다항-분포의-이해&quot;&gt;이항 분포와 다항 분포의 이해&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;다항 분포의 식을 자세히 보면 이항 분포의 일반화임을 알 수 있다.&lt;/li&gt;
  &lt;li&gt;마지막으로, 비교를 위해 깡통에 동전을 던져서 넣는 게임을 생각해보자
    &lt;ul&gt;
      &lt;li&gt;동전 1개를 던져서 1개의 깡통에 들어가는 확률을 p라고 하면, 이 시행 1번이 베르누이 시행이 된다.&lt;/li&gt;
      &lt;li&gt;동전을 n개 던져서 1개의 깡통에 들어가는 동전의 갯수를 X라고 하면 이항 분포를 따른다 \( X \sim Bin(n,p) \)&lt;/li&gt;
      &lt;li&gt;k개의 깡통에 동전 n개를 던진다면 이게 다항분포가 되는 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/blog/%E1%84%8B%E1%85%B5%E1%84%92%E1%85%A1%E1%86%BC-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%91%E1%85%A9%E1%84%8B%E1%85%AA-%E1%84%83%E1%85%A1%E1%84%92%E1%85%A1%E1%86%BC-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%91%E1%85%A9/&quot;&gt;이항 분포와 다항 분포&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 03, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[2.2 Multinomial Variables]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-2.2-Multinomial-Variables/" />
  <id>http://localhost:4000/articles/[PRML]2.2 Multinomial Variables</id>
  <published>2022-03-03T04:01:50+09:00</published>
  <updated>2022-03-03T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;p&gt;서로 다른 K개의 값들 중 하나를 취할 수 있는 이산 변수를 표현하기 위해 &lt;b&gt;원 핫 인코딩&lt;/b&gt;을 많이 사용한다.
원 핫 인코딩에서는 각각의 변수가 K차원의 벡터 X로 나타내며, \( x_{k} \)값들 중 하나는 1, 나머지 값들은 0으로 설정된다. 예를 들어 4개의 상태를 가질 수 있는 변수가 \( x_{3} = 1\) 이라는 상태를 가졌다면 해당 변수 X를 다음과 같이 표현할 수 있다.
\[ \textbf{X} = (0,0,1,0)^{T}\]&lt;/p&gt;

&lt;p&gt;이러한 벡터들은 \( \sum_{k = 1}^{K} x_{k} = 1\)이라는 성질을 만족한다. 만약 우리가 \( x_{k} = 1\)이 될 확률을 \( \mu_{k} \)라고 한다면, X의 분포는 다음과 같이 주어진다.&lt;/p&gt;

&lt;p&gt;\[ p(\textbf{X} \mid \mu) = \prod_{k = 1}^{K} \mu_{k}^{x_{k}}  \]&lt;/p&gt;

&lt;p&gt;여기서 \( \textbf{ μ } = (\mu_{1},…..,\mu_{K})^{T}\)이며, 매개변수 \( \mu_{k}\)는 \( \mu_{k} \geq 0\)과 \( \sum_{k} \mu_{k} =1 \)이라는 성질을 만족시켜야 한다. (\( \mu_{k} \)는 확률이기 때문에)&lt;/p&gt;

&lt;p&gt;이 분포에 대해서 다음의 두 가지를 쉽게 증명할 수 있다.
\[ \sum_{\textbf{X}} p(\textbf{X} \mid \textbf{ μ }) = \sum_{k = 1}^{K} \mu_{k} = 1\]&lt;/p&gt;

&lt;p&gt;\[ E(\textbf{X} \mid \textbf{μ}) = \sum_{\textbf{X}} p(\textbf{X} \mid \textbf{ μ })\textbf{X} = (\mu_{1},…,\mu_{k})^{T} = \textbf{μ} \]&lt;/p&gt;

&lt;p&gt;N개의 독립적인 관측값 \( \textbf{X}&lt;em&gt;{1},…,\textbf{X}&lt;/em&gt;{N} \)을 가진 데이터 집합 D를 고려해 보자. 해당 가능도 함수는 다음의 형태를 가진다.
\[ p(D \mid \textbf{μ} ) = \prod_{n=1}^{N}\prod_{k = 1}^{K} \mu_{k}^{x_{n,k}} = \prod_{k=1}^{K}\mu_{k}^{(\sum_{n}x_{n,k})} = \prod_{k=1}^{K}\mu_{k}^{m_{k}}\]
가능도 함숫값이 K값을 통해서만 N개의 데이터 포인트와 연관되어 있음을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ m_{k} = \sum_{n}x_{n,k}\]&lt;/p&gt;

&lt;p&gt;위의 식은 \( x_{k} = 1\)인 관측값의 숫자에 해당한다. 위의 식은 분포의 &lt;b&gt;충분 통계량&lt;/b&gt;이라고 한다.&lt;/p&gt;

&lt;p&gt;\(\textbf{ μ } \)값의 최대 가능도 해를 찾기 위해서는 \( \mu_{k} \)의 합이 1이어야 한다는 제약 조건하에서 \(lnp(D \mid \textbf{ μ }  )\)의 최댓값을 찾아야 한다. 이를 위해서는 라그랑주 승수 \(\lambda \)를 사용해서 다음 식의 최댓값을 구하면 된다.&lt;/p&gt;

&lt;p&gt;\[L(\mu,m,\lambda) = \sum_{k = 1}^{k}m_{k}ln(\mu_{k}) + \lambda(\sum_{k=1}^{K}\mu_{k}-1) \]&lt;/p&gt;

&lt;p&gt;먼저, \( \mu_{k}\)에 대해 미분한 뒤 이를 0으로 설정한다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/25.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/25.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;그 다음 \(\lambda\)에 대해 미분하여 0이 되는 식을 구한다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/26.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/26.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;따라서 최대 가능도의 해는 다음의 형태를 띠게 된다.&lt;/p&gt;

&lt;p&gt;\[ \mu_{k}^{ML} = \frac{m_{k}}{N}\]&lt;/p&gt;

&lt;p&gt;이는 N개의 관측값 중 \(x_{k} = 1 \)인 경우의 비율이다.&lt;/p&gt;

&lt;p&gt;매개변수 \( \textbf{μ} \)와 관측값 숫자 N에 의해 결정되는 수량 \( m_{1},….,m_{k}\)의 결합 분포를 고려해 보자.
가능도 함수에 따라 이는 다음의 형태를 띠게 된다.&lt;/p&gt;

&lt;p&gt;\[ Mult(m_{1},m_{2},…,m_{k} \mid \textbf{μ},N) = \binom{N}{m_{1}m_{2}…m_{K}}\prod_{k=1}^{K}\mu_{k}^{m_{k}}\]&lt;/p&gt;

&lt;p&gt;위의 식을 &lt;b&gt;다항 분포&lt;/b&gt;라고 한다.( &lt;a href=&quot;https://jjomaeng.github.io/blog/이항-분포와-다항-분포/&quot;&gt;이항 분포와 유사한 형태를 띠고 있다는 것을 확인할 수 있다! 이에 대한 자세한 설명은 여기에 작성하였습니다.&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;정규화 계수는 N개의 물체를 각각 \(m_{1},…,m_{K} \)의 수량을 가지는 K개의 집단으로 나누는 가짓수에 해당하며 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ \binom{N}{m_{1}m_{2}…m_{K}} = \frac{N!}{m_{1}!m_{2}!…m_{k}!}\]&lt;/p&gt;

&lt;p&gt;변수 \( m_{k} \)는 다음의 제약 조건을 가진다&lt;/p&gt;

&lt;p&gt;\[ \sum_{k = 1 }^{K}m_{k} = N\]&lt;/p&gt;

&lt;h4 id=&quot;디리클레-분포&quot;&gt;디리클레 분포&lt;/h4&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-2.2-Multinomial-Variables/&quot;&gt;2.2 Multinomial Variables&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 03, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[2.1 Binary Variables]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-2.1-Binary-Variables/" />
  <id>http://localhost:4000/articles/[PRML]2.1 Binary Variables</id>
  <published>2022-03-02T04:01:50+09:00</published>
  <updated>2022-03-02T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;p&gt;이 장에서 논의할 분포의 역할들 중 하나는 한정된 수의 관찰 집합 x1,….,xn이 주어졌을 때 확률 변수 x의 확률 분포 p(x)를 모델링하는 것이다. 이를 &lt;b&gt;밀도 추정 문제 &lt;/b&gt; 라고 한다.
이 장의 목표를 위해서 데이터 포인트들은 독립적이며, 동일하게 분포되어 있다고 가정할 것이다.&lt;/p&gt;

&lt;p&gt;우선, 이산 확률 변수의 이항 분포와 다항 분포를 살펴보고 그 다음으로 연속 확률 변수의 가우시안 분포에 대해 논의할 것이다. 이 분포들은 매개변수 분포의 예다. 
이러한 모델을 밀도 추정 문제에 적용하기 위해서는 관찰된 데이터 집합을 바탕으로 적절한 매개변숫값을 구하는 과정이 필요하다.&lt;br /&gt;
두 가지 관점에서 살펴보자&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;빈도적 관점 : 어떤 특정 기준을 최적화하는 방법으로 매개변수를 찾는다. 최적화 기준의 예로 가능도 함수가 있다.&lt;/li&gt;
  &lt;li&gt;베이지안 관점 : 매개변수에 대한 사전 분포를 바탕으로 관측된 데이터 집합이 주어졌을 때의 해당 사후분포를 계산한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;켤레 사전 확률이 중요한 역할을 한다는 것도 살펴볼 것이다. 켤레 사전 확률은 사후 확률이 사전 확률과 같은 함수적 형태를 띠도록 만들어준다. 그 결과 , 베이지안 분석이 매우 단순해진다.&lt;/p&gt;

&lt;p&gt;매개변수적인 접근법의 한계점도 존재한다. 분포가 특정한 함수 형태를 띠고있다고 가정하지만 몇몇 적용 사례의 경우에는 이 가정이 적절하지 않다. 이런 경우, 비매개변수적 밀도 추정 방식을 대안으로 활용할 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;이산-확률-변수&quot;&gt;이산 확률 변수&lt;/h4&gt;

&lt;p&gt;하나의 이진 확률 변수 \( x \in (0,1) \)을 고려해 보자. 0과 1이 나올 확률이 동일하지 않다고 가정하면 이때 x = 1일 확률은 매개변수 \( \mu \)를 통해 다음과 같이 표현할 수 있다.
\[ p(x = 1 \mid \mu) = \mu (0 \leq \mu \leq 1)\]
\[ p(x = 0 \mid \mu ) = 1 - \mu \]&lt;/p&gt;

&lt;p&gt;따라서 x에 대한 확률은 다음의 형태로 적을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ Bern(x \mid \mu ) = \mu^{x}(1 - \mu)^{1-x} \]&lt;/p&gt;

&lt;p&gt;이것을 &lt;b&gt; 베르누이 분포 &lt;/b&gt;라고 한다. 베르누이 분포는 정규화되어 있으며, 그 평균과 분산이 다음과 같이 주어진다.&lt;/p&gt;

&lt;p&gt;\[ E(x) = \mu \]
\[ var(x) = \mu(1-\mu) \]&lt;/p&gt;

&lt;p&gt;x의 관측값이 데이터 집합 \( D = (x_{1},…..x_{N})\)이 주어졌다고 하자. 관측값들이 \( p(x \mid \mu)\)에서 독립적으로 추출되었다는 가정하에 \( \mu \)함수로써 가능도 함수를 구성할 수 있다.
\[ p(D \mid \mu ) = \prod_{n=1}^{N}p(x_{n} | \mu) = \prod_{n=1}^{N}\mu^{x_{n}}(1-\mu)^{1 - x_{n}} \]&lt;/p&gt;

&lt;p&gt;빈도적 관점에서는 가능도 함수를 최대화하는 \( \mu \)를 찾아서 \( \mu \)값을 추정할 수 있다. 베르누이 분포의 경우 로그 가능도 함수는 다음으로 주어진다.&lt;/p&gt;

&lt;p&gt;\[ lnp(D \mid \mu ) = \sum_{n=1}^{N}lnp(x_{n} \mid \mu) = \sum_{n=1}^{N}(x_{n}ln\mu + (1-x_{n})ln(1-\mu)) \]&lt;/p&gt;

&lt;p&gt;로그 가능도함수는 오직 관측값들의 합인 \(\sum_{n}x_{n}\)을 통해서만 N개의 관측값 \(x_{n} \)과 연관된다는 점에 주목할 필요가 있다. 이 합은 &lt;b&gt; 충분 통계량 &lt;/b&gt;의 예시 중 하나다.&lt;/p&gt;

&lt;p&gt;\( lnp(D \mid \mu )\)을 \(\mu \)에 대해 미분하고 이를 0과 같다고 놓으면 다음과 같은 최대 가능도 추정값을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n}\]&lt;/p&gt;

&lt;p&gt;이는 &lt;b&gt; 표본 평균 &lt;/b&gt;이라고도 불린다. 데이터에서 x = 1인 관찰값의 수를 m이라고 하면 다음의 형태로 다시 적을 수 있다. 
\[ \mu_{ML} = \frac{m}{N} \]&lt;/p&gt;

&lt;p&gt;즉, 최대가능도 체계하에에서 동전으로 예시를 들면, 동전의 앞면이 나올 확률은 데이터 집합에서 앞면이 나온 비율로 주어지게 되는 것이다.&lt;/p&gt;

&lt;p&gt;하지만 최대가능도를 사용할 경우 과적합이 발생할 수 있다.&lt;/p&gt;

&lt;p&gt;동전을 세 번 던졌는데 세 번 다 앞면이 나온 경우를 생각해보자. 그러면 N = m = 3이고 따라서 \( \mu_{ML} = 1 \)이 된다( 이 경우 앞으로 던지는 모든 동전이 앞면으로 나올 것이라고 예측하는 것이다! ) 이것이 실제로 최대 가능도를 사용했을 경우 발생할 수 있는 극단적인 사례다.( 이후에 사전 분포를 바탕으로 나은 결과를 도출할 수 있다.)&lt;/p&gt;

&lt;p&gt;크기 N의 데이터가 주어졌을 때 x = 1인 관측값의 수 m에 대해서도 분포를 생각할 수 있다. 이를 &lt;b&gt; 이항 분포 &lt;/b&gt;라 한다. 위에서 구한 가능도 함수로부터 이항 분포는 \( \mu^{m}(1 - \mu)^{N - M}\)에 비례한다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;정규화 계수를 구하기 위해서 동전 던지기로 예시를 들면, 동전 던지기를 N번 했을 때 앞면이 m번 나올 수 있는 가능한 모든 가짓수를 구해야 한다. 따라서 이항 분포를 다음과 같이 적을 수 있다.
\[Bin(m \mid N,\mu) = \binom{N}{m}\mu^{m}(1-\mu)^{N-m} \]&lt;/p&gt;

&lt;p&gt;여기서 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ \binom{N}{m} \equiv \frac{N!}{(N-m)!m!} \]&lt;/p&gt;

&lt;p&gt;\( \binom{N}{m} \)은 N개의 물체들 중 m개의 물체를 선별하는 가짓수를 구한 것이다. 이해를 위해 N = 10이고 \(\mu \)= 0.25일 때의 이항 분포의 히스토그램은 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/8.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/8.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;베타-분포&quot;&gt;베타 분포&lt;/h4&gt;

&lt;p&gt;이미 살펴본 방법들은 데이터 수가 적을 때 심각한 과적합이 일어나기 쉽다(동전 던지기 예시를 생각해보자!)&lt;/p&gt;

&lt;p&gt;이 문제에 베이지안적으로 접근하기 위해서는 매개변수 \( \mu \)에 대한 사전 분포 \( p(\mu) \)를 도입하는 것이 필요하다.&lt;/p&gt;

&lt;p&gt;가능도 함수가 \( \mu^{x}(1-x)^{1-x}\)의 형태를 가지는 인자들의 곱의 형태를 띠고 있다는 것에 주목해보면, 만약 \(\mu \)dhk \( (1 - \mu) \)의 거듭제곱에 비례하는 형태를 사전 분포에 선택한다면, 사전 확률과 가능도 함수의 곱에 비례하는 사후 분포 역시 사전 분포와 같은 함수적 형태를 가지게 될 것이다. 이러한 성질을 &lt;b&gt; 켤레성&lt;/b&gt;이라고 한다. ( 켤레성에 대한 이해를 돕고자 켤레의 수학적 의미를 설명하면, 두 개의 점,선,도형,수 등이 서로 대칭이거나 보완적인 관계에 있는 경우 켤레라고 한다.)&lt;/p&gt;

&lt;p&gt;지금까지의 논의를 바탕삼아 사전 분포로 베타 분포를 사용할 것이다.( &lt;a href=&quot;https://jjomaeng.github.io/blog/베타-분포/&quot;&gt;왜 베타 분포를 사용하는 지에 대한 자세한 설명은 여기에 작성하였습니다!&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;베타 분포는 다음의 형태를 가진다.
\[ Beta(\mu \mid a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\]&lt;/p&gt;

&lt;p&gt;\( \Gamma(x) \)는 다음과 같이 정의된다. (&lt;a href=&quot;https://jjomaeng.github.io/blog/감마-분포/&quot;&gt;감마 함수에 대한 자세한 내용은 여기에 작성하였습니다!&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;\[ \Gamma(x) = \int_{0}^{\infty } u^{x-1} e^{-1} du \]&lt;/p&gt;

&lt;p&gt;베타 분포의 계수들은 베타 분포가 정규화되도록 한다.&lt;/p&gt;

&lt;p&gt;\[ \int_{0}^{1}Beta(\mu \mid a,b) d\mu = 1 \]&lt;/p&gt;

&lt;p&gt;베타 분포의 평균과 분산은 다음과 같이 주어진다.&lt;/p&gt;

&lt;p&gt;\[ E(\mu) = \frac{a}{a+b} \]
\[ var(\mu) = \frac{ab}{(a+b)^{2}(a+b+1)} \]&lt;/p&gt;

&lt;p&gt;매개변수 a와 b는 이들이 매개변수 \( \mu \)의 분포를 조절하기 때문에 &lt;b&gt;초매개변수&lt;/b&gt;라고 불린다.&lt;/p&gt;

&lt;p&gt;이제 베타 사전 분포와 이항 가능도 함수를 곱한 후 정규화를 시행함으로써 \( \mu \)의 사후 분포를 구할 수 있다.
\( \mu \)와 관련되어 있는 인자들만 남기면 사후 분포가 다음의 형태를 가지게 되는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ p(\mu \mid m,l,a,b) \propto \mu^{m+a-1}(1 - /m)^{l+b-1}\]&lt;/p&gt;

&lt;p&gt;여기서 l = N -m이며 동전 던기기 예시에서 ‘뒷면’의 개수에 해당한다.&lt;/p&gt;

&lt;p&gt;사후 분포의 식은 사전 분포와 \(\mu \)에 대해서 같은 함수적 종속성을 가지고 있다는 것을 확인할 수 있다. 이 사실이 가능도 함수에 대해서 사전 분포가 켤레성을 가지고 있다는 것을 반영한다. 
실제로 사후 분포는 또 다른 베타 분포일 뿐이다.
그러면 사후 분포의 정규화 계수는 베타 분포의 식을 참고하여 구할 수 있다. 그 결과 다음을 얻게 된다.&lt;/p&gt;

&lt;p&gt;\[ p(\mu \mid m,l,a,b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)} \mu^{m+a-1}(1-\mu)^{l+b-1}\]&lt;/p&gt;

&lt;p&gt;x = 1인 값이(예를 들어 앞면의 수) m개 있고 x = 0인 값 ㅣ개가 있는 데이터 집합을 관찰한 결과, 사전 분포와 비교했을 떄 사후 분포에서는 a의 값이 m만큼, b의 값이 l만큼 증가하는 것을 확인할 수 있다.
이 사실로부터 사전 분포의 초매개변수 a와b를 각각 x = 1, x = 0인 경우에 대한 &lt;b&gt;유효 관찰수&lt;/b&gt;로 해석할 수 있다.(a와 b는 반드시 정수가 아니어도 된다.)&lt;/p&gt;

&lt;p&gt;만약 우리가 추가적으로 관측 데이터를 얻게 되면 지금의 사후 분포가 새로운 사전 분포가 될 수 있다. 
매번 업데이트 단계에서 새로운 관측값에 해당하는 가능도 함수를 곱하고 그 다음에 정규화를 시행해서 새로운 수정된 사후 분포를 &lt;b&gt;순차적&lt;/b&gt;으로 얻는 것이다.
각 단계에서 사후 분포는 x = 1과 x = 0에 해당하는 관측값들의 전체 숫자가 새로운 a와 b값으로 주어지는 베타 분포에 해당한다. x = 1인 새로운 관측값이 주어졌을 때는 단순히 a값을 1 증가시키고 x = 0인 관측값이 새로 주어졌을 때는 b값을 1 증가시키면 된다.
이 과정의 한 단계를 그림으로 보면 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/10.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/10.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;이는 사전 분포나 가능도 함수의 선택과는 상관없이 오직 데이터가 독립적이고 동이랗게 분포되었다는 것에만 의존적이다.
순차적인 방법론은 관측값을 한 번에 하나씩, 혹은 한 번에 적은 수 만큼 사용한다.
사용한 관측값들을 다음 관측값을 사용하기 전에 버린다. 
이는 실시간 학습에 적용할 수 있으며, 큰 데이터 집합에도 사용 가능하다.&lt;/p&gt;

&lt;p&gt;만약 우리의 목표가 다음 시도의 결과값을 가장 잘 예측하는 것이라면, 관측 데이터 집합 D가 주어진 상황하에서 x의 예측 분포를 계산해야 한다.
확률의 합과 곱의 법칙에 따라서 이는 다음 형태를 띤다.&lt;/p&gt;

&lt;p&gt;\[ p(x = 1 \mid D) = \int_{0}^{1}p(x=1 \mid \mu)p(\mu \mid D)d\mu = \int_{0}^{1}\mu p(\mu \mid D )d\mu = E(\mu \mid D)\]&lt;/p&gt;

&lt;p&gt;사후 분포 \(p(\mu \mid D)\)에 대한 결과 식과 베타 분포의 평균에 대한 식을 이용해서 다음을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ p(x = 1 \mid D) = \frac{m+a}{m_a+l+b}\]&lt;/p&gt;

&lt;p&gt;전체 관측값(실제 관측값과 허구의 사전 관측값 둘 다)중에서 x = 1인 관측값의 비율로 해석할 수 있다.&lt;/p&gt;

&lt;p&gt;데이터 집합이 무한이 커서 \( m,l \rightarrow  \infty \)이 된다면 최대가능도의 결과값과 같아진다.
(베이지안 결과값과 최대 가능도의 결과값이 무한하게 큰 데이터 집합 하에서 동일한 것은 매우 일반적인 성향이다.)&lt;/p&gt;

&lt;p&gt;제한된 크기의 데이터 집합에서 \(\mu \)의 사후 평균 값은 사전 평균값과 사건의 상대적 빈도수를 바탕으로 한 가능도 추정치 사이에 있게 된다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/9.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/9.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;위의 그림에서 관측값의 수가 늘어날수록 사후 분포가 더 날카롭고 뾰족해는 것을 확인할 수 있다. 이는 베타 분포의 분산에 대한 식의 결과에도 확인할 수 있다.
\( a \rightarrow  \infty \), 또는 \( b \rightarrow  \infty \)가 됨에 따라서 분산이 0에 가까워지는 것이다.&lt;/p&gt;

&lt;p&gt;더 많은 데이터를 관측할수록 사후 분포의 확실성의 정도가 꾸준히 감소하는 것은 평균적으로 베이지안 학습의 일반적인 성질이다.&lt;/p&gt;

&lt;p&gt;이를 확인하기 위해 베이지안 학습을 빈도적 관점에서 살펴보도록 하자.&lt;/p&gt;

&lt;p&gt;관측된 데이터 집합 D에 대해서 매개변수 \( \theta \)를 추정하는 일반적인 베이지안 추론 문제를 고려해 보자. 이 문제는 결합 분포 \( p(\theta ,D)\)로 표현 가능하다. 매개변수 \( \theta \)에 대한 기댓값은 다음과 같다.&lt;/p&gt;

&lt;p&gt;먼저, \( E_{\theta}(\theta) = E_{D}(E_{\theta}(\theta \mid D))\) 증명하면 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/19.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/19.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;수식으로 다시 정리하면,&lt;/p&gt;

&lt;p&gt;\[ E_{\theta}(\theta) = E_{D}(E_{\theta}(\theta \mid D))\]
\[E_{\theta}(\theta) \equiv \int p(\theta)\theta d\theta \]
\[E_{D}(E_{\theta}(\theta \mid D)) \equiv \int(\int \theta p(\theta \mid D)d\theta)p(D)dD \]&lt;/p&gt;

&lt;p&gt;이로부터 데이터가 생성된 원 분포에 대해 평균을 낸 \( \theta \)의 사후 평균값은 \( \theta \)의 사전 평균과 같다는 것을 알 수 있다. 이와 유사하게 다음도 증명할 수 있다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/20.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/20.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;수식으로 다시 정리하면,&lt;/p&gt;

&lt;p&gt;\[ var_{\theta }(\theta ) = E_{D}(var_{\theta}( \theta \mid D)) + var_{D}(E_{\theta}( \theta \mid D))\]&lt;/p&gt;

&lt;p&gt;위의 식의 왼쪽 변은 \(\theta \)의 사전 분산에 해당한다. 오른쪽 변의 첫 번째 항은 \(\theta \)의 사후 분산의 평균이며, 두 번째 항은 \(\theta \)의 사후 평균의 분산에 해당한다.
이 분산은 양의 값을 가진다. 따라서 이 결과에 따르면 평균적으로 \(\theta \)의 사후 분산은 사전 분산보다 작다는 것을 알 수 있다. 이 분산값의 감소치는 사후 평균의 분산값이 클수록 더 크게 된다.
(이러한 추세는 평균적으로만 옳으며 , 특정 관찰 집합에 대해서는 사후 분산이 사전 분산보다 클수도 있다.)&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-2.1-Binary-Variables/&quot;&gt;2.1 Binary Variables&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 02, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[1.6 Information Theory]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-1.6-Information-Theory/" />
  <id>http://localhost:4000/articles/[PRML]1.6 Information Theory</id>
  <published>2022-03-01T04:01:50+09:00</published>
  <updated>2022-03-01T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;p&gt;이번에는 패턴 인식과 머신 러닝 테크닉을 이해하는 데 있어서 중요한 역할을 하게 될 또 하나의 이론인 정보 이론에 대해 살펴보고자 한다.
먼저, 엔트로피를 포함한 정보 이론의 중요한 개념들에 대해 살펴보자.&lt;/p&gt;

&lt;p&gt;이산 확률 변수 x를 고려했을 때, x의 값을 학습하는 데 있어서 정보의 양은 ‘놀라움의 정도’라고 생각할 수 있을 것이다.
따라서 우리가 사용하게 될 정보량의 측정 단위는 확률 분포 p(x)에 종속적이게 된다.&lt;/p&gt;

&lt;p&gt;지금부터 p(x)에 대해 단조 함수인 정보량을 표현하는 함수 h(x)에 대해 살펴보도록 하자.
서로 연관되어 있지 않은 두 사건 x와 y를 고려해 보면, 이 경우 x와 y가 함께 일어났을 때 얻는 정보량은 각자의 사건이 따로 일어났을 때 얻는 정보량의 합이 될 것이다. 따라서 h(x,y) = h(x) + h(y)가 된다. 연관되어 있지 않은 두 사건은 통계적으로 독립적이며 따라서 p(x,y) = p(x)p(y)다. 이 관계로부터 h(x)는 p(x)의 로그에 해당하는 것을 보일 수 있다. 이에 따라 다음 식을 얻게 된다.
\[h(x) = -log_{2}p(x) \]&lt;/p&gt;

&lt;p&gt;사건 x의 확률이 낮을수록 그로부터 얻을 수 있는 정보량은 크다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;송신자가 어떤 확률 변수의 값을 수신자에게 전송하고자 하는 상황을 가정해 보자.
전송에 필요한 정보량의 평균치는 p(x)에 대해 위의 식을 통해 기대값을 구함으로써 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;\[ H(X) = -\sum_{x}p(x)log_{2}p(x)  \]&lt;/p&gt;

&lt;p&gt;이 값이 바로 확률 변수의 x의 엔트로피다.&lt;/p&gt;

&lt;p&gt;(이제부터는 엔트로피를 정의하는 데 있어서 자연 로그를 사용하도록 하겠다.)
이 장에서는 엔트로피를 확률 변수의 상태를 결정짓는 데 필요한 정보량의 평균으로 정의하였다. 다음의 관점으로 엔트로피를 이해해보자.
N개의 동일한 물체가 몇 개의 통 안에 담겨 있다고 가정해 보자. 이때 i번쨰 통 안의 ni개의 물체가 담기도록 할 것이다. 물체를 통 안에 담는 가짓수는 N!개의 방법이있다. 하지만 한 통 안에서 물체들이 어떤 순서로 놓여 있는지는 중요하지 않다. i 번째 통에는 물체를 정렬하기 위한 ni!가지 방법이 있을 것이고, 이에 따라 N개의 물체를 통에 넣는 가짓수는 다음과 같이 될 것이다.&lt;/p&gt;

&lt;p&gt;\[ W = \frac{N!}{\prod_{i}n_{i}!}\]&lt;/p&gt;

&lt;p&gt;위의 식을 다중도라 한다. 엔트로피는 다중도의 로그를 취해서 적절한 상수로 나눈 것이다.&lt;/p&gt;

&lt;p&gt;비율 \( \frac{ni}{N} \)을 그대로 유지시킨 상태에서 N-&amp;gt;∞을 취하여 보자. 그리고 다음 식의 스털링 근사식을 적용해보자
\[lnN! \simeq N ln N - N \]&lt;/p&gt;

&lt;p&gt;그러면 다음을 얻게 된다.&lt;/p&gt;

&lt;p&gt;\[ H = -lim_{N -&amp;gt; \infty }\sum_{i}(\frac{n_{i}}{N})ln(\frac{n_{i}}{N})= -\sum_{i}p_{i}lnp_{i} (\sum_{i}n_{i} = N) \]&lt;/p&gt;

&lt;p&gt;여기서 pi는 물체가 i번째 통에 속하게 될 확률이다. 물리학 용어로 통 안의 물체들의 순서를 미시 상태라 하며, ni/N으로 표현되는 통 각각이 가지고 있는 물체의 숫자 비율을 일컬어 거시 상태라 한다. 다중도 W를 거시 상태의 가중치라 일컫기도 한다.&lt;/p&gt;

&lt;p&gt;각각의 통을 확률 변수 X의 상태 xi라고 해석할 수 있다. 여기서 p(X = xi) = pi다. 이 경우 확률 변수 X의 엔트로피는 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ H(p) = -\sum_{i}p(x_{i})lnp(x_{i}) \]&lt;/p&gt;

&lt;p&gt;이를 그림으로 확인해보자.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/24.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/24.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;그림에서 볼 수 있는 것처럼 분포\(p(x_{i}) \)가 몇몇 값에 뾰족하게 집중되어 있는 경우에는 상대적으로 낮은 엔트로피를 가지는 반면, 더 많은 값들 사이에 퍼져 있을 때는 높은 엔트로피를 가지게 된다.&lt;/p&gt;

&lt;p&gt;(중간 내용은 완전히 이해하지 못하여 이후에 정리하여 작성하겠습니다.)&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-1.6-Information-Theory/&quot;&gt;1.6 Information Theory&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on March 01, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[1.5 Decision Theory(2)]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-1.5-Decision-Theory(2)/" />
  <id>http://localhost:4000/articles/[PRML]1.5 Decision Theory(2)</id>
  <published>2022-02-28T04:01:50+09:00</published>
  <updated>2022-02-28T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;h4 id=&quot;추론과-결정&quot;&gt;추론과 결정&lt;/h4&gt;

&lt;p&gt;지금까지 분류 문제를 두 개의 단계로 나누어 보았다. 첫 번째는 추론 단계로 훈련 집단을 활용하여 \(p(C_{k} \mid x) \)에 대한 모델을 학습시키는 단계. 두 번째는 결정 단계로 학습된 사후 확률들을 이용해서 최적의 클래스 할당을 시행하는 것이다. 두 가지 문제를 한 번에 풀어내는 방식도 생각해 볼 수 있다. x가 주어졌을 때 결정값을 돌려주는 함수를 직접 학습시키는 것이다. 이러한 함수를 판별함수라고 한다.&lt;/p&gt;

&lt;p&gt;결정 문제를 푸는 데는 세 가지 다른 접근법이 있다. 이를 표로 정리하면 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/21.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/21.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;
&lt;p&gt;하지만 (c)방식을 사용할 경우 사후 확률을 알지 못하게 되는데, 사후 확률을 구하는 것에는 유의미한 여러 가지 이유가 있다.&lt;/p&gt;

&lt;p&gt;① 위험의 최소화
손실 행렬의 값들이 때때로 변하는 문제를 고려했을 때, 사후 확률을 알고 있다면 앞 서 설명한 기대 손실 식을 수정함으로써 쉽게 최소 위험 결정 기준을 구할 수 있다. 반면에 판별 함수만 알고 있을 경우에는 손실 행렬 값이 변할 때마다 훈련 집합을 활용하여 분류 문제를 새로 풀어야 할 것이다.&lt;/p&gt;

&lt;p&gt;② 거부 옵션
사후 확률을 알고 있으면 주어진 거부 데이터 포인트 비율에 대해 오분류 비율(기대 손실값)을 최소화하는 거부 기준을 쉽게 구할 수 있다.&lt;/p&gt;

&lt;p&gt;③ 클래스 사전 확률에 대한 보상
클래스의 불균형을 가지는 데이터에 대해, 각각의 클래스에서 같은 숫자 예시를 선택한 균형 잡힌 데이터 집합을 활용하여 더 정확한 모델을 찾을 수 있다. 하지만 그렇게 할 경우에는 우리가 훈련 집합을 변경한 것에 대한 보상을 적용해야 한다. 이러한 수정된 데이터 집합을 사용하여 사후 확률에 대한 모델을 찾아낸다고 가정할 경우, 사후 확률 식은 베이지안 정리로부터 사전 확률에 비례함을 알 수 있다. 이 예시의 경우 사전 확률로는 각 클래스의 비율을 사용할 수 있다. 따라서 인공의 균형 잡힌 데이터 집합에서 구한 사후 확률을 인공 데이터 집합의 클래스 비율로 나누고, 여기에 우리가 실제로 모델을 적용할 모수 집합의 클래스이 비율을 곱함으로써 수정된 사후 확률을 구할 수 있다. 최종적으로는 새 사후 확률이 합이 1이 되도록 정규화해야 한다. 하지만 직접 판별 함수를 구하는 방식으로 학습을 시킨 경우에는 이러한 수정이 불가능하다.&lt;/p&gt;

&lt;p&gt;④ 모델들의 결합
복잡한 응용 사례의 경우에는 하나의 큰 문제를 여러 개의 작은 문제로 나누어서 각각을 분리된 모듈로써 해결하는 것이 바람직한 경우가 있다. 이를 위한 한 가지 쉬운 방법은 각 클래스에 대해서 분리된 데이터 분포가 독립적이라고 가정하는 것이다. (분리된 데이터 \(X_{a},X_{b} \)라고 가정)&lt;/p&gt;

&lt;p&gt;\[ p(x_{A},x_{B} \mid C_{k}) = p(x_{A} \mid C_{k})p(x_{B} \mid C_{k})\]&lt;/p&gt;

&lt;p&gt;이는 조건부 독립의 성질의 예시다. 분포들이 클래스 Ck에 포함된다는 조건하에 독립적이기 때문이다. 이를 바탕으로 분리된 데이터들이 주어졌을 때의 사후 확률을 다음과 같이 구할 수 있다.
\[ p(C_{k}|x_{A},x_{B}) \propto p(x_{A},x_{B}|C_{k})p(C_{k}) \propto p(x_{A}|C_{k})p(x_{B}|C_{k})p(C_{k}) \propto \frac{p(C_{k}|x_{A})p(C_{k}|x_{B})}{p(C_{k})} \]&lt;/p&gt;

&lt;p&gt;클래스 사전 확률 \( p(C_{k}) \)가 필요한데, 이는 데이터 포인트들의 각 클래스별 비율에서부터 쉽게 유추하는 것이 가능하다. 그리고 최종적으로 사후 확률을 정규화하여 합이 1이 되도록 하는 과정이 필요하다.  여기서 보인 특정 조건부 독립 가정은 나이브 베이즈 모델의 예시다. 결합 확률 분포 \( p(x_{a},x_{b}) \)는 보통 나이브 베이즈 모델하에서는 인수분해가 되지 않는다. (조건부 독립 가정 없어도 데이터들을 결합시키는 방법은 이후에 나온다)&lt;/p&gt;

&lt;h4 id=&quot;회귀에서-손실-함수&quot;&gt;회귀에서 손실 함수&lt;/h4&gt;

&lt;p&gt;지금까지 분류 문제를 바탕으로 결정 이론에 대해 살펴보았다. 이제부터는 회귀 문제의 경우에 대해 살펴보도록 하자.
회귀 문제의 결정 단계에서는 각각의 x에 대해서 t의 추정값 y(x)를 선택해야 한다. 이 과정에서 손실 L(t,y(x))가 발생한다고 가정해보자. 그러면 평균(기대) 손실은 다음과 같이 주어진다.&lt;/p&gt;

&lt;p&gt;\[ E(L) = \int \int L(t,y(\textbf{x}))p(\textbf{x},t)dxdt \]&lt;/p&gt;

&lt;p&gt;손실 함수가 L(t,y(x)) = {y(X) - t}^2으로 주어지는 제곱 손실인 경우 기대 손실은 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ E(L) = \int \int (y(\textbf{x})-t)^{2}p(\textbf{x},t)dxdt \]&lt;/p&gt;

&lt;p&gt;우리의 목표는 E[L]을 최소화하는 y(x)를 선택하는 것이다. 만약 완벽하게 유연하게 함수 y(x)를 결정할 수 있다고 가정하면, 변분법을 적용해서 다음과 같이 적을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \frac{\delta E(L)}{\delta y(x)} = 2\int (y(\textbf{x}) - t)p(\textbf{x},t)dt = 0 \]&lt;/p&gt;

&lt;p&gt;y(x)에 대해서 해를 구하고 확률의 합과 곱의 법칙을 적용하면 다음을 얻게 된다.&lt;/p&gt;

&lt;p&gt;\[ y(\textbf{x}) = \frac{\int tp(\textbf{x},t)dt}{p(x)} = \frac{\int tp(\textbf{x},t)dt}{p(\textbf{x})} = \int tp(t \mid \textbf{x})dt = E_{t}[t \mid \textbf{x}] \]&lt;/p&gt;

&lt;p&gt;위의 식은 x가 주어졌을 때의 t의 조건부 평균으로써 회귀 함수라고 한다. 이에 대한 그림은 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/22.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/22.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;타깃 변수가 벡터 t로 표현되는 다차원 변수일 경우에 대해서도 이 결과를 쉽게 확장할 수 있다. 이 경우 최적의 해는 조건부 평균 \( y(x) = E[t \mid x] \)다.&lt;/p&gt;

&lt;p&gt;이 결과는 약간 다른 방식을 통해서도 도출할 수 있다. 이 다른 도출 방법을 통해 회귀 문제의 본질에 대해 더 자세히 살펴볼 수 있을 것이다. 최적의 해가 조건부 기댓값이라는 지식을 바탕으로 제곱항을 다음과 같이 전개할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ (y(\textbf{x}) - t)^{2} = (y(\textbf{x}) - E(t \mid \textbf{x}) + E(t \mid \textbf{x}) -t)^{2} \]
\[= (y(x)-E(t \mid \textbf{x}))^{2}+2(y(x)-E(t \mid x))(E(t \mid x)-t)+(E(t \mid x)-t)^{2} \]&lt;/p&gt;

&lt;p&gt;이 전개 결과를 손실 함수에 대입하고 t에 대해 적분하면 교차항들이 사라지게 된다. 그 결과로 다음 형태의 손실 함수를 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ E(L) = \int (y(\textbf{x})-E(t \mid \textbf{x}))^{2}p(\textbf{x})dx + \int var(t \mid \textbf{x})p(\textbf{x})dx \]&lt;/p&gt;

&lt;p&gt;우리가 찾고자 하는 함수 y(x)는 첫 번째 항에만 있는데, y(x)가 E(t|x)일 때 이 항은 최소화되어 항 자체가 사라지게 된다.
따라서 이는 우리가 앞서서 도출한 결과와 동일하다. 즉 최적의 최소 제곱 예측은 조건부 평균으로 주어진다는 것을 보여 준다.
두 번째 항은 t에 대한 분포의 분산을 계사하고, 이를 x에 대해 평균을 낸 것이다.  이 항은 표적 데이터가 가지고 있는 내재적인 변동성을 표현한 것으로, 노이즈라고 해석할 수 있다. 이 항은 y(x)에는 독립적이며, 따라서 절대로 더 이상 줄일 수 없는 손실 함수의 최솟값에 해당한다.&lt;/p&gt;

&lt;p&gt;분류 문제와 마찬가지로 회귀 문제를 풀기 위한 세 가지 서로 다른 방식은 다음과 같다.&lt;/p&gt;

&lt;p&gt;(a) 우선 결합 밀도 p(x,t)를 구하는 추론 문제를 풀어낸다. 다음에 이를 정규화하여 조건부밀도 \( p(t \mid x) \)를 구하고 최종적으로 조건부 평균을 구한다.&lt;/p&gt;

&lt;p&gt;(b)우선 조건부 밀도 \(p(t \mid x) \)를 구하는 추론 문제를 풀고 주어지는 조건부 평균을 구한다.&lt;/p&gt;

&lt;p&gt;(c)훈련 데이터로부터 회귀 함수 y(x)를 직접 구한다.&lt;/p&gt;

&lt;p&gt;각각의 방식의 장단점은 분류 문제에서의 세 가지 방식의 장단점과 일맥상통한다.&lt;/p&gt;

&lt;p&gt;회귀 문제의 손실 함수로 제곱 손실 이외의 다른 것을 사용하는 것도 가능하다. 실제로 제곱 손실이 상당히 좋지 않은 결과를 가져오기 때문에 더 복잡한 접근법을 사용해야 하는 경우가 종종 있다. 조건부 분푸 \( p(t \mid x) \)가 다봉 분포인 상황이 이 중 하나다. 역 문제의 해를 구할 때 이런 상황이 종종 발생한다.제곱 손실을 일반화한 예시인 민코프스키 손실에 대해 간략히 살펴보자. &lt;b&gt;민코프스키 손실&lt;/b&gt;의 기댓값은 다음과 같이 주어진다.&lt;/p&gt;

&lt;p&gt;\[ E[L_{q}] = \int \int \mid y(x)-t \mid ^{q}p(\textbf{x},t)dxdt \]&lt;/p&gt;

&lt;p&gt;q=2인 경우 제곱 손실에 해당하게 된다. 다양한 q 값에 대하여 함수 \(\mid y-t \mid ^{q} \) 와 y-t값의 그래프를 확인할 수 있다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/23.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/23.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-1.5-Decision-Theory(2)/&quot;&gt;1.5 Decision Theory(2)&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 28, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[1.5 Decision Theory]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-1.5-Decision-Theory/" />
  <id>http://localhost:4000/articles/[PRML]1.5 Decision Theory</id>
  <published>2022-02-27T04:01:50+09:00</published>
  <updated>2022-02-27T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;p&gt;패턴 인식 문제를 풀 때는 불확실성이 존재하는 상황에서 의사 결정을 내려야 하는 경우가 많다. 이런 상황에서 결정 이론과 확률론을 함께 사용하면 최적의 의사 결정을 내릴 수 있다.&lt;/p&gt;

&lt;p&gt;입력 벡터 X와 타깃 변수 벡터 t가 존재하는 상황에서 새로운 입력 변수 X가 주어졌을 때 해당 타깃 변수 벡터 t를 예측하는 문제에 대해서 생각해보자. 결합 확률 분포 p(x,t) 는 이 변수들의 전체 불확실성을 요약해서 나타낸다. 주어진 훈련 집합 데이터에서 p(x.t)를 찾아내는 것은 추론문제의 대표적인 예시다. 실제 응용 사례에서는 대부분의 경우 t에 대해서 예측하는 것이 더 중요한 문제이다. t가 어떤 값을 가질 것 같은지를 바탕으로 특정 행동을 취해야 할 수도 있다. 이를 위한 이론적 토대가 바로 결정 이론이다.&lt;/p&gt;

&lt;p&gt;일반적인 추론 문제는 결합 확률 분포를 결정하는 과정을 포함하고 있다. 결합 확률 분포는 매우 유용한 값이긴 하지만 최종적으로 우리가 하고 싶은 것은 우리가 어떤 행동을 취해야 할 지 결정하는 것이다. 또한, 해당 결정이 최적이기를 바라는데 이것이 바로 결정단계이다. 결정 이론이 하려는 것은 적절한 확률들이 주어진 상태에서 어떻게 하면 최적의 결정을 내릴 수 있는 가를 설명하는 것이다.&lt;/p&gt;

&lt;p&gt;더 자세한 분석을 하기에 앞서, 의사 결정에 있어서 확률이 어떤 역할을 하는지 간략하게 살펴보자. 우리의 목표는 새 데이터 X를 구한 후에 두 개의 클래스 중 어떤 것으로 분류해보는 것이라고 하자. 우리는 데이터가 주어졌을 때 각각의 클래스의 조건부 확률을 알아내고 싶으며 이는 \( p(C_{k}\mid x) \)로 표현된다. 베이지안 정리를 사용하면 이 확률들을 다음과 같은 형태로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ p(C_{k} \mid \textbf{x}) = \frac{p(\textbf{x} \mid C_{k})p(C_{k})}{p(\textbf{x})}\]&lt;/p&gt;

&lt;p&gt;위의 베이지안 정리에서 사용된 모든 값들은 결합 확률 분포 \( p(\textbf{x} \mid C_{k}) \)를 활용하여 구할 수 있다. \( p(C_{k}) \)는 클래스 \( C_{k} \)에 포함될 사전 확률, \( p(C_{k} \mid \textbf{x}) \)는 사후 확률에 해당한다. 만약 우리의 목표가 X를 잘못된 클래스에 포함시킬 가능성을 최소화하는 것이라면, 직관적으로 우리는 더 높은 사후 확률을 가진 클래스를 고르게 될 것이다.&lt;/p&gt;

&lt;h4 id=&quot;오분류-비율의-최소화&quot;&gt;오분류 비율의 최소화&lt;/h4&gt;

&lt;p&gt;우리는 목표가 단순히 잘못된 분류 결과의 숫자를 가능한 한 줄이는 것이라고 해보자. 이를 위해서는 각각의 x를 가능한 클래스들 중 하나에 포함시키는 규칙이 필요하다. 이 규칙은 입력 공간을 결정 구역이라고 불리는 구역 \(R_{k} \)들로 나누게 될 것이다. \( R_{k} \)는 클래스의 수만큼 존재할 것이고 Rk에 존재하는 모든 포인트들은 클래스 \( C_{k} \)에 포함될 것이다. 결정 구역들 사이의 경계를 결정 경계 혹은 결정 표면이라고 부른다.
최적의 결정 규칙을 찾아내기 위해서 두 개의 클래스를 가진 경우를 생각해보자. \( C_{1} \)에 속한 변수 x가 \(C_{2} \)에 포함되는 경우나 그 반대의 경우에 ‘실수’가 발생한다. 실수가 발생할 확률은 다음의 식으로 주어진다.&lt;/p&gt;

&lt;p&gt;\[ p(mistake) = p(x \in R_{1},C_{2}) + p( x \in R_{2},R_{1}) = \int_{R_{1}}p(x,C_{2}) + \int_{R_{2}}p(x,C_{1})dx\]&lt;/p&gt;

&lt;p&gt;p(mistake)를 최소화하기 위해서는 각각의 X를 피적분 함수들 중 더 작은 값을 가진 클래스에 포함시켜야 한다.
따라서 \( p(x,C_{1}) &amp;gt; p(x,C_{2}) \)인 경우에는 x를 C1에 포함시켜야 한다. 확률의 곱 법칙에 따라서 \( p(x,C_{k}) = p(C_{k} \mid x)p(x) \)다. p(x)는 양쪽의 항에서 동일하다. 따라서 p(mistake)를 최소화하기 위해서는 각각의 X를 사후 확률 \(p(C_{k} \mid x) \)가 최대가 되는 클래스에 포함시키면 된다는 결론을 낼 수 있다. 하나의 입력 변수 x와 두 클래스의 경우에 대한 해당 도식은 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/16.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/16.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;다른 관점에서 K개의 클래스를 가진 경우에는 올바르게 분류된 경우의 확률을 극대화할 수 있으며 이 방법이 더 일반적이고 조금 더 쉽다.&lt;/p&gt;

&lt;p&gt;\[ p(correct) = \sum_{k=1}^{K}p(x \in R_{k},C_{k}) = \sum_{k=1}^{K}\int_{R_{k}}p(\textbf{x},C_{k})dx \]&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;각각의 x가 p(x,Ck)가 최대인 클래스로 분류되도록 Rk를 선택할 경우에 위의 식 값이 최대화된다. 따라서 각각의 x는 가장 큰 사후 확률 p(Ck&lt;/td&gt;
      &lt;td&gt;x)를 가지는 클래스로 분류되어야 함을 확인할 수 있다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;기대-손실의-최소화&quot;&gt;기대 손실의 최소화&lt;/h4&gt;

&lt;p&gt;한 가지 예를 들어 , 환자의 데이터를 가지고 암의 여부를 추론한다고 해보자. 만약 암에 걸리지 않은 환자를 걸렸다고 판단하는 것보다 잘못된 진단으로 암에 걸린 환자를 건강하다고 판단했을 경우 심각한 결과를 초해라 수 있다. 이처럼 두 잘못된 판단의 결과가 다르게 나타날 수 있다. 이 경우에는 전자의 실수보다 후자의 실수를 줄이는 것이 중요하다.&lt;/p&gt;

&lt;p&gt;비용함수라고도 부르는 손실 함수를 도입함으로써 이러한 문제들을 더 공식화할 수 있다. 손실 함수는 어떤 결정이나 행동이 일어났을 때의 손실을 전체적으로 측정하는 함수다. 이를 활용하면 우리의 목표를 발생하는 전체 손실을 최소화하는 것으로 변경할 수가 있다.&lt;/p&gt;

&lt;p&gt;실제 클래스 \(C_{k} \)인 새 입력값 X를 클래스 \(C_{j} \)로 분류했다고 가정해보자. 이 과정에서 우리는 \(L_{k,j} \)로 표현할 수 있는 손실을 발생시키게 된다.
\(L_{k,j} \)는 손실 행렬의 k,j번째 원소로 볼 수 있다. 예를 들어 암 환자 분류 예시에 대해서 다음과 같은 손실 행렬을 가정하여 결과에 따른 손실을 다르게 줄 수 있다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/17.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/17.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;손실 함수를 최소화하는 해가 최적의 해다. 하지만 손실 함숫값은 알려져 있지 않는 실제 클래스값을 알아야만 계산이 가능하다. 주어진 입력 벡터 X에 대해서 실제 클래스값에 대한 불확실성은 결합 확률 분포 p(x,ck)로 표현된다. 그렇기 때문에 우리는 이 분포에 대해 계산한 평균 손실을 최소화하는 것을 목표로 삼을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ E(L) = \sum_{k}\sum_{j}\int_{R_{j}}L_{k,j}p(\textbf{x},C_{k})dx \]&lt;/p&gt;

&lt;p&gt;확률의 곱의 법칙을 활용하면 \( p(x,C_{k}) = p(C_{k} \mid x)p(x) \)임을 알 수 있고 공통 인자 p(x)를 제거할 수 있다. 따라서 기대 손실을 최소화하는 결정 법칙은 각각의 x를 다음 식을 최소화하는 클래스 j에 할당하는 것이다.&lt;/p&gt;

&lt;p&gt;\[ \int_{k} L_{k,j}p(C_{k} \mid \textbf{x})\]&lt;/p&gt;

&lt;p&gt;각각의 클래스에 대한 사후 확률 \( P(C_{k} \mid x) \)를 알고 나면 이 방법을 쉽게 시행할 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;거부옵션&quot;&gt;거부옵션&lt;/h4&gt;

&lt;p&gt;결합확률 \( p(x,C_{k}) \)들이 비슷한 값을 가지고 있을 경우 이러한 구역들로 인해서 분류 오차가 생겨나기도 한다. 이 구역들에 대해서는 해당 구역이 어떤 클래스에 속할지에 대한 우리의 확신 정도가 비교적 적은 것이다. 이 처럼 결정을 내리기 힘든 지역에 대해서는 결정을 피하는 ‘ 거부 옵션’을 주는 것이 적절할 수도 있다. 임계값 θ를 설정해서 사후 확률 \( P(C_{k} \mid x) \)들 중에서 가장 큰 값이 θ보다 작거나 같을 경우에 해당 입력값 X를 거부하는 방식으로 이를 시행할 수 있다. 하나의 연속 입력 변수 x와 두 개의 클래스의 경우에 대한 예시는 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/18.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/18.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;손실 행렬이 주어진 경우에는 기대 손실값을 최소화하도록 거부 옵션을 확장할 수 있다. 이 경우에는 거부 결정이 내려졌을 때 발생하는 손실값을 고려 사항에 포함해야 한다.&lt;/p&gt;

&lt;p&gt;추론과 결정에 대한 내용은 다음에 이어 설명하겠습니다.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-1.5-Decision-Theory/&quot;&gt;1.5 Decision Theory&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 27, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[ LECTURE 02 Classification and Logistic Regression]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/CS229-LECTURE-02/" />
  <id>http://localhost:4000/articles/[CS229]LECTURE 02</id>
  <published>2022-02-26T07:50:50+09:00</published>
  <updated>2022-02-26T07:50:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/CS229-LECTURE-02/&quot;&gt; LECTURE 02 Classification and Logistic Regression&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 26, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[1.3 Model Selection - 1.4 The Curse of Dimensionality]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-1.3-Model-Selection-1.4-The-Curse-of-Dimensionality/" />
  <id>http://localhost:4000/articles/[PRML]1.3 Model Selection - 1.4 The Curse of Dimensionality</id>
  <published>2022-02-26T07:08:50+09:00</published>
  <updated>2022-02-26T07:08:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;h4 id=&quot;모델-선택&quot;&gt;모델 선택&lt;/h4&gt;

&lt;p&gt;주어진 한 모델에서 매개변수의 값을 결정하는 것뿐만이 아니라 다양한 여러 모델들을 고려하여 해당 응용 사례에 가장 적합한 모델을 선택해야 할 경우도 있다.&lt;/p&gt;

&lt;p&gt;최대 가능도 접근법에서 이미 확인한 것과 같이 훈련 집한에서의 좋은 성능이 반드시 좋은 예측 성능을 보장해 주지는 못한다. 이는 과적합 문제 때문이다. 이를 해결하기 위한 한 가지 방법은 데이터가 충분할 경우 일부의 데이터만 사용하여 다양한 모델과 모델의 매개변수들을 훈련시키고 독립적인 데이터 집합인 검증 집합에서 이 모델들과 매개변수들을 비교/선택하는 것이다. 만약 한정된 크기의 데이터 집합이라면 시험 집합을 따로 분리해 두고 이 집합을 통해서 선택된 모델의 최종 성능을 판단하는 것이 좋을 수도 있다.&lt;/p&gt;

&lt;p&gt;하지만, 실제 경우에는 훈련과 시험을 위한 데이터의 공급이 제한적이다. 이런 상황에서는 교차 검증법이 한 가지 방법이 된다.
교차 검증법을 활용하면 전체 데이터 (s) 중 데이터 (s-1)/s 비율만틈 훈련에 사용하고. 모든 데이터를 다 활용하여 성능을 추정할 수 있다. 특히 데이터가 부족할 경우에는 S=N( N은 전체 데이터 숫자) 의 교차 검증법으 고려할 수 있다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/12.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/12.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;그림과 같이 S = N 교차 검증법은 데이터 포인트 하나만(다홍색)  남겨 두고 모델을 훈련시키는 테크닉이다.&lt;/p&gt;

&lt;p&gt;교차 검증법의 주요 단점은 S의 수가 늘어남에 따라서 모델 훈련의 시행 횟수가 함께 늘어난다는 것이다. 또 다른 문제점은 한 가지 모델에 여러 가지의 복잡도 매개변수가 있을 경우에 발생한다. 여러 매개변수들의 조합들을 확인해보기 위해서는 최악의 경우 매개변수 숫자에 대해 기하급수적인 수의 훈련 실행이 필요할지도 모른다.&lt;/p&gt;

&lt;p&gt;이보다 더 나은 이상적인 방식에서는 훈련 집합만을 활용해서 여러 종류의 초 매개변수와 각 모델 종류에 대한 비교를 한 번의 훈련 과정동안 시행할 수 있어야 한다. 이를 위해서는 오직 훈련 집합만을 활용하는 성능 척도가 필요하다. 또한 이 척도는 과적합으로 인한 편향으로부터 자유로워야 한다.&lt;/p&gt;

&lt;p&gt;역사적으로 다양한 정보 기준들이 최대 가능도 방법의 편향 문제에 대한 대안으로 제시되어 왔다. 예를 들어서 &lt;b&gt;아카이케의 정보량 기준, 베이지안 정보 기준&lt;/b&gt; 등이 있다.&lt;/p&gt;

&lt;h4 id=&quot;차원의-저주&quot;&gt;차원의 저주&lt;/h4&gt;

&lt;p&gt;실제 사례에서는 입력 변수가 하나가 아닌 많은 종류의 입력 변수로 구성된 고차원 공간을 다뤄야 한다.
이 문제에 대해 좀 더 살펴보기 위해서 다음 예시를 살펴보자.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/13.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/13.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;이 데이터는 오일,물,가스가 혼합되어 운반되는 송규관에서 측정된 데이터다. 이 세 가지 원료는 서로 다른 ‘균질’,’환형’,’층상’이라는 세 가지 방식으로 송유관 안에 혼한되어 있을 수 있다. 그리고 각각의 방식 내에서 세 가지 성분의 배합 비율 역시 다를 수 있다. 각 데이터 포인트의 입력값은 12차원의 입력 벡터로 표현되며, 이는 감사선 농도계를 이용하여 측정한 것이다.&lt;/p&gt;

&lt;p&gt;위의 그림은 데이터 집합에서 선별된 100개의 포인트에 대해서 입력 변수 x6와 x7을 표현한 산포도이다. 각 데이터 포인트에는 세 가지 혼합 방식 중 어떤  것인지 색으로 분류하였다. 우리의 목표는 각 데이터 포인트를 훈련 집합으로 사용해서 새로운 관측값을 분류하는 것이다.&lt;/p&gt;

&lt;p&gt;가장 단순한 접근법은 다음 그림과 같이 입력 공간을 같은 크리의 여러 칸들로 나누는 것이다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/14.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/14.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;클래스를 예측하고 싶은 어떤 시험 포인트가 주어졌을 경우에는 일단 해당 포인트가 속한 칸을 찾아내고. 그 칸에 속한 훈련 포인트들을 모두 찾는다. 그리고 해당 칸에 속한 훈력 포인트들의 클래스들을 살펴보고 그중 대다수인 것을 시험 포인트의 클래스로 지정한다.&lt;/p&gt;

&lt;p&gt;이 접근법에는 심각한 문제가 있다. 입력 변수가 더 많아지는 경우, 공간을 단위 크기로 나눌 때, 공간의 차원이 높아짐에 따라서 필요한 칸의 숫자가 기하급수적으로 늘어나는 것이다. 기하급수적으로 많은 칸이 있다면 각 칸이 비어 있지 않도록 하기 위해서 그만큼 많은 수의 훈련 데이터가 필요하다. 이를 그림으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/15.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/15.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;고차원에서 발생할 수 있는 심각한 문제를 &lt;b&gt;차원의 저주&lt;/b&gt;라고 지칭한다. 
차원의 저주는 패턴 인식을 고차원 입력값에 적용하는 데 있어서의 중요한 문제점을 시사한다. 하지만 그렇다고 해서 고차원 입력값에 대해 사용할 수 있는 효과적인 패턴 인식 테크닉을 찾아내는 것이 불가능한 일이 아니다. 그 이유는 2가지이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;실제 세계의 고차원 데이터들의 경우에 유의미한 차원의 수는 제한적이다.&lt;/li&gt;
  &lt;li&gt;대부분의 경우 입력값에 작은 변화가 일어나면 표적값에서도 작은 변화만 일어나게 되고, 지역 보간법 등의 테크닉을 적용하여 새로운 입력 변수에 대한 타킷 변수를 예측하는 것이 가능해진다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;성공적인 패턴 인식 테크닉은 이 특성들을 활용하여 만들어지는 경우가 많다.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-1.3-Model-Selection-1.4-The-Curse-of-Dimensionality/&quot;&gt;1.3 Model Selection - 1.4 The Curse of Dimensionality&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 26, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[1.2 Probability theory(3)]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-1.2-Probability-theory(3)/" />
  <id>http://localhost:4000/articles/[PRML]1.2 Probability theory(3)</id>
  <published>2022-02-26T07:08:50+09:00</published>
  <updated>2022-02-26T07:08:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;p&gt;이번에는 곡선 피팅 문제를 확률적 측면에서 살펴보자.&lt;/p&gt;

&lt;p&gt;곡선 피팅 문제의 목표는 N개의 입력값 \( X = (x1,….xn)^{T} \) 과 해당 표적값 \( t = (t1,…tn)^{T} \)가 주어진 상황에서 새로운 입력 변수 x가 
주어졌을 때 그에 대한 타깃 변수 t를 예측해 내는 것이다.
확률 분포를 이용해서 타깃 변수의 값에 대한 불확실성을 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;먼저, 주어진 x값에 대한 t값이 y(x,w)를 평균으로 가지는 가우시안 분포를 가진다고 가정한다(y(x,w)에 대한 설명은 1.1 참고 )
이를 바탕으로 다음의 조건부 분포를 적을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ “p(t \mid x,\textbf{w},b) = N(t \mid y(x,\textbf{w}),\beta ^{-1}) \]&lt;/p&gt;

&lt;p&gt;여기서 β는 정밀도 매개변수로써 분포의 표본의 역수에 해당한다.
이 식을 도식화해 확인해보면 다음과 같다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/7.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/7.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;이제 훈련 집합 {X,t}를 바탕으로 최대 가능도 방법을 이용해서 알려지지 않은 매개변수 w와 β를 구해보도록 하자.
데이터는 위의 식에서 독립적으로 추출했다고 가정하면(i.i.d), 가능도 함수는 다음과 같이 주어진다.&lt;/p&gt;

&lt;p&gt;\[ p(\textbf{t} \mid \textbf{x},\textbf{w},b) = \prod_{n=1}^{N}N(t \mid y(x,\textbf{w}),\beta ^{-1}) \]&lt;/p&gt;

&lt;p&gt;계산의 편의를 위해 로그를 취해 로그 가능도 함수를 얻으면 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ ln p(\textbf{t} \mid \textbf{x},\textbf{w},b) = -\frac{\beta }{2}\sum_{n= 1}^{N}{(y(x_{n},\textbf{w})-t_{n})}^{2} + \frac{N}{2}ln\beta - \frac{N}{2}ln(2\pi) \]&lt;/p&gt;

&lt;p&gt;다항식 계수(w)의 최대 가능도 해를 구해보면 w와 관련된 것은 오른쪽 식의 맨 왼쪽만 해당한다. 따라서 음의 로그이기 때문에 이를 최소화하는 것이다. 즉 가능도 함수를 최대화하려는 시도의 결과로 제곱합 오차 함수를 유도할 수 있는 것이다.&lt;/p&gt;

&lt;p&gt;마찬가지로 가우시안 조건부 분포의 정밀도 매개변수 β를 결정하는 데도 최대 가능도 방법을 사용할 수 있다. 로그 가능도를 β에 대해 최대화하면 다음의 식이 도출된다.&lt;/p&gt;

&lt;p&gt;\[ \frac{1}{B_{ML}} = \frac{1}{N}\sum_{n = 1}^{N}(y(x_{n},w_{ML})-t_{n})^{2} \]&lt;/p&gt;

&lt;p&gt;매개변수 w와  β를 구했으니, 이제 이를 바탕으로 새로운 변수 x에 대해 예측값을 구할 수 있다.
확률모델을 사용하고 있으므로 예측값은 t에 대한 예측 분포로 표현될 것이다(점 X)
최대 가능도 매개변수들을 식(1)에 대입하면 다음을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ p(t \mid x,w_{ML},\beta_{ML}) = N(t \mid  y(x,w_{ML}),\beta^{-1}_{ML}) \]&lt;/p&gt;

&lt;p&gt;이제 다항 계수 w에 대한 사전 분포를 도입할 것이다. 문제의 단순화를 위해서 다음 형태를 지닌 가우시안 분포를 사용할 것이다.
\[ p(w \mid \alpha) = N( w \mid 0,\alpha^{-1}\textbf{I}) = (\frac{\alpha^{\frac{M+1}{2}}}{2\pi})exp(-\frac{\alpha}{2}w^{T}w)\]&lt;/p&gt;

&lt;p&gt;식에 대해 설명을 하자면 α는 분포의 정밀도이며 모델 매개변수의 분포를 제어하는 초매개변수이다. 또한 M+1은 m차수 다항식 벡터 w의 원소의 개수이다.&lt;/p&gt;

&lt;p&gt;따라서 베이지안 정리에 의해 다음이 성립한다( 자세한 내용은 1.2(2)를 참고)&lt;/p&gt;

&lt;p&gt;\[ p(w \mid x,t,\alpha, \beta ) \propto p(t \mid t,w,\beta)p(w \mid \alpha) \]&lt;/p&gt;

&lt;p&gt;이제 주어진 데이터에 대해 가장 가능성 높은 w를 찾는 방식으로 w를 결정할 수 있다.
즉, 사후 분포를 최대화하는 방식으로 w를 결정할 수 있다. 이 테크닉을 &lt;b&gt;최대 사후 분포&lt;/b&gt;라 한다.&lt;/p&gt;

&lt;p&gt;따라서 사후 확률의 최댓값을 찾는 것이 다음 식 값의 최솟값을 찾는 것과 동일함을 알 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \frac{\beta}{2}\sum_{n=1}^{N}(y(x_{n},\textbf{w}) - t_{n})^{2} + \frac{\alpha}{2}\textbf{w}^{T}\textbf{w} \]&lt;/p&gt;

&lt;p&gt;따라서 사후 분포를 최대화하는 것이 정규화 매개변수가 \( \lambda = \frac{α}{β} \) 로 주어진 식의 정규화된 제곱합 오차 함수(1.1참고)를 최소화하는 것과 동일함을 확인할 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;베이지안-곡선-피팅&quot;&gt;베이지안 곡선 피팅&lt;/h4&gt;

&lt;p&gt;비록 사전 분포 p(w | α)를 포함시키긴 했지만, 여전히 w에 대해 점 추정을 하고 있기 때문에 아직은 완벽한 베이지안 방법론을 구사한다고 말할 수 없다. 완전한 베이지안적 접근을 위해서는 확률의 합의 법칙과 곱의 법칙을 일괄적으로 적용해야 한다. 이를 위해서는 모든 w값에 대해 적분을 시행해야 한다. 이러한 주변화가 패턴 인식에서의 베이지안 방법론의 핵심이다.
(주변화에 대하여 좀 더 설명하자면, 여러개의 확률 변수로 구성된 조합 확률 분포에서 한 가지 변수에 대한 확률값을 추정하기 위해 나머지 변수를 모두 적분하여 제거하는 과정을 말한다.)&lt;/p&gt;

&lt;p&gt;곡선 피팅 문제의 새로운 변수 x에 대한 표적값 t를 예측하기 위해 예측 분포 p(t|x,X,t)를 구해 보자. 여기서는 매개변수 α 와 β는 고정되어 있으며, 미리 알려졌다고 가정한다.
단순히 말하자면 베이지안 방법은 단지 확률의 합과 곱의 법칙을 계속적으로 적용하는 것이다. 이를 통해 예측 분포를 다음과 같은 형태로 적을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ p(t \mid x,\textbf{x},t) = \int p(t \mid x,\textbf{w})p(w \mid \textbf{x},t)dw\]&lt;/p&gt;

&lt;p&gt;p(w|x,t)는 매개변수들에 대한 사후 분포이다.
위 식의 적분을 시행하면 예측 분포가 다음의 식과 같이 가우시안 분포로 주어진다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;\[ p(t \mid x,\textbf{x},t) = N(t \mid m(x),s^{2}(x)) \]&lt;/p&gt;

&lt;p&gt;여기서 예측분포의 평균과 분산이 x에 종속되어 있음을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;여기서 평균과 분산은 다음과 같다&lt;/p&gt;

&lt;p&gt;\[ m(x) = \beta \phi (x)^{T}\textbf{S} \sum_{n=1}^{N}\phi(x_{x_{n}})t_{n} \]
\[ s^{2}(x) = \beta^{-1} + \phi(x)^{T} \textbf{S} \phi(x)\]&lt;/p&gt;

&lt;p&gt;식에 대해 설명하자면, 분산의 \(\beta^{-1} \)은 타깃 변수의 노이즈로 인한 예측값 t의 불확실성이며 최대가능도 해의 \(\beta^{-1} \)과 같다,
또한 분산의 두 번째 항은 w의 불확실성으로 부터 기인한다.&lt;/p&gt;

&lt;p&gt;행렬 S 는 다음처럼 주어진다.&lt;/p&gt;

&lt;p&gt;\[\textbf{S}^{-1} = \alpha \textbf{I} + \beta \sum_{n=1}{N} \phi(x_{n})\phi(x_{n})^{T} \]&lt;/p&gt;

&lt;p&gt;I 는 단위 행렬이며 ø(x)는 각각의 원소가 i = 0,…,M에 대해 \( \phi_{i}(x) = x^{i} \)인 벡터이다&lt;/p&gt;

&lt;p&gt;마지막으로 합성 사인 회귀 문제에 대한 예측 분포의 시각화를 통해 확인해보면서 이해를 돕도록 한다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/11.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/11.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;이는 베이지안적 방법을 통해 구한 M = 9인 다항식 곡선 피팅 문제의 예측 분포이다.
빨간색 선은 예측 분포의 평균값을, 그리고 빨간색 영역은 평균값으로부터 ±1 표준 편찻값을 가지는 부분을 표현한 것이다.&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-1.2-Probability-theory(3)/&quot;&gt;1.2 Probability theory(3)&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 26, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[1.2. Probability theory (2)]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-1.2-Probability-theory(2)/" />
  <id>http://localhost:4000/articles/[PRML]1.2 Probability theory(2)</id>
  <published>2022-02-25T07:08:50+09:00</published>
  <updated>2022-02-25T07:08:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;p&gt;지금까지 확률을 ‘반복 가능한 임의의 사건의 빈도수’라는 측면에서 살펴보았다. 이러한 해석을 고전적 또는 빈도적 관점이라 일컫는다.
이번에는 더 포괄적인 베이지안 관점에서 살펴보자. 베이지안 관점을 이용하면 확률을 이용해서 불확실성을 정량화하는 것이 가능하다.&lt;/p&gt;

&lt;p&gt;베이지안 관점을 사용하면 모델 매개변수의 불확실성을 설명할 수 있고 더 나아가 모델 그 자체를 선택하는 데 있어서도 유용하다.&lt;/p&gt;

&lt;p&gt;앞에서 설명한 다항식 곡선 피팅(1.1장) 예시의 매개변수 w에 적용해보자&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;첫 번째로 데이터를 관측하기 전의 w에 대한 우리의 가정을 사전 확률 분포 p(w)로 표현할 수 있다.&lt;/li&gt;
  &lt;li&gt;관측된 데이터 D = {t1,t2,,,,,tn}은 조건부 확률 p(D|w)로써 작용하게 된다.
이 경우 베이지안 정리는 다음 형태를 띤다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\( p(w \mid D) = \frac{p(D \mid w)p(w)}{p(D)} \) 식(1)&lt;/p&gt;

&lt;p&gt;자세히 설명하면&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\(p(w \mid D) \): D를 관측한 후의 w에 대한 불확실성을 사후 확률로 표현&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(p(D \mid w) \) : 각각의 다른 매개변수 벡터 w에 대해 관측된 데이터 집합이 얼마나 그렇게 나타날 가능성이 있는지를 표현한 가능도 함수 
           (가능도 함수는 w에 대한 확률 분포가 아니며, 따라서 w에 대해 가능도 함수를 적분하여도 1이 될 필요가 없다)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\( p(w) \): 데이터를 관측하기 전의 w에 대한 우리의 가정, 사전 확률&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\( p(D) \) : 사후 분포가 적합한 확률 분포가 되고 적분값이 1이 되도록 하기 위한 정규화 상수&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;가능도 함수에 대한 정의를 바탕으로 베이지안 정리를 다음처럼 적을 수가 있다.&lt;/p&gt;

&lt;p&gt;사후 확률  ∝ 가능도 x 사전확률 ( 각 값은 전부 w에 대한 함수)&lt;/p&gt;

&lt;p&gt;식(1)의 양쪽 변을 w에 대해 적분하면 베이지안 정리의 분모를 사전확률과 가능도 함수로 표현 가능하다&lt;/p&gt;

&lt;p&gt;\[p(D) = \int p(D \mid \textbf{w})p(\textbf{w})d\textbf{w} \]&lt;/p&gt;

&lt;p&gt;여기서 가능도 함수의 역할을 살펴보면 빈도적 확률 관점과 베이지안 확률 관전에서 다르다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;빈도적 확률 관점 : w가 고정 매개변수로 여겨지며, w의 값은 어떤 형태의 ‘추정값’을 통해서 결정된다. 그리고 추정에서의 오류는
                            가능한 데이터 집합들 D의 분포를 고려함으로써 구할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;베이지안 확률 관점 : 오직 하나의 (실제로 관측될) 데이터 집합 D만 존재하며, 매개변수의 불확실성은 w의 확률 분포를 통해 표현한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;u&gt;빈도적 확률 관점&lt;/u&gt;에서 널리 사용되는 추정값 중 하나는 바로 &lt;b&gt;최대가능도&lt;/b&gt;이다.&lt;/p&gt;

&lt;p&gt;최대가능도를 사용할 경우에 w의 가능도 함수 p(D|w)를 최대화하는 값으로 선택한다.
(머신러닝에서 종종 음의 로그 가능도 함숫값을 오차함수라고 일컫는다. 음의 로그 함수는 단조 감소하는 함수이기 때문에 가능도의 최댓값을 찾는 것이 오차를 최소화하는 것과 동일하다 -&amp;gt; 자세한 내용은 CS229 강의를 정리하면서 설명할 예정이다)&lt;/p&gt;

&lt;p&gt;또한, 빈도적 확률론자들이 오차를 측정하는 방법 중 하나가 부트스트랩이다.
예를 들어 설명하면, 원 데이터 집합이 N개인 데이터 포인트 X = {x1,x2,,,,xn}가 있다고 가정해보자.
X에서 N개의 데이터 포인트를 임의로 복원 추출하여 데이터 집합 Xb를 만든다. 
이 과정을 L번 반복하면 원래 데이터 집합의 표본에 해당하는 크기 N의 데이터 집합을 L개 만들 수 있다.
각각의 부트스트랩 데이터 집합에서의 예측치와 실제 매개변수 값과의 차이를 바탕으로 매개변수 추정값의 통계적 정확도를 판단할 수 있다.&lt;/p&gt;

&lt;p&gt;베이지안 관점의 장점 중 하나는 사전 지식을 추론 과정에서 자연스럽게 포함시킬 수 있다.
동전 10개를 던졌는데 모두 앞면이 나왔다고 가정하자. 빈도적 관점에서는 미래의 모든 동전 던지기에서 앞면만 나올 것이라고 예측한다.대조적으로 베이지안 관점에서는 적당히 합리적인 사전 확률을 사용한다면 이렇게까지 과도한 결론이 나오지 않을 것이다.&lt;/p&gt;

&lt;p&gt;하지만, 베이지안 관점에서는 사전 확률의 선택에 따라 결론이 나오기 때문에 추론 과정에서 주관이 포함될 수밖에 없고 실제로 좋지 않은 사전 분포를 바탕으로 한 베이지안 방법은 부족한 결과물을 높은 확신으로 내놓기도 한다.&lt;/p&gt;

&lt;h4 id=&quot;가우시안-분포&quot;&gt;가우시안 분포&lt;/h4&gt;

&lt;p&gt;이제 가장 중요한 연속 활률 분포 하나를 살펴보고자 한다. 바로 정규 분포라고도 불리는 가우시안 분포이다.&lt;/p&gt;

&lt;p&gt;단일 실수 변수 x에 대해서 가우시안 분포는 다음과 같이 정의된다.&lt;/p&gt;

&lt;p&gt;\[ N(x \mid \mu ,\sigma ^{2}) = \frac{1}{\sqrt{2\pi \sigma ^{2}}}exp(-\frac{(x-\mu)^{2}}{2\sigma^{2}}) \]&lt;/p&gt;

&lt;p&gt;위의 식은 두 개의 매개변수 μ(평균),  \(σ^{2} \)(분산)에 의해 통제된다.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/6.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/6.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;위의 가우시안 식으로부터 가우시안 분포가 다음의 성질은 만족함을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;\[N(x \mid \mu ,\sigma ^{2})&amp;gt; 0 \]&lt;/p&gt;

&lt;p&gt;가우시안 분포가 정규화되어 있다는 것 또한 쉽게 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \int_{-\infty }^{\infty}N(x \mid \mu,\sigma^{2})dx = 1 \]&lt;/p&gt;

&lt;p&gt;가우시안 분포를 따르는 임의의 x에 대한 함수의 기댓값을 구할 수 있다. 특히 x의 평균값은 다음과 같다&lt;/p&gt;

&lt;p&gt;\[ E(x) = \int_{-\infty }^{\infty}N(x \mid \mu ,\sigma ^{2}) x dx = \mu \]&lt;/p&gt;

&lt;p&gt;이와 비슷하게 x에 대한 이차 모멘트를 계산해 보자&lt;/p&gt;

&lt;p&gt;\[ E(x^{2}) = \int_{-\infty }^{\infty}N(x \mid \mu ,\sigma ^{2}) x^{2} dx = \mu^{2} +\sigma^{2} \]&lt;/p&gt;

&lt;p&gt;이를 통해 x의 분산을 다음과 같이 계산할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ var(x) = E(x^{2}) - E(x)^{2} = \sigma^{2} \]&lt;/p&gt;

&lt;p&gt;분포의 최댓값을 최빈값(mode)이라 하는데, 가우시안 분포의 경우에는 최빈값과 평균값이 동일하다.&lt;/p&gt;

&lt;p&gt;다음으로는 연속 변수로 이루어진 D차원 벡터 X에 대한 가우시안 분포를 살펴보도록 하자&lt;/p&gt;

&lt;p&gt;\[ N(X \mid \mu,\Sigma ) = \frac{1}{(2\pi) ^{\frac{D}{2}}}\frac{1}{ \mid \Sigma \mid ^{\frac{1}{2}}}exp(-\frac{1}{2}(X-\mu)^{T}\Sigma^{-1}(X-\mu)) \]&lt;/p&gt;

&lt;p&gt;이 식은 가우시안 분포의 가능도 함수에 해당한다.&lt;/p&gt;

&lt;p&gt;관측된 데이터 집합을 바탕으로 확률 분포의 매개변수를 결정하는 표준적인 방법 중 하나는 가능도 함수를 최대화하는 매개변수를 찾는 것이다.&lt;/p&gt;

&lt;p&gt;위의 식에 로그를 취하여 (수학적 분석, 계산을 쉽게하기 위해서 또한 언더플로우 방지) 로그가능도 함수를 다음과 같이 적을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ lnp(X \mid \mu,\sigma^{2} ) =-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}(x_{n} - \mu)^{2} - \frac{N}{2}ln\sigma^{2}-\frac{N}{2}ln(2\pi) \]&lt;/p&gt;

&lt;p&gt;μ에 대해 이 식의 최댓값을 찾으면 다음의 최대 가능도해를 찾을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n} \]&lt;/p&gt;

&lt;p&gt;이는 바로 관찰된 값{xn}들의 평균인 표본평균이다&lt;/p&gt;

&lt;p&gt;이와 비슷한 방식으로 최댓값을 \(σ^{2} \)에 대해 찾으면 분산에 대한 최대 가능도 해를 다음과 같이 찾을 수 있다.&lt;/p&gt;

&lt;p&gt;\[ \sigma^{2}(ML) = \frac{1}{N}\sum_{n=1}^{N}(x_{n} - \mu_{ML})^{2}\]&lt;/p&gt;

&lt;p&gt;이는 표본 평균에 대해 계산된 표본 분산이다(가우시안 분포의 경우, μ,  \( σ^{2} \)에 대한 해가 연관이 없기 때문에 계산 순서를 변경해도 무방하다)&lt;/p&gt;

&lt;p&gt;하지만, 최대가능도 방법에는 한계가 있다
최대가능도 방법이 구조적으로 분포의 분산을 과소평가하게 된다.즉 편향된다.
최대가능도 해인 μ,  \( σ^{2} \)은 데이터 집합 x1,….,xn의 함수다. 각 데이터 집합의 값에 대해 이들의 기댓값을 고려해보자&lt;/p&gt;

&lt;p&gt;\[ E(\mu_{ML}) = \mu \]
\[ E(\sigma^{2}_{ML}) = (\frac{N-1}{N})\sigma^{2} \]&lt;/p&gt;

&lt;p&gt;여기서 분산이 (N-1/N)만큼 과소평가된다.
최대 가능도 방법의 편향 문제는 우리가 앞에서 살펴본 다항식 곡선 피팅에서의 과적합 문제의 근본적인 원인이 된다.
또한, 데이터 포인트의 개수인 N이 커질수록 최대 가능도 해에서의 편향치는 점점 줄어든다&lt;/p&gt;

&lt;p&gt;곡선 피팅에 대한 내용은 다음에 이어 설명하겠습니다.&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-1.2-Probability-theory(2)/&quot;&gt;1.2. Probability theory (2)&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 25, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[1.2 Probability theory]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/PRML-1.2-Probability-theory/" />
  <id>http://localhost:4000/articles/[PRML]1.2 Probability theory</id>
  <published>2022-02-24T04:01:50+09:00</published>
  <updated>2022-02-24T04:01:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p class=&quot;notice&quot;&gt;이 글은 &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.&lt;/p&gt;

&lt;p&gt;패턴 인식에서 ‘불확실성’은 측정할 때의 노이즈를 통해서도 발생하고 데이터 집합 수가 제한되어 있다는 한계점 때문에도 발생한다.
따라서, 확률론은 불확실성을 계량화하고 조작하기 위한 이론적 토대를 마련해 준다.&lt;/p&gt;

&lt;p&gt;예시를 들어보자&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/5.png&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/5.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;이 예시에서는 X,Y라는 두 가지 확률 변수가 존재한다. X는 \(x_{i} \) (i = 1,….,M)중 아무 값이나 취할 수 있고 Y는 \(y_{j} \) (j = 1,…,L)중 아무 값이나 취할 수 있다. 또한, X와 Y각각에서 표본을 추출하는 시도를 N번 한다고 가정한다.
\(n_{i,j} \)는 \( X = x_{i}, Y = y_{j} \) 인 시도 개수를 의미하며, \(c_{i} \)는 Y 값과는 상관없이 \(X= x_{i} \)인 시도의 숫자, \(r_{j} \)는  X값과 상관없이 \(Y=y_{j} \)인 시도의 숫자를 의미한다.&lt;/p&gt;

&lt;p&gt;\( X = x_{i}, Y = y_{j} \) 일 결합 확률은 다음과 같이 표현된다.&lt;/p&gt;

&lt;p&gt;\[ p(X = x_{i},Y = y_{i}) = \frac{n_{i,j}}{N} \]&lt;/p&gt;

&lt;p&gt;비슷하게 Y값과 무관하게 X가 \(x_{i} \)값을 가질 확률을 다음과 같다&lt;/p&gt;

&lt;p&gt;\[p(X = x_{i}) =\frac{c_{i}}{N} \]&lt;/p&gt;

&lt;p&gt;이 두 식을 이용하여 다음을 도출해 낼 수 있다.&lt;/p&gt;

&lt;p&gt;\[p(X = x_{i}) = \sum_{j=1}^{L}p(X = x_{i},Y = y_{j}) \]&lt;/p&gt;

&lt;p&gt;이것이 확률의 &lt;b&gt;합의 법칙&lt;/b&gt;이다. 여기서 \(P(X = x_{i}) \)는 주변 확률이라 불린다.&lt;/p&gt;

&lt;p&gt;여기서 \(X = x_{i} \)인 사례들만 고려해보자. 그들 중에서 \(Y = y_{i} \)인 사례들의 비율을 생각해 볼 수 있고, 이를 확률 \(p(Y = y_{i} | X = x_{i}) \)로 적을 수 있으며
이를 &lt;b&gt;조건부 확률&lt;/b&gt;이라고 부른다&lt;/p&gt;

&lt;p&gt;\[ p(Y = y_{i} \mid X = x_{i}) = \frac{n_{i,j}}{c_{i}} \]&lt;/p&gt;

&lt;p&gt;지금까지의 식을 이용해서 다음의 관계를 도출할 수 있다.&lt;/p&gt;

&lt;p&gt;\[ p(X = x_{i} \mid Y = x_{j}) = \frac{n_{i,j}}{c_{i}} = \frac{n_{i,j}}{c_{i}}\cdot \frac{c_{i}}{N} = p(Y = y_{j} \mid X = x_{i})p(X=x_{i}) \]&lt;/p&gt;

&lt;p&gt;이것이 바로 확률의&lt;b&gt; 곱의 법칙&lt;/b&gt;이다&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;이제, 곱의 법칙과 합의 법칙, 대칭성 p(X,Y) = p(Y,X)로 부터 조건부 확률 간의 관계인 다음 식을 도출해 낼 수 있다.
(여기서 부터는 간단하게 확률 변수 X에서 분포를 표현할 때는 p(X)라 적고 특정 값 xi에서의 분포를 표현할 때는 \(p(x_{i}) \)로 적기로 한다)&lt;/p&gt;

&lt;p&gt;\[p(Y \mid X) = \frac{p(X\mid Y)p(Y)}{p(X)} = \frac{p(X\mid Y)p(Y)}{\sum_{Y}p(X\mid Y)p(Y)} \]&lt;/p&gt;

&lt;p&gt;이를 &lt;b&gt;베이즈 정리&lt;/b&gt;라고 한다( 책의 전반에 걸쳐 중요한 역할을 한다)&lt;/p&gt;

&lt;p&gt;여기서 분모는 왼쪽 항을 모든 Y값에 대하여 합했을 때 1이 되도록 하는 역할을 한다.
베이지안 정리는 다음과 같이 해석할 수 있다.
확률 p(A|B)를 알고 있을 때, 관계가 정반대인 확률 p(B|A)를 계산하는 것으로 여기서 P(B)는 사전 확률, p(B|A)는 사후 확률에 해당한다.
여기서, 두 확률 변수가 독립이라면, p(X,Y) = p(X)p(Y) 이다.&lt;/p&gt;

&lt;h4 id=&quot;확률-밀도&quot;&gt;확률 밀도&lt;/h4&gt;

&lt;p&gt;이번에는 연속적인 변수에서의 확률에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;만약 실수 변수 x가 (x,x+δx)구간 안의 값을 가지고 그 변수의 확률이 p(x)δx로 주어진다면, p(x)를 x의 확률 밀도라고 부른다.
이때, x가 (a,b) 구간 사이의 값을 가질 확률은 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ p(x \in (a,b)) = \int_{a}^{b}p(x)dx \]&lt;/p&gt;

&lt;p&gt;여기서, 확률은 양의 값을 가지고 x의 값은 실축선상에 존재해야 하기 때문에 다음의 두 조건을 만족시켜야 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\( p(x) \geq 0 \)&lt;/li&gt;
  &lt;li&gt;\( \int_{-\infty }^{\infty}p(x)dx = 1 \)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;누적-분포-함수&quot;&gt;누적 분포 함수&lt;/h4&gt;

&lt;p&gt;x가 (-∞,z) 범위에 속할 확률은 &lt;b&gt;누적 분포 함수 &lt;/b&gt; 로 표현된다.&lt;/p&gt;

&lt;p&gt;\[ P(z) = \int_{-\infty }^{z}p(x)dx \]&lt;/p&gt;

&lt;p&gt;여기서 P’(x) = p(x)이다.&lt;/p&gt;

&lt;p&gt;만약 x가 이산 변수일 경우 p(x)를  &lt;b&gt; 확률 질량 함수 &lt;/b&gt;라고 부르기도 한다.&lt;/p&gt;

&lt;h4 id=&quot;기댓값&quot;&gt;기댓값&lt;/h4&gt;
&lt;p&gt;기댓값은 확률 밀도 p(x)하에서 어떤 함수 f(x)의 평균값을 의미한다. 이산 분포의 경우 기댓값은 다음과 같이 주어진다.&lt;/p&gt;

&lt;p&gt;\[ E(f) = \sum_{x}p(x)f(x) \]&lt;/p&gt;

&lt;p&gt;여기서 p(x)는 각 x값에 해당하는 확률이며 이것을 가중치로 사용한 가중 평균을 구하는 것으로 해석할 수 있다.&lt;/p&gt;

&lt;p&gt;연속 변수의 경우에는 해당 확률 밀도에 대해 적분을 시행해서 기댓값을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;\[E(f) = \int p(x)f(x)dx \]&lt;/p&gt;

&lt;p&gt;공분산에 해당하는 특징은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;E[aX + b] = aE[X] + b&lt;/li&gt;
  &lt;li&gt;E[X + Y] = E[X] + E[Y]&lt;/li&gt;
  &lt;li&gt;E[XY] = E[X]E[Y]&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;공분산&quot;&gt;공분산&lt;/h4&gt;

&lt;p&gt;공분산을 설명하기 앞서 분산부터 설명하면, f(x)의 분산은 다음과 같다.&lt;/p&gt;

&lt;p&gt;\[ var(f) = E((f(x) - E(f(x)))^{2}) \]&lt;/p&gt;

&lt;p&gt;이는 f(x)가 평균값 E[f(x)]로부터 전반적으로 얼마나 멀리 분포되었는지를 나타내는 값이다.&lt;/p&gt;

&lt;p&gt;이를 전개하여 정리하면 다음과 같다. (식의 이해를 돕기 위하여 상수인 E[f(x)] 부분을  μ라 표현)&lt;/p&gt;

&lt;p&gt;\( var(f) = E(f(x)^{2} -2\mu f(x) +\mu^{2})) \)  -&amp;gt; 공분산 특징 첫 번째 이용&lt;/p&gt;

&lt;p&gt;\( = E(f(x)^{2}) -2\mu E(f(x)) +\mu^{2} \)&lt;/p&gt;

&lt;p&gt;\( = E(f(x)^{2}) - E(f(x))^{2} \)&lt;/p&gt;

&lt;p&gt;여기서 변수 x 그 자체에도 고려가능하다.&lt;/p&gt;

&lt;p&gt;다음으로 두 개의 확률 변수 x 와 y에 대해서 공분산은 다음과 같이 정의된다.&lt;/p&gt;

&lt;p&gt;\[ cov(x,y) = E((x - E(x))(y - E(y)) \]
\[= E(xy) - E(x)E(y) \]&lt;/p&gt;

&lt;p&gt;이는 x값과 y값이 얼마나 함께 같이 변동하는가에 대한 지표이며 만얀 x,y가 서로 독립적일 경우 공부산값은 0으로 간다.&lt;/p&gt;

&lt;p&gt;베이지안 확률에 대한 내용은 다음에 이어 설명하겠습니다.&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/PRML-1.2-Probability-theory/&quot;&gt;1.2 Probability theory&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 24, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[ LECTURE 01]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/CS231-LECTURE-01/" />
  <id>http://localhost:4000/articles/[CS231]LECTURE 01</id>
  <published>2022-02-23T07:50:50+09:00</published>
  <updated>2022-02-23T07:50:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/CS231-LECTURE-01/&quot;&gt; LECTURE 01&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 23, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[ LECTURE 01 Rinear Regression]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/articles/CS229-LECTURE-01/" />
  <id>http://localhost:4000/articles/[CS229]LECTURE 01</id>
  <published>2022-02-23T07:50:50+09:00</published>
  <updated>2022-02-23T07:50:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/articles/CS229-LECTURE-01/&quot;&gt; LECTURE 01 Rinear Regression&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 23, 2022.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[베타 분포]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/blog/%E1%84%87%E1%85%A6%E1%84%90%E1%85%A1-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%91%E1%85%A9/" />
  <id>http://localhost:4000/blog/베타 분포</id>
  <published>2022-02-22T07:08:50+09:00</published>
  <updated>2022-02-22T07:08:50+09:00</updated>
  <author>
    <name>Cho Min Hee</name>
    <uri>http://localhost:4000</uri>
    <email>aezjk56@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;이번엔 베타 분포(Beta Distribution)에 대해 알아보자&lt;/p&gt;

&lt;h3 id=&quot;베타-분포&quot;&gt;베타 분포&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;베타 분포는 확률에 대한 확률 분포이다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;먼저, 베타 분포의 확률 밀도 함수를 살펴보자
\[ X \sim Beta(\alpha,\beta )\]
\[ f_{x}(x) = \frac{1}{\beta(\alpha,\beta)}d^{\alpha-1}(1-x)^{\beta-1} \qquad (0 &amp;lt; x &amp;lt;1,\alpha,\beta &amp;gt;0) \]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;이전에 했던 감마 분포가 대기 시간에 대한 분포이기 때문에 x의 범위가 0보다 컸다.&lt;/li&gt;
  &lt;li&gt;베타 분포는 확률에 대한 분포이기 때문에 x의 범위가 0에서 1사이 이다.&lt;/li&gt;
  &lt;li&gt;여기서 α-1, β -1 는 각각 어떤 사건의 ‘성공 횟수’,’실패 횟수’로 볼 수 있다.&lt;/li&gt;
  &lt;li&gt;β(α,β)는 확률 밀도 함수의 0부터 1사이의 integral sumdl 1이 되도록 하는 상수이다.
\[ 1 = \int_{0}^{1}\frac{1}{\beta(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}dx \]
\[ \beta(\alpha,\beta) = \int_{0}^{1}x^{\alpha-1}(1-x)^{\beta-1}dx \]&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 상수는 감마함수로 표현된다(&lt;a href=&quot;https://jjomaeng.github.io/blog/감마-분포/&quot;&gt;감마 함수에 대한 자세한 내용은 여기에 작성하였습니다!&lt;/a&gt;)
\[ \beta(\alpha,\beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;왜 감마 함수의 분수로 증명되는지는 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/40.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/40.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;이항-분포의-켤레-사전-분포&quot;&gt;이항 분포의 켤레 사전 분포&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;베타 분포의 확률 밀도 함수에서 앞의 상수항만 떼고 보면 이항 분포의 확률 질량 함수와 닮아 있다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이항 분포에서는 확률 p가 고정된 것이고, 성공 횟수 및 실패 횟수가 확률 변수인 반면, 베타 분포에서는 성공 횟수(α -1)와 실패 횟수(β-1)가 고정된 것이고, 확률이 확률 변수이다.
\[ beta = x^{\alpha -1}(1-x)^{\beta -1}\] 
\[ binomial = p^{x}(1-p)^{n -x}\]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;따라서, 베타 분포는 베이지안 방법에서 이항 분포의 켤레 사전 분포로 활용된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;베타-분포의-기대값과-분산&quot;&gt;베타 분포의 기대값과 분산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;마지막으로 베타 분포의 기대값과 분산을 구하고 마무리하자.&lt;/li&gt;
  &lt;li&gt;먼저, 기대값을 구해보자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/41.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/41.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;다음은 분산을 구해보자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;a href=&quot;/PRML/42.jpeg&quot; alt=&quot;image&quot;&gt;&lt;img src=&quot;/PRML/42.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/blog/%E1%84%87%E1%85%A6%E1%84%90%E1%85%A1-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%91%E1%85%A9/&quot;&gt;베타 분포&lt;/a&gt; was originally published by Cho Min Hee at &lt;a href=&quot;http://localhost:4000&quot;&gt;MinPy&lt;/a&gt; on February 22, 2022.&lt;/p&gt;
  </content>
</entry>

</feed>