I"y<p class="notice">이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<h3 id="가우시안-분포의-최대-가능도">가우시안 분포의 최대 가능도</h3>

<p>책에는 없지만, 앞의 내용을 복습할 겸 다변량 가우시안 분포부터 자세히 설명할 예정이다.</p>
<ul>
  <li>먼저, 다변량 가우시안 분포부터 살펴보자.</li>
  <li>데이터 집합  \(X = ( x_{1},\cdots,x_{D} )^{T}\)이 주어졌으며, 관측값 \( {X_{d}}\)들이 다변량 가우시안 분포로 부터 독립적으로 추출되었다고 가정해 보자.</li>
  <li>이때 원 분산의 매개변수들을 최대 가능도 방법을 이용하여 추정할 수 있다.</li>
</ul>

\[X = \begin{pmatrix}
X_{1} \\
X_{2} \\
\vdots  \\
X_{d}
\end{pmatrix} \sim MVN(\mu,\Sigma)\]

<ul>
  <li>
    <p>여기서 \(\mu \)는 d x 1 벡터, \( \Sigma\)는 d x d행렬이 된다.</p>
  </li>
  <li>이러한 다변량 가우시안 분포의 pdf는 다음과 같이 쓸 수 있다.
\[f(x_{i};\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}}\left| \Sigma \right|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \]</li>
  <li>만약 우리가 n개의 샘플을 가지고 있다고 가정 할 경우에 이 것들의 가능도 함수는 다음과 같이 각 샘플들을 pdf에 집어넣은 것들을 곱한 꼴로 나타낼 수 있다.
\[ L(x;\mu,\Sigma) = \prod_{i = 1}^{n}f(x_{i})= \prod_{i = 1}^{n}\left [ \frac{1}{(2\pi)^{\frac{d}{2}}\left|\Sigma \right|^{\frac{1}{2}}} exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \right ] \]</li>
  <li>양변에 로그 함수를 취하여 로그 가능도 함수를 만들면 다음과 같다.</li>
</ul>

\[lnp(x \mid \mu,\Sigma) = \sum_{i=1}^{n}(-\frac{d}{2}log\pi - \frac{1}{2}log\left| \Sigma \right|-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x-\mu))\]

\[\qquad \qquad  \qquad \qquad \qquad \quad = -\frac{ND}{2}ln(2\pi) - \frac{N}{2}ln\left| \Sigma \right| -\frac{1}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{T}\Sigma^{-1}(x_{n}-\mu) \qquad 식(2.118)\]

<p>최대 가능도(MLE)는 위의 로그 가능도 함수를 최대로 만드는 모수 \(\mu \) 와 \(\Sigma \)의 값을 구하는 것이다.</p>
<ul>
  <li>따라서 첫번째 항은 두 개의 모수와 관계 없으므로 생략할 수 있다.</li>
  <li>그러면 이제, MLE를 구해보자</li>
  <li>구하기에 앞서 벡터 편미분에 대한 기본적인 개념을 알아야한다.
    <ul>
      <li>모수 \( \mu \)와 \(\Sigma \)로 각각 편미분하기 위하여 간단하게만 살펴보자.</li>
      <li>a 와 b가 p x 1 벡터이고, A가 p x p 상수로 이루어진 행렬일 때, 다음의 미분식이 성립한다.</li>
    </ul>

    <ol>
      <li>\( \frac{\partial a^{T}b}{\partial b} =a  \)</li>
      <li>\( \frac{\partial a^{T}b}{\partial a} =b \)</li>
      <li>\( \frac{\partial b^{T}Ab}{\partial B} =(A + A^{T})b \)</li>
      <li>\( \frac{\partial log \mid A \mid}{\partial A} =(A^{-1})^{T} \)</li>
      <li>\( \frac{\partial tr(AB)}{\partial A} = B^{T} \)</li>
    </ol>
  </li>
</ul>

<p>이제, \( \mu \)벡터에 대하여 로그 가능도 함수를 편미분 해보자.</p>
<ul>
  <li>\( \mu\)를 포함하고 있는 항만 살펴보면 된다.</li>
  <li>이 항은 다음과 같은 함수 f의 합의 꼴로 볼 수 있다.
\[ f(\mu) = (x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu) \]</li>
</ul>

<figure>
    <a href="/PRML/43.jpeg" alt="image"><img src="/PRML/43.jpeg" alt="image" /></a>
</figure>

<ul>
  <li>마지막으로 위에서 구한 로그 가능도 함수의 편미분 벡터를 영벡터로 만드는 벡터 \( \mu \)의 값을 찾자.</li>
</ul>

<figure>
    <a href="/PRML/44.jpeg" alt="image"><img src="/PRML/44.jpeg" alt="image" /></a>
</figure>

<ul>
  <li>정리하면 평균에 대한 최대 가능도 추정값의 해는 다음과 같다.
\[ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n} \qquad 식(2.121)\]</li>
</ul>

<p>이제, 로그 가능도 함수를 \( \Sigma \)행렬로 편미분해보자.</p>

<ul>
  <li>
    <p>마찬가지로 \( \Sigma \)를 로그가능도에서 찾아보면 다음과 같다.
\[ lnp(X \mid \mu,\Sigma) \propto -\frac{n}{2}log \mid \Sigma \mid -\frac{1}{2} \sum_{i=1}^{n}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu) \]</p>
  </li>
  <li>
    <p>행렬 미적분에서 2차 형식(Quadratic form)으로 나타내어진 행렬에 대해 미분하는 방법이 역랭렬로 나타내어진 것을 미분하는 것보다 훨씬 쉽기 때문에, \( \Sigma^{-1} \)로 미분하여 진행.</p>
  </li>
</ul>

<figure>
    <a href="/PRML/45.jpeg" alt="image"><img src="/PRML/45.jpeg" alt="image" /></a>
</figure>

<ul>
  <li>그러면 이제 행렬 A로 미분해보자</li>
</ul>

<figure>
    <a href="/PRML/46.jpeg" alt="image"><img src="/PRML/46.jpeg" alt="image" /></a>
</figure>

<ul>
  <li>정리하면 다음과 같다.
\[ \Sigma_{ML} = \frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})(x_{n}-\mu_{ML})^{T} \qquad 식(2.122)\]</li>
</ul>

<h3 id="순차-추정">순차 추정</h3>

<ul>
  <li>순차 추정 방식은 데이터 포인트들을 하나씩 처리하고 버리는 방식이다,</li>
  <li>이는 다음과 같은 상황에서 유용하게 사용할 수 있다.
    <ul>
      <li>관찰 데이터 집합이 매우 커서 한번에 계산이 불가능할 때</li>
      <li>실시간 처리를 해야하는 온라인 적용 사례</li>
    </ul>
  </li>
  <li>MLE로 얻어진 \( \mu_{ML}\)식을 업데이트 되는 방식으로 바꿔보자.
    <ul>
      <li>
        <p>\( \mu_{ML}\)에서 마지막 데이터 포인트 \(X_{N}\)따로 빼내어 보자
  \[ \mu_{ML}^{(N)} = \frac{1}{N}\sum_{n=1}^{N}X_{n} = \frac{1}{N}X_{N} +\frac{1}{N}\sum_{n=1}^{ N-1}X_{n} = \frac{1}{N}X_{N} + \frac{N-1}{N}\mu_{ML}^{(N-1)} = \mu_{ML}^{(N-1)} + \frac{1}{N}(X_{N} - \mu_{ML}^(N-1)) \qquad  식(2.126)\]</p>
      </li>
      <li>N-1개의 데이터로부터 추정된 \( \mu_{ML}^{(N-1)}\)와 N번째 관측된 데이터를 이용하여 \( \mu_{ML}^{(N)}\)을 구한다.</li>
      <li>N의 값이 증가할수록 새로 관측되는 데이터의 기여도가 점점 작아지게 된다.</li>
      <li>한번에 계산을 처리하는 배치 방식으로부터 식을 유도했기 때문에 실제 결과는 동일하게 된다.</li>
      <li>이런 방식은 매우 유용하지만, 배치 방식의 식에서 업데이트 방식의 식을 항상 유도할 수 있는 것은 아니다.</li>
      <li>따라서 순차적인 학습에 대한 더 일반적인 방식인 로빈스 몬로(Robbins &amp; Monro) 알고리즘을 알아보자</li>
    </ul>
  </li>
</ul>

<h4 id="robins--monro-알고리즘">Robins &amp; Monro 알고리즘</h4>

<ul>
  <li>두 개의 확률 변수 z와 \(\theta \)가 주어졌을 때 이 분포의 결합 분포는 \( p(z,\theta) \)이다.</li>
  <li>\(\theta \)가 주어졌을 때의 z의 조건부 기댓값으로 결정 함수 \( f(\theta) \)를 정의할 수 있다.
\[ f(\theta) = E[z \mid \theta ] = \int zp (z \mid \theta ) dz \]</li>
  <li>이러한 함수를 회귀 함수라고 한다.</li>
  <li>우리의 목표는 \( f(\theta^{<em>}) = 0 \)을 만족하는 \( \theta ^{</em>}\)을 찾는 것이다.</li>
</ul>
<figure>
    <a href="/PRML/46.jpeg" alt="image"><img src="/PRML/46.jpeg" alt="image" /></a>
</figure>
:ET