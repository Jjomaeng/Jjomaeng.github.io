I"<p class="notice">이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<h3 id="가우시안-분포의-최대-가능도">가우시안 분포의 최대 가능도</h3>

<ul>
  <li>책에는 없지만, 앞의 내용을 복습할 겸 다변량 가우시안 분포부터 자세히 설명할 예정이다.</li>
  <li>먼저, 다변량 가우시안 분포부터 살펴보자.</li>
  <li>데이터 집합  \(X = ( x_{1},\cdots,x_{D} )^{T}\)이 주어졌으며, 관측값 \( {X_{d}}\)들이 다변량 가우시안 분포로 부터 독립적으로 추출되었다고 가정해 보자.</li>
  <li>이때 원 분산의 매개변수들을 최대 가능도 방법을 이용하여 추정할 수 있다.
\(X = \begin{pmatrix}
X_{1} \\
X_{2} \\
\vdots  \\
X_{d}
\end{pmatrix} \sim MVN(\mu,\Sigma)\)</li>
  <li>
    <p>여기서 \(\mu \)는 d x 1 벡터, \( \Sigma\)는 d x d행렬이 된다.</p>
  </li>
  <li>이러한 다변량 가우시안 분포의 pdf는 다음과 같이 쓸 수 있다.
\[f(x_{i};\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}}\left| \Sigma \right|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \]</li>
  <li>만약 우리가 n개의 샘플을 가지고 있다고 가정 할 경우에 이 것들의 가능도 함수는 다음과 같이 각 샘플들을 pdf에 집어넣은 것들을 곱한 꼴로 나타낼 수 있다.
\[ L(x;\mu,\Sigma) = \prod_{i = 1}^{n}f(x_{i})= \prod_{i = 1}^{n}\left [ \frac{1}{(2\pi)^{\frac{d}{2}}\left|\Sigma \right|^{\frac{1}{2}}} exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \right ] \]</li>
  <li>양변에 로그 함수를 취하여 로그 가능도 함수를 만들면 다음과 같다.
\[\]</li>
</ul>

:ET