I"<p class="notice">이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>이번에는 패턴 인식과 머신 러닝 테크닉을 이해하는 데 있어서 중요한 역할을 하게 될 또 하나의 이론인 정보 이론에 대해 살펴보고자 한다.
먼저, 엔트로피를 포함한 정보 이론의 중요한 개념들에 대해 살펴보자.</p>

<p>이산 확률 변수 x를 고려했을 때, x의 값을 학습하는 데 있어서 정보의 양은 ‘놀라움의 정도’라고 생각할 수 있을 것이다.
따라서 우리가 사용하게 될 정보량의 측정 단위는 확률 분포 p(x)에 종속적이게 된다.</p>

<p>지금부터 p(x)에 대해 단조 함수인 정보량을 표현하는 함수 h(x)에 대해 살펴보도록 하자.
서로 연관되어 있지 않은 두 사건 x와 y를 고려해 보면, 이 경우 x와 y가 함께 일어났을 때 얻는 정보량은 각자의 사건이 따로 일어났을 때 얻는 정보량의 합이 될 것이다. 따라서 h(x,y) = h(x) + h(y)가 된다. 연관되어 있지 않은 두 사건은 통계적으로 독립적이며 따라서 p(x,y) = p(x)p(y)다. 이 관계로부터 h(x)는 p(x)의 로그에 해당하는 것을 보일 수 있다. 이에 따라 다음 식을 얻게 된다.
\[h(x) = -log_{2}p(x) \]</p>
<figure>
    <a href="/PRML/1.png" alt="image"><img src="/PRML/5.png" alt="image" /></a>
</figure>

:ET