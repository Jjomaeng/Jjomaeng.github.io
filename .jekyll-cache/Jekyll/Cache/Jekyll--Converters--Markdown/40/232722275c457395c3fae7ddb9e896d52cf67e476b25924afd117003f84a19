I"h<p class="notice">이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<h3 id="가우시안-분포의-최대-가능도">가우시안 분포의 최대 가능도</h3>

<p>책에는 없지만, 앞의 내용을 복습할 겸 다변량 가우시안 분포부터 자세히 설명할 예정이다.</p>
<ul>
  <li>먼저, 다변량 가우시안 분포부터 살펴보자.</li>
  <li>데이터 집합  \(X = ( x_{1},\cdots,x_{D} )^{T}\)이 주어졌으며, 관측값 \( {X_{d}}\)들이 다변량 가우시안 분포로 부터 독립적으로 추출되었다고 가정해 보자.</li>
  <li>이때 원 분산의 매개변수들을 최대 가능도 방법을 이용하여 추정할 수 있다.</li>
</ul>

\[X = \begin{pmatrix}
X_{1} \\
X_{2} \\
\vdots  \\
X_{d}
\end{pmatrix} \sim MVN(\mu,\Sigma)\]

<ul>
  <li>
    <p>여기서 \(\mu \)는 d x 1 벡터, \( \Sigma\)는 d x d행렬이 된다.</p>
  </li>
  <li>이러한 다변량 가우시안 분포의 pdf는 다음과 같이 쓸 수 있다.
\[f(x_{i};\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}}\left| \Sigma \right|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \]</li>
  <li>만약 우리가 n개의 샘플을 가지고 있다고 가정 할 경우에 이 것들의 가능도 함수는 다음과 같이 각 샘플들을 pdf에 집어넣은 것들을 곱한 꼴로 나타낼 수 있다.
\[ L(x;\mu,\Sigma) = \prod_{i = 1}^{n}f(x_{i})= \prod_{i = 1}^{n}\left [ \frac{1}{(2\pi)^{\frac{d}{2}}\left|\Sigma \right|^{\frac{1}{2}}} exp(-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu)) \right ] \]</li>
  <li>양변에 로그 함수를 취하여 로그 가능도 함수를 만들면 다음과 같다.</li>
</ul>

\[lnp(x \mid \mu,\Sigma) = \sum_{i=1}^{n}(-\frac{d}{2}log\pi - \frac{1}{2}log\left| \Sigma \right|-\frac{1}{2}(x_{i} - \mu)^{T}\Sigma^{-1}(x-\mu))\]

\[\qquad \qquad  \qquad \qquad \qquad \quad = -\frac{ND}{2}ln(2\pi) - \frac{N}{2}ln\left| \Sigma \right| -\frac{1}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{T}\Sigma^{-1}(x_{n}-\mu) \qquad 식(2.118)\]

<p>최대 가능도(MLE)는 위의 로그 가능도 함수를 최대로 만드는 모수 \(\mu \) 와 \(\Sigma \)의 값을 구하는 것이다.</p>
<ul>
  <li>따라서 첫번째 항은 두 개의 모수와 관계 없으므로 생략할 수 있다.</li>
  <li>그러면 이제, MLE를 구해보자</li>
  <li>구하기에 앞서 벡터 편미분에 대한 기본적인 개념을 알아야한다.
    <ul>
      <li>모수 \( \mu \)와 \(\Sigma \)로 각각 편미분하기 위하여 간단하게만 살펴보자.</li>
      <li>a 와 b가 p x 1 벡터이고, A가 p x p 상수로 이루어진 행렬일 때, 다음의 미분식이 성립한다.</li>
    </ul>

    <ol>
      <li>\( \frac{\partial a^{T}b}{\partial b} =a  \)</li>
      <li>\( \frac{\partial a^{T}b}{\partial a} =b \)</li>
      <li>\( \frac{\partial b^{T}Ab}{\partial B} =(A + A^{T})b \)</li>
      <li>\( \frac{\partial log \mid A \mid}{\partial A} =(A^{-1})^{T} \)</li>
      <li>\( \frac{\partial tr(AB)}{\partial A} = B^{T} \)</li>
    </ol>
  </li>
</ul>

<p>이제, \( \mu \)벡터에 대하여 로그 가능도 함수를 편미분 해보자.</p>
<ul>
  <li>\( \mu\)를 포함하고 있는 항만 살펴보면 된다.</li>
  <li>이 항은 다음과 같은 함수 f의 합의 꼴로 볼 수 있다.
\[ f(\mu) = (x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - \mu) \]</li>
</ul>

<figure>
    <a href="/PRML/43.jpeg" alt="image"><img src="/PRML/43.jpeg" alt="image" /></a>
</figure>

<ul>
  <li>마지막으로 위에서 구한 로그 가능도 함수의 편미분 벡터를 영벡터로 만드는 벡터 \( \mu \)의 값을 찾자.</li>
</ul>

<figure>
    <a href="/PRML/44.jpeg" alt="image"><img src="/PRML/44.jpeg" alt="image" /></a>
</figure>

<ul>
  <li>정리하면 평균에 대한 최대 가능도 추정값의 해는 다음과 같다.
\[ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n} \qquad 식(2.121)\]</li>
</ul>

<p>이제, 로그 가능도 함수를 \( \Sigma \)행렬로 편미분해보자.</p>

<ul>
  <li>
    <p>마찬가지로 \( \Sigma \)를 로그가능도에서 찾아보면 다음과 같다.
\[ lnp(X \mid \mu,\Sigma) \propto -\frac{n}{2}log \mid \Sigma \mid -\frac{1}{2} \sum_{i=1}^{n}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu) \]</p>
  </li>
  <li>
    <p>행렬 미적분에서 2차 형식(Quadratic form)으로 나타내어진 행렬에 대해 미분하는 방법이 역랭렬로 나타내어진 것을 미분하는 것보다 훨씬 쉽기 때문에, \( \Sigma^{-1} \)로 미분하여 진행.</p>
  </li>
</ul>

<figure>
    <a href="/PRML/45.jpeg" alt="image"><img src="/PRML/45.jpeg" alt="image" /></a>
</figure>

<ul>
  <li>그러면 이제 행렬 A로 미분해보자</li>
</ul>

<figure>
    <a href="/PRML/46.jpeg" alt="image"><img src="/PRML/46.jpeg" alt="image" /></a>
</figure>

<ul>
  <li>정리하면 다음과 같다.
\[ \Sigma_{ML} = \frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})(x_{n}-\mu_{ML})^{T} \qquad 식(2.122)\]</li>
</ul>

:ET