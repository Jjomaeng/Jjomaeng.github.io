I"r/<p>이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>이 장에서 논의할 분포의 역할들 중 하나는 한정된 수의 관찰 집합 x1,….,xn이 주어졌을 때 확률 변수 x의 확률 분포 p(x)를 모델링하는 것이다. 이를 <b>밀도 추정 문제 </b> 라고 한다.
이 장의 목표를 위해서 데이터 포인트들은 독립적이며, 동일하게 분포되어 있다고 가정할 것이다.</p>

<p>우선, 이산 확률 변수의 이항 분포와 다항 분포를 살펴보고 그 다음으로 연속 확률 변수의 가우시안 분포에 대해 논의할 것이다. 이 분포들은 매개변수 분포의 예다. 
이러한 모델을 밀도 추정 문제에 적용하기 위해서는 관찰된 데이터 집합을 바탕으로 적절한 매개변숫값을 구하는 과정이 필요하다.<br />
두 가지 관점에서 살펴보자</p>
<ul>
  <li>빈도적 관점 : 어떤 특정 기준을 최적화하는 방법으로 매개변수를 찾는다. 최적화 기준의 예로 가능도 함수가 있다.</li>
  <li>베이지안 관점 : 매개변수에 대한 사전 분포를 바탕으로 관측된 데이터 집합이 주어졌을 때의 해당 사후분포를 계산한다.</li>
</ul>

<p>켤레 사전 확률이 중요한 역할을 한다는 것도 살펴볼 것이다. 켤레 사전 확률은 사후 확률이 사전 확률과 같은 함수적 형태를 띠도록 만들어준다. 그 결과 , 베이지안 분석이 매우 단순해진다.</p>

<p>매개변수적인 접근법의 한계점도 존재한다. 분포가 특정한 함수 형태를 띠고있다고 가정하지만 몇몇 적용 사례의 경우에는 이 가정이 적절하지 않다. 이런 경우, 비매개변수적 밀도 추정 방식을 대안으로 활용할 수 있다.</p>

<h4 id="이산-확률-변수">이산 확률 변수</h4>

<p>하나의 이진 확률 변수 \( x \in (0,1) \)을 고려해 보자. 0과 1이 나올 확률이 동일하지 않다고 가정하면 이때 x = 1일 확률은 매개변수 \( \mu \)를 통해 다음과 같이 표현할 수 있다.
\[ p(x = 1 \mid \mu) = \mu (0 \leq \mu \leq 1)\]
\[ p(x = 0 \mid \mu ) = 1 - \mu \]</p>

<p>따라서 x에 대한 확률은 다음의 형태로 적을 수 있다.</p>

<p>\[ Bern(x \mid \mu ) = \mu^{x}(1 - \mu)^{1-x} \]</p>

<p>이것을 <b> 베르누이 분포 </b>라고 한다. 베르누이 분포는 정규화되어 있으며, 그 평균과 분산이 다음과 같이 주어진다.</p>

<p>\[ E(x) = \mu \]
\[ var(x) = \mu(1-\mu) \]</p>

<p>x의 관극값이 데이터 집합 \( D = (x_{1},…..x_{N})\)이 주어졌다고 하자. 관측값들이 \( p(x \mid \mu)\)에서 독립적으로 추출되었다는 가정하에 \( \mu \)함수로써 가능도 함수를 구성할 수 있다.
\[ p(D \mid \mu ) = \prod_{n=1}^{N}p(x_{n} | \mu) = \prod_{n=1}^{N}\mu^{x_{n}}(1-\mu)^{1 - x_{n}} \]</p>

<p>빈도적 관점에서는 가능도 함수를 최대화하는 \( \mu \)를 찾아서 \( \mu \)값을 추정할 수 있다. 베르누이 분포의 경우 로그 가능도 함수는 다음으로 주어진다.</p>

<p>\[ lnp(D \mid \mu ) = \sum_{n=1}^{N}lnp(x_{n} \mid \mu) = \sum_{n=1}^{N}(x_{n}ln\mu + (1-x_{n})ln(1-\mu)) \]</p>

<p>로그 가능도함수는 오직 관측값들의 합인 \(\sum_{n}x_{n}\)을 통해서만 N개의 관측값 \(x_{n} \)과 연관된다는 점에 주목할 필요가 있다. 이 합은 <b> 충분 통계량 </b>의 예시 중 하나다.</p>

<p>\( lnp(D \mid \mu )\)을 \(\mu \)에 대해 미분하고 이를 0과 같다고 놓으면 다음과 같은 최대 가능도 추정값을 구할 수 있다.</p>

<p>\[ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n}\]</p>

<p>이는 <b> 표본 평균 </b>이라고도 불린다. 데이터에서 x = 1인 관찰값의 수를 m이라고 하면 다음의 형태로 다시 적을 수 있다. 
\[ \mu_{ML} = \frac{m}{N} \]</p>

<p>즉, 최대가능도 체계하에에서 동전으로 예시를 들면, 동전의 앞면이 나올 확률은 데이터 집합에서 앞면이 나온 비율로 주어지게 되는 것이다.</p>

<p>하지만 최대가능도를 사용할 경우 과적합이 발생할 수 있다.</p>

<p>동전을 세 번 던졌는데 세 번 다 앞면이 나온 경우를 생각해보자. 그러면 N = m = 3이고 따라서 \( \mu_{ML} = 1 \)이 된다( 이 경우 앞으로 던지는 모든 동전이 앞면으로 나올 것이라고 예측하는 것이다! ) 이것이 실제로 최대 가능도를 사용했을 경우 발생할 수 있는 극단적인 사례다.( 이후에 사전 분포를 바탕으로 나은 결과를 도출할 수 있다.)</p>

<p>크기 N의 데이터가 주어졌을 때 x = 1인 관측값의 수 m에 대해서도 분포를 생각할 수 있다. 이를 <b> 이항 분포 </b>라 한다. 위에서 구한 가능도 함수로부터 이항 분포는 \( \mu^{m}(1 - \mu)^{N - M}\)에 비례한다는 것을 알 수 있다.</p>

<p>정규화 계수를 구하기 위해서 동전 던지기로 예시를 들면, 동전 던지기를 N번 했을 때 앞면이 m번 나올 수 있는 가능한 모든 가짓수를 구해야 한다. 따라서 이항 분포를 다음과 같이 적을 수 있다.
\[Bin(m \mid N,\mu) = \binom{N}{m}\mu^{m}(1-\mu)^{N-m} \]</p>

<p>여기서 다음과 같다.</p>

<p>\[ \binom{N}{m} \equiv \frac{N!}{(N-m)!m!} \]</p>

<p>\( \binom{N}{m} \)은 N개의 물체들 중 m개의 물체를 선별하는 가짓수를 구한 것이다. 이해를 위해 N = 10이고 \(\mu \)= 0.25일 때의 이항 분포의 히스토그램은 다음과 같다.</p>

<figure>
    <a href="/PRML/8.png" alt="image"><img src="/PRML/8.png" alt="image" /></a>
</figure>

<h4 id="베타-분포">베타 분포</h4>

<p>이미 살펴본 방법들은 데이터 수가 적을 때 심각한 과적합이 일어나기 쉽다(동전 던지기 예시를 생각해보자!)</p>

<p>이 문제에 베이지안적으로 접근하기 위해서는 매개변수 \( \mu \)에 대한 사전 분포 \( p(\mu) \)를 도입하는 것이 필요하다.</p>

<p>가능도 함수가 \( \mu^{x}(1-x)^{1-x}\)의 형태를 가지는 인자들의 곱의 형태를 띠고 있다는 것에 주목해보면, 만약 \(\mu \)dhk \( (1 - \mu) \)의 거듭제곱에 비례하는 형태를 사전 분포에 선택한다면, 사전 확률과 가능도 함수의 곱에 비례하는 사후 분포 역시 사전 분포와 같은 함수적 형태를 가지게 될 것이다. 이러한 성질을 <b> 켤레성</b>이라고 한다. ( 켤레성에 대한 이해를 돕고자 켤레의 수학적 의미를 설명하면, 두 개의 점,선,도형,수 등이 서로 대칭이거나 보완적인 관계에 있는 경우 켤레라고 한다.)</p>

<p>지금까지의 논의를 바탕삼아 사전 분포로 베타 분포를 사용할 것이다.( <a href="https://jjomaeng.github.io/blog/베타-분포/">왜 베타 분포를 사용하는 지에 대한 자세한 설명은 여기에 작성하였습니다!</a>)</p>

<p>베타 분포는 다음의 형태를 가진다.
\[ Beta(\mu \mid a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\]</p>

<p>\( \Gamma(x) \)는 다음과 같이 정의된다. (<a href="https://jjomaeng.github.io/blog/감마-분포/">감마 함수에 대한 자세한 내용은 여기에 작성하였습니다!</a>)</p>

<p>\[ \Gamma(x) = \int_{0}^{\infty } u^{x-1} e^{-1} du \]</p>

<p>베타 분포의 계수들은 베타 분포가 정규화되도록 한다.</p>

<p>\[ \int_{0}^{1}Beta(\mu \mid a,b) d\mu = 1 \]</p>

<p>베타 분포의 평균과 분산은 다음과 같이 주어진다.</p>

<p>\[ E(\mu) = \frac{a}{a+b} \]
\[ var(\mu) = \frac{ab}{(a+b)^{2}(a+b+1)} \]</p>

<p>매개변수 a와 b는 이들이 매개변수 \( \mu \)의 분포를 조절하기 때문에 <b>초매개변수</b>라고 불린다.</p>

<p>이제 베타 사전 분포와 이항 가능도 함수를 곱한 후 정규화를 시행함으로써 \( \mu \)의 사후 분포를 구할 수 있다.
\( \mu \)와 관련되어 있는 인자들만 남기면 사후 분포가 다음의 형태를 가지게 되는 것을 확인할 수 있다.</p>

<p>\[ p(\mu \mid m,l,a,b) \propto \mu^{m+a-1}(1 - /m)^{l+b-1}\]</p>

<p>여기서 l = N -m이며 동전 던기기 예시에서 ‘뒷면’의 개수에 해당한다.</p>

<p>사후 분포의 식은 사전 분포와 \(\mu \)에 대해서 같은 함수적 종속성을 가지고 있다는 것을 확인할 수 있다. 이 사실이 가능도 함수에 대해서 사전 분포가 켤레성을 가지고 있다는 것을 반영한다. 
실제로 사후 분포는 또 다른 베타 분포일 뿐이다.
그러면 사후 분포의 정규화 계수는 베타 분포의 식을 참고하여 구할 수 있다. 그 결과 다음을 얻게 된다.</p>

<p>\[ p(\mu \mid m,l,a,b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)} \mu^{m+a-1}(1-\mu)^{l+b-1}\]</p>

<p>x = 1인 값이(예를 들어 앞면의 수) m개 있고 x = 0인 값 ㅣ개가 있는 데이터 집합을 관찰한 결과, 사전 분포와 비교했을 떄 사후 분포에서는 a의 값이 m만큼, b의 값이 l만큼 증가하는 것을 확인할 수 있다.
이 사실로부터 사전 분포의 초매개변수 a와b를 각각 x = 1, x = 0인 경우에 대한 <b>유효 관찰수</b>로 해석할 수 있다.(a와 b는 반드시 정수가 아니어도 된다.)</p>

<p>만약 우리가 추가적으로 관측 데이터를 얻게 되면 지금의 사후 분포가 새로운 사전 분포가 될 수 있다. 
매번 업데이트 단계에서 새로운 관측값에 해당하는 가능도 함수를 곱하고 그 다음에 정규화를 시행해서 새로운 수정된 사후 분포를 <b>순차적</b>으로 얻는 것이다.
각 단계에서 사후 분포는 x = 1과 x = 0에 해당하는 관측값들의 전체 숫자가 새로운 a와 b값으로 주어지는 베타 분포에 해당한다. x = 1인 새로운 관측값이 주어졌을 때는 단순히 a값을 1 증가시키고 x = 0인 관측값이 새로 주어졌을 때는 b값을 1 증가시키면 된다.
이 과정의 한 단계를 그림으로 보면 다음과 같다.</p>

<figure>
    <a href="/PRML/9.png" alt="image"><img src="/PRML/9.png" alt="image" /></a>
</figure>

<p>이는 사전 분포나 가능도 함수의 선택과는 상관없이 오직 데이터가 독립적이고 동이랗게 분포되었다는 것에만 의존적이다.
순차적인 방법론은 관측값을 한 번에 하나씩, 혹은 한 번에 적은 수 만큼 사용한다.
사용한 관측값들을 다음 관측값을 사용하기 전에 버린다. 
이는 실시간 학습에 적용할 수 있으며, 큰 데이터 집합에도 사용 가능하다.</p>

<p>만약 우리의 목표가 다음 시도의 결과값을 가장 잘 예측하는 것이라면, 관측 데이터 집합 D가 주어진 상황하에서 x의 예측 분포를 계산해야 한다.
확률의 합과 곱의 법칙에 따라서 이는 다음 형태를 띤다.</p>

<p>\[ p(x = 1 \mid D) = \int_{0}^{1}p(x=1 \mid \mu)p(\mu \mid D)d\mu = \int_{0}^{1}\mu p(\mu \mid D )d\mu = E(\mu \mid D)\]</p>

<p>사후 분포 \(p(\mu \mid D)\)에 대한 결과 식과 베타 분포의 평균에 대한 식을 이용해서 다음을 구할 수 있다.</p>

<p>\[ p(x = 1 \mid D) = \frac{m+a}{m_a+l+b}\]</p>

<p>전체 관측값(실제 관측값과 허구의 사전 관측값 둘 다)중에서 x = 1인 관측값의 비율로 해석할 수 있다.</p>

<p>데이터 집합이 무한이 커서 \( m,l -&gt; \infty \)이 된다면 최대가능도의 결과값과 같아진다.
(베이지안 결과값과 최대 가능도의 결과값이 무한하게 큰 데이터 집합 하에서 동일한 것은 매우 일반적인 성향이다.)</p>

<p>제한된 크기의 데이터 집합에서 \(\mu \)의 사후 평균 값은 사전 평균값과 사건의 상대적 빈도수를 바탕으로 한 가능도 추정치 사이에 있게 된다.</p>

:ET