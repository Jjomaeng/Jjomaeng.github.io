I"$<p class="notice">이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<h4 id="추론과-결정">추론과 결정</h4>

<p>지금까지 분류 문제를 두 개의 단계로 나누어 보았다. 첫 번째는 추론 단계로 훈련 집단을 활용하여 \(p(C_{k} \mid x) \)에 대한 모델을 학습시키는 단계. 두 번째는 결정 단계로 학습된 사후 확률들을 이용해서 최적의 클래스 할당을 시행하는 것이다. 두 가지 문제를 한 번에 풀어내는 방식도 생각해 볼 수 있다. x가 주어졌을 때 결정값을 돌려주는 함수를 직접 학습시키는 것이다. 이러한 함수를 판별함수라고 한다.</p>

<p>결정 문제를 푸는 데는 세 가지 다른 접근법이 있다. 이를 표로 정리하면 다음과 같다.</p>

<figure>
    <a href="/PRML/21.png" alt="image"><img src="/PRML/21.png" alt="image" /></a>
</figure>
<p>하지만 (c)방식을 사용할 경우 사후 확률을 알지 못하게 되는데, 사후 확률을 구하는 것에는 유의미한 여러 가지 이유가 있다.</p>

<p>① 위험의 최소화
손실 행렬의 값들이 때때로 변하는 문제를 고려했을 때, 사후 확률을 알고 있다면 앞 서 설명한 기대 손실 식을 수정함으로써 쉽게 최소 위험 결정 기준을 구할 수 있다. 반면에 판별 함수만 알고 있을 경우에는 손실 행렬 값이 변할 때마다 훈련 집합을 활용하여 분류 문제를 새로 풀어야 할 것이다.</p>

<p>② 거부 옵션
사후 확률을 알고 있으면 주어진 거부 데이터 포인트 비율에 대해 오분류 비율(기대 손실값)을 최소화하는 거부 기준을 쉽게 구할 수 있다.</p>

<p>③ 클래스 사전 확률에 대한 보상
클래스의 불균형을 가지는 데이터에 대해, 각각의 클래스에서 같은 숫자 예시를 선택한 균형 잡힌 데이터 집합을 활용하여 더 정확한 모델을 찾을 수 있다. 하지만 그렇게 할 경우에는 우리가 훈련 집합을 변경한 것에 대한 보상을 적용해야 한다. 이러한 수정된 데이터 집합을 사용하여 사후 확률에 대한 모델을 찾아낸다고 가정할 경우, 사후 확률 식은 베이지안 정리로부터 사전 확률에 비례함을 알 수 있다. 이 예시의 경우 사전 확률로는 각 클래스의 비율을 사용할 수 있다. 따라서 인공의 균형 잡힌 데이터 집합에서 구한 사후 확률을 인공 데이터 집합의 클래스 비율로 나누고, 여기에 우리가 실제로 모델을 적용할 모수 집합의 클래스이 비율을 곱함으로써 수정된 사후 확률을 구할 수 있다. 최종적으로는 새 사후 확률이 합이 1이 되도록 정규화해야 한다. 하지만 직접 판별 함수를 구하는 방식으로 학습을 시킨 경우에는 이러한 수정이 불가능하다.</p>

<p>④ 모델들의 결합
복잡한 응용 사례의 경우에는 하나의 큰 문제를 여러 개의 작은 문제로 나누어서 각각을 분리된 모듈로써 해결하는 것이 바람직한 경우가 있다. 이를 위한 한 가지 쉬운 방법은 각 클래스에 대해서 분리된 데이터 분포가 독립적이라고 가정하는 것이다. (분리된 데이터 \(X_{a},X_{b} \)라고 가정)</p>

<p>\[ p(x_{A},x_{B} \mid C_{k}) = p(x_{A} \mid C_{k})p(x_{B} \mid C_{k})\]</p>

<p>이는 조건부 독립의 성질의 예시다. 분포들이 클래스 Ck에 포함된다는 조건하에 독립적이기 때문이다. 이를 바탕으로 분리된 데이터들이 주어졌을 때의 사후 확률을 다음과 같이 구할 수 있다.
\[ p(C_{k}|x_{A},x_{B}) \propto p(x_{A},x_{B}|C_{k})p(C_{k}) \propto p(x_{A}|C_{k})p(x_{B}|C_{k})p(C_{k}) \propto \frac{p(C_{k}|x_{A})p(C_{k}|x_{B})}{p(C_{k})} \]</p>

<p>클래스 사전 확률 p(Ck)가 필요한데, 이는 데이터 포인트들의 각 클래스별 비율에서부터 쉽게 유추하는 것이 가능하다. 그리고 최종적으로 사후 확률을 정규화하여 합이 1이 되도록 하는 과정이 필요하다.  여기서 보인 특정 조건부 독립 가정은 나이브 베이즈 모델의 예시다. 결합 확률 분포 p(xa,xb)는 보통 나이브 베이즈 모델하에서는 인수분해가 되지 않는다. (조건부 독립 가정 없어도 데이터들을 결합시키는 방법은 이후에 나온다)</p>

<h4 id="회귀에서-손실-함수">회귀에서 손실 함수</h4>

<p>지금까지 분류 문제를 바탕으로 결정 이론에 대해 살펴보았다. 이제부터는 회귀 문제의 경우에 대해 살펴보도록 하자.
회귀 문제의 결정 단계에서는 각각의 x에 대해서 t의 추정값 y(x)를 선택해야 한다. 이 과정에서 손실 L(t,y(x))가 발생한다고 가정해보자. 그러면 평균(기대) 손실은 다음과 같이 주어진다.</p>

<p>\[ E(L) = \int \int L(t,y(\textbf{x}))p(\textbf{x},t)dxdt \]</p>

<p>손실 함수가 L(t,y(x)) = {y(X) - t}^2으로 주어지는 제곱 손실인 경우 기대 손실은 다음과 같다.</p>

<p>\[ E(L) = \int \int (y(\textbf{x})-t)^{2}p(\textbf{x},t)dxdt \]</p>

<p>우리의 목표는 E[L]을 최소화하는 y(x)를 선택하는 것이다. 만약 완벽하게 유연하게 함수 y(x)를 결정할 수 있다고 가정하면, 변분법을 적용해서 다음과 같이 적을 수 있다.</p>

<p>\[ \frac{\delta E(L)}{\delta y(x)} = 2\int (y(\textbf{x}) - t)p(\textbf{x},t)dt = 0 \]</p>

<p>y(x)에 대해서 해를 구하고 확률의 합과 곱의 법칙을 적용하면 다음을 얻게 된다.</p>

<p>\[ y(\textbf{x}) = \frac{\int tp(\textbf{x},t)dt}{p(x)} = \frac{\int tp(\textbf{x},t)dt}{p(\textbf{x})} = \int tp(t \mid \textbf{x})dt = E_{t}[t \mid \textbf{x}] \]</p>

<p>위의 식은 x가 주어졌을 때의 t의 조건부 평균으로써 회귀 함수라고 한다. 이에 대한 그림은 다음과 같다.</p>

<figure>
    <a href="/PRML/22.png" alt="image"><img src="/PRML/22.png" alt="image" /></a>
</figure>

<table>
  <tbody>
    <tr>
      <td>타깃 변수가 벡터 t로 표현되는 다차원 변수일 경우에 대해서도 이 결과를 쉽게 확장할 수 있다. 이 경우 최적의 해는 조건부 평균 y(x) = E[t</td>
      <td>x]다.</td>
    </tr>
  </tbody>
</table>

<p>이 결과는 약간 다른 방식을 통해서도 도출할 수 있다. 이 다른 도출 방법을 통해 회귀 문제의 본질에 대해 더 자세히 살펴볼 수 있을 것이다. 최적의 해가 조건부 기댓값이라는 지식을 바탕으로 제곱항을 다음과 같이 전개할 수 있다.</p>

<p>\[ (y(\textbf{x}) - t)^{2} = (y(\textbf{x}) - E(t \mid \textbf{x}) + E(t \mid \textbf{x}) -t)^{2} \]
\[= (y(x)-E(t \mid \textbf{x}))^{2}+2(y(x)-E(t \mid x))(E(t \mid x)-t)+(E(t \mid x)-t)^{2} \]</p>

<p>이 전개 결과를 손실 함수에 대입하고 t에 대해 적분하면 교차항들이 사라지게 된다. 그 결과로 다음 형태의 손실 함수를 얻을 수 있다.</p>

<p>\[ E(L) = \int (y(\textbf{x})-E(t \mid \textbf{x}))^{2}p(\textbf{x})dx + \int var(t \mid \textbf{x})p(\textbf{x})dx \]</p>

<p>우리가 찾고자 하는 함수 y(x)는 첫 번째 항에만 있는데, y(x)가 E[t|x]일 때 이 항은 최소화되어 항 자체가 사라지게 된다.
따라서 이는 우리가 앞서서 도출한 결과와 동일하다. 즉 최적의 최소 제곱 예측은 조건부 평균으로 주어진다는 것을 보여 준다.
두 번째 항은 t에 대한 분포의 분산을 계사하고, 이를 x에 대해 평균을 낸 것이다.  이 항은 표적 데이터가 가지고 있는 내재적인 변동성을 표현한 것으로, 노이즈라고 해석할 수 있다. 이 항은 y(x)에는 독립적이며, 따라서 절대로 더 이상 줄일 수 없는 손실 함수의 최솟값에 해당한다.</p>

<p>분류 문제와 마찬가지로 회귀 문제를 풀기 위한 세 가지 서로 다른 방식은 다음과 같다.</p>

<p>(a) 우선 결합 밀도 p(x,t)를 구하는 추론 문제를 풀어낸다. 다음에 이를 정규화하여 조건부밀도 p(t|x)를 구하고 최종적으로 조건부 평균을 구한다.
(b)우선 조건부 밀도 p(t|x)를 구하는 추론 문제를 풀고 주어지는 조건부 평균을 구한다.
(c)훈련 데이터로부터 회귀 함수 y(x)를 직접 구한다.</p>

<p>각각의 방식의 장단점은 분류 문제에서의 세 가지 방식의 장단점과 일맥상통한다.</p>

<table>
  <tbody>
    <tr>
      <td>회귀 문제의 손실 함수로 제곱 손실 이외의 다른 것을 사용하는 것도 가능하다. 실제로 제곱 손실이 상당히 좋지 않은 결과를 가져오기 때문에 더 복잡한 접근법을 사용해야 하는 경우가 종종 있다. 조건부 분푸 p(t</td>
      <td>x)가 다봉 분포인 상황이 이 중 하나다. 역 문제의 해를 구할 때 이런 상황이 종종 발생한다.제곱 손실을 일반화한 예시인 민코프스키 손실에 대해 간략히 살펴보자. <b>민코프스키 손실</b>의 기댓값은 다음과 같이 주어진다.</td>
    </tr>
  </tbody>
</table>

:ET