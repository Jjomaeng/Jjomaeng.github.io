I"<p>이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>패턴 인식 문제를 풀 때는 불확실성이 존재하는 상황에서 의사 결정을 내려야 하는 경우가 많다. 이런 상황에서 결정 이론과 확률론을 함께 사용하면 최적의 의사 결정을 내릴 수 있다.</p>

<p>입력 벡터 X와 타깃 변수 벡터 t가 존재하는 상황에서 새로운 입력 변수 X가 주어졌을 때 해당 타깃 변수 벡터 t를 예측하는 문제에 대해서 생각해보자. 결합 확률 분포 p(x,t) 는 이 변수들의 전체 불확실성을 요약해서 나타낸다. 주어진 훈련 집합 데이터에서 p(x.t)를 찾아내는 것은 추론문제의 대표적인 예시다. 실제 응용 사례에서는 대부분의 경우 t에 대해서 예측하는 것이 더 중요한 문제이다. t가 어떤 값을 가질 것 같은지를 바탕으로 특정 행동을 취해야 할 수도 있다. 이를 위한 이론적 토대가 바로 결정 이론이다.</p>

<p>일반적인 추론 문제는 결합 확률 분포를 결정하는 과정을 포함하고 있다. 결합 확률 분포는 매우 유용한 값이긴 하지만 최종적으로 우리가 하고 싶은 것은 우리가 어떤 행동을 취해야 할 지 결정하는 것이다. 또한, 해당 결정이 최적이기를 바라는데 이것이 바로 결정단계이다. 결정 이론이 하려는 것은 적절한 확률들이 주어진 상태에서 어떻게 하면 최적의 결정을 내릴 수 있는 가를 설명하는 것이다.</p>

<p>더 자세한 분석을 하기에 앞서, 의사 결정에 있어서 확률이 어떤 역할을 하는지 간략하게 살펴보자. 우리의 목표는 새 데이터 X를 구한 후에 두 개의 클래스 중 어떤 것으로 분류해보는 것이라고 하자. 우리는 데이터가 주어졌을 때 각각의 클래스의 조건부 확률을 알아내고 싶으며 이는 \( p(C_{k}\mid x) \)로 표현된다. 베이지안 정리를 사용하면 이 확률들을 다음과 같은 형태로 표현할 수 있다.</p>

<p>\[ p(C_{k} \mid \textbf{x}) = \frac{p(\textbf{x} \mid C_{k})p(C_{k})}{p(\textbf{x})}\]</p>

<p>위의 베이지안 정리에서 사용된 모든 값들은 결합 확률 분포 \( p(\textbf{x} \mid C_{k}) \)를 활용하여 구할 수 있다. \( p(C_{k}) \)는 클래스 \( C_{k} \)에 포함될 사전 확률, \( p(C_{k} \mid \textbf{x}) \)는 사후 확률에 해당한다. 만약 우리의 목표가 X를 잘못된 클래스에 포함시킬 가능성을 최소화하는 것이라면, 직관적으로 우리는 더 높은 사후 확률을 가진 클래스를 고르게 될 것이다.</p>

<h4 id="오분류-비율의-최소화">오분류 비율의 최소화</h4>

<p>우리는 목표가 단순히 잘못된 분류 결과의 숫자를 가능한 한 줄이는 것이라고 해보자. 이를 위해서는 각각의 x를 가능한 클래스들 중 하나에 포함시키는 규칙이 필요하다. 이 규칙은 입력 공간을 결정 구역이라고 불리는 구역 \(R_{k} \)들로 나누게 될 것이다. \( R_{k} \)는 클래스의 수만큼 존재할 것이고 Rk에 존재하는 모든 포인트들은 클래스 \( C_{k} \)에 포함될 것이다. 결정 구역들 사이의 경계를 결정 경계 혹은 결정 표면이라고 부른다.
최적의 결정 규칙을 찾아내기 위해서 두 개의 클래스를 가진 경우를 생각해보자. \( C_{1} \)에 속한 변수 x가 \(C_{2} \)에 포함되는 경우나 그 반대의 경우에 ‘실수’가 발생한다. 실수가 발생할 확률은 다음의 식으로 주어진다.</p>

<p>\[ p(mistake) = p(x \in R_{1},C_{2}) + p( x \in R_{2},R_{1}) = \int_{R_{1}}p(x,C_{2}) + \int_{R_{2}}p(x,C_{1})dx\]</p>

<p>p(mistake)를 최소화하기 위해서는 각각의 X를 피적분 함수들 중 더 작은 값을 가진 클래스에 포함시켜야 한다.
따라서 \( p(x,C_{1}) &gt; p(x,C_{2}) \)인 경우에는 x를 C1에 포함시켜야 한다. 확률의 곱 법칙에 따라서 \( p(x,C_{k}) = p(C_{k} \mid x)p(x) \)다. p(x)는 양쪽의 항에서 동일하다. 따라서 p(mistake)를 최소화하기 위해서는 각각의 X를 사후 확률 \(p(C_{k} \mid x) \)가 최대가 되는 클래스에 포함시키면 된다는 결론을 낼 수 있다. 하나의 입력 변수 x와 두 클래스의 경우에 대한 해당 도식은 다음과 같다.</p>

<figure>
    <a href="/PRML/1.png" alt="image"><img src="/PRML/5.png" alt="image" /></a>
</figure>

:ET