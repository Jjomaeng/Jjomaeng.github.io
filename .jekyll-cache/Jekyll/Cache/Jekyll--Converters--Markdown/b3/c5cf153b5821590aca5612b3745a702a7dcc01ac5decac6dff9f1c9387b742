I"<p>이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>패턴 인식 문제를 풀 때는 불확실성이 존재하는 상황에서 의사 결정을 내려야 하는 경우가 많다. 이런 상황에서 결정 이론과 확률론을 함께 사용하면 최적의 의사 결정을 내릴 수 있다.</p>

<p>입력 벡터 X와 타깃 변수 벡터 t가 존재하는 상황에서 새로운 입력 변수 X가 주어졌을 때 해당 타깃 변수 벡터 t를 예측하는 문제에 대해서 생각해보자. 결합 확률 분포 p(x,t) 는 이 변수들의 전체 불확실성을 요약해서 나타낸다. 주어진 훈련 집합 데이터에서 p(x.t)를 찾아내는 것은 추론문제의 대표적인 예시다. 실제 응용 사례에서는 대부분의 경우 t에 대해서 예측하는 것이 더 중요한 문제이다. t가 어떤 값을 가질 것 같은지를 바탕으로 특정 행동을 취해야 할 수도 있다. 이를 위한 이론적 토대가 바로 결정 이론이다.</p>

<p>일반적인 추론 문제는 결합 확률 분포를 결정하는 과정을 포함하고 있다. 결합 확률 분포는 매우 유용한 값이긴 하지만 최종적으로 우리가 하고 싶은 것은 우리가 어떤 행동을 취해야 할 지 결정하는 것이다. 또한, 해당 결정이 최적이기를 바라는데 이것이 바로 결정단계이다. 결정 이론이 하려는 것은 적절한 확률들이 주어진 상태에서 어떻게 하면 최적의 결정을 내릴 수 있는 가를 설명하는 것이다.</p>

<p>더 자세한 분석을 하기에 앞서, 의사 결정에 있어서 확률이 어떤 역할을 하는지 간략하게 살펴보자. 우리의 목표는 새 데이터 X를 구한 후에 두 개의 클래스 중 어떤 것으로 분류해보는 것이라고 하자. 우리는 데이터가 주어졌을 때 각각의 클래스의 조건부 확률을 알아내고 싶으며 이는 \( p(C_{k}\mid x) \)로 표현된다. 베이지안 정리를 사용하면 이 확률들을 다음과 같은 형태로 표현할 수 있다.</p>

<p>\[ p(C_{k} \mid \textbf{x}) = \frac{p(\textbf{x} \mid C_{k})p(C_{k})}{p(\textbf{x})}\]</p>

<p>위의 베이지안 정리에서 사용된 모든 값들은 결합 확률 분포 \( p(\textbf{x} \mid C_{k}) \)를 활용하여 구할 수 있다. \( p(C_{k}) \)는 클래스 \( C_{k} \)에 포함될 사전 확률, \( p(C_{k} \mid \textbf{x}) \)는 사후 확률에 해당한다. 만약 우리의 목표가 X를 잘못된 클래스에 포함시킬 가능성을 최소화하는 것이라면, 직관적으로 우리는 더 높은 사후 확률을 가진 클래스를 고르게 될 것이다.</p>

<h4 id="오분류-비율의-최소화">오분류 비율의 최소화</h4>

<p>우리는 목표가 단순히 잘못된 분류 결과의 숫자를 가능한 한 줄이는 것이라고 해보자. 이를 위해서는 각각의 x를 가능한 클래스들 중 하나에 포함시키는 규칙이 필요하다. 이 규칙은 입력 공간을 결정 구역이라고 불리는 구역 \(R_{k} \)들로 나누게 될 것이다. \( R_{k} \)는 클래스의 수만큼 존재할 것이고 Rk에 존재하는 모든 포인트들은 클래스 \( C_{k} \)에 포함될 것이다. 결정 구역들 사이의 경계를 결정 경계 혹은 결정 표면이라고 부른다.
최적의 결정 규칙을 찾아내기 위해서 두 개의 클래스를 가진 경우를 생각해보자. \( C_{1} \)에 속한 변수 x가 \(C_{2} \)에 포함되는 경우나 그 반대의 경우에 ‘실수’가 발생한다. 실수가 발생할 확률은 다음의 식으로 주어진다.</p>

<p>\[ p(mistake) = p(x \in R_{1},C_{2}) + p( x \in R_{2},R_{1}) = \int_{R_{1}}p(x,C_{2}) + \int_{R_{2}}p(x,C_{1})dx\]</p>

<p>p(mistake)를 최소화하기 위해서는 각각의 X를 피적분 함수들 중 더 작은 값을 가진 클래스에 포함시켜야 한다.
따라서 \( p(x,C_{1}) &gt; p(x,C_{2}) \)인 경우에는 x를 C1에 포함시켜야 한다. 확률의 곱 법칙에 따라서 \( p(x,C_{k}) = p(C_{k} \mid x)p(x) \)다. p(x)는 양쪽의 항에서 동일하다. 따라서 p(mistake)를 최소화하기 위해서는 각각의 X를 사후 확률 \(p(C_{k} \mid x) \)가 최대가 되는 클래스에 포함시키면 된다는 결론을 낼 수 있다. 하나의 입력 변수 x와 두 클래스의 경우에 대한 해당 도식은 다음과 같다.</p>

<figure>
    <a href="/PRML/16.png" alt="image"><img src="/PRML/16.png" alt="image" /></a>
</figure>

<p>다른 관점에서 K개의 클래스를 가진 경우에는 올바르게 분류된 경우의 확률을 극대화할 수 있으며 이 방법이 더 일반적이고 조금 더 쉽다.</p>

<p>\[ p(correct) = \sum_{k=1}^{K}p(x \in R_{k},C_{k}) = \sum_{k=1}^{K}\int_{R_{k}}p(\textbf{x},C_{k})dx \]</p>

<table>
  <tbody>
    <tr>
      <td>각각의 x가 p(x,Ck)가 최대인 클래스로 분류되도록 Rk를 선택할 경우에 위의 식 값이 최대화된다. 따라서 각각의 x는 가장 큰 사후 확률 p(Ck</td>
      <td>x)를 가지는 클래스로 분류되어야 함을 확인할 수 있다.</td>
    </tr>
  </tbody>
</table>

<h4 id="기대-손실의-최소화">기대 손실의 최소화</h4>

<p>한 가지 예를 들어 , 환자의 데이터를 가지고 암의 여부를 추론한다고 해보자. 만약 암에 걸리지 않은 환자를 걸렸다고 판단하는 것보다 잘못된 진단으로 암에 걸린 환자를 건강하다고 판단했을 경우 심각한 결과를 초해라 수 있다. 이처럼 두 잘못된 판단의 결과가 다르게 나타날 수 있다. 이 경우에는 전자의 실수보다 후자의 실수를 줄이는 것이 중요하다.</p>

<p>비용함수라고도 부르는 손실 함수를 도입함으로써 이러한 문제들을 더 공식화할 수 있다. 손실 함수는 어떤 결정이나 행동이 일어났을 때의 손실을 전체적으로 측정하는 함수다. 이를 활용하면 우리의 목표를 발생하는 전체 손실을 최소화하는 것으로 변경할 수가 있다.</p>

<p>실제 클래스 \(C_{k} \)인 새 입력값 X를 클래스 \(C_{j} \)로 분류했다고 가정해보자. 이 과정에서 우리는 \(L_{k,j} \)로 표현할 수 있는 손실을 발생시키게 된다.
\(L_{k,j} \)는 손실 행렬의 k,j번째 원소로 볼 수 있다. 예를 들어 암 환자 분류 예시에 대해서 다음과 같은 손실 행렬을 가정하여 결과에 따른 손실을 다르게 줄 수 있다.</p>

<figure>
    <a href="/PRML/17.png" alt="image"><img src="/PRML/17.png" alt="image" /></a>
</figure>

<p>손실 함수를 최소화하는 해가 최적의 해다. 하지만 손실 함숫값은 알려져 있지 않는 실제 클래스값을 알아야만 계산이 가능하다. 주어진 입력 벡터 X에 대해서 실제 클래스값에 대한 불확실성은 결합 확률 분포 p(x,ck)로 표현된다. 그렇기 때문에 우리는 이 분포에 대해 계산한 평균 손실을 최소화하는 것을 목표로 삼을 수 있다.</p>

<p>\[ E(L) = \sum_{k}\sum_{j}\int_{R_{j}}L_{k,j}p(\textbf{x},C_{k})dx \]</p>

<p>확률의 곱의 법칙을 활용하면 \( p(x,C_{k}) = p(C_{k} \mid x)p(x) \)임을 알 수 있고 공통 인자 p(x)를 제거할 수 있다. 따라서 기대 손실을 최소화하는 결정 법칙은 각각의 x를 다음 식을 최소화하는 클래스 j에 할당하는 것이다.</p>

<p>\[ \int_{k} L_{k,j}p(C_{k} \mid \textbf{x})\]</p>

<p>각각의 클래스에 대한 사후 확률 \( P(C_{k} \mid x) \)를 알고 나면 이 방법을 쉽게 시행할 수 있다.</p>

:ET