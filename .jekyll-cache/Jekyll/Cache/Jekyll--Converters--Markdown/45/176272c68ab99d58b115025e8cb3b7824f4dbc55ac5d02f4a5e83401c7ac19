I"<p class="notice">이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>서로 다른 K개의 값들 중 하나를 취할 수 있는 이산 변수를 표현하기 위해 <b>원 핫 인코딩</b>을 많이 사용한다.
원 핫 인코딩에서는 각각의 변수가 K차원의 벡터 X로 나타내며, \( x_{k} \)값들 중 하나는 1, 나머지 값들은 0으로 설정된다. 예를 들어 4개의 상태를 가질 수 있는 변수가 \( x_{3} = 1\) 이라는 상태를 가졌다면 해당 변수 X를 다음과 같이 표현할 수 있다.
\[ \textbf{X} = (0,0,1,0)^{T}\]</p>

<p>이러한 벡터들은 \( \sum_{k = 1}^{K} x_{k} = 1\)이라는 성질을 만족한다. 만약 우리가 \( x_{k} = 1\)이 될 확률을 \( \mu_{k} \)라고 한다면, X의 분포는 다음과 같이 주어진다.</p>

<p>\[ p(\textbf{X} \mid \mu) = \prod_{k = 1}^{K} \mu_{k}^{x_{k}}  \]</p>

<p>여기서 \( \textbf{ μ } = (\mu_{1},…..,\mu_{K})^{T}\)이며, 매개변수 \( \mu_{k}\)는 \( \mu_{k} \geq 0\)과 \( \sum_{k} \mu_{k} =1 \)이라는 성질을 만족시켜야 한다. (\( \mu_{k} \)는 확률이기 때문에)</p>

<p>이 분포에 대해서 다음의 두 가지를 쉽게 증명할 수 있다.
\[ \sum_{\textbf{X} p(\textbf{X} \mid \textbf{ μ }) = \sum_{k = 1}^{K}\]</p>

<figure>
    <a href="/PRML/20.jpeg" alt="image"><img src="/PRML/20.jpeg" alt="image" /></a>
</figure>

:ET