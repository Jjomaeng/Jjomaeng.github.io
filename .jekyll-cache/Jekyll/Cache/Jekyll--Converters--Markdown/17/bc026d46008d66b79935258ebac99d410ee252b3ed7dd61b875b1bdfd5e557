I"<p class="notice">이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>이번에는 패턴 인식과 머신 러닝 테크닉을 이해하는 데 있어서 중요한 역할을 하게 될 또 하나의 이론인 정보 이론에 대해 살펴보고자 한다.
먼저, 엔트로피를 포함한 정보 이론의 중요한 개념들에 대해 살펴보자.</p>

<p>이산 확률 변수 x를 고려했을 때, x의 값을 학습하는 데 있어서 정보의 양은 ‘놀라움의 정도’라고 생각할 수 있을 것이다.
따라서 우리가 사용하게 될 정보량의 측정 단위는 확률 분포 p(x)에 종속적이게 된다.</p>

<p>지금부터 p(x)에 대해 단조 함수인 정보량을 표현하는 함수 h(x)에 대해 살펴보도록 하자.
서로 연관되어 있지 않은 두 사건 x와 y를 고려해 보면, 이 경우 x와 y가 함께 일어났을 때 얻는 정보량은 각자의 사건이 따로 일어났을 때 얻는 정보량의 합이 될 것이다. 따라서 h(x,y) = h(x) + h(y)가 된다. 연관되어 있지 않은 두 사건은 통계적으로 독립적이며 따라서 p(x,y) = p(x)p(y)다. 이 관계로부터 h(x)는 p(x)의 로그에 해당하는 것을 보일 수 있다. 이에 따라 다음 식을 얻게 된다.
\[h(x) = -log_{2}p(x) \]</p>

<p>사건 x의 확률이 낮을수록 그로부터 얻을 수 있는 정보량은 크다는 것을 확인할 수 있다.</p>

<p>송신자가 어떤 확률 변수의 값을 수신자에게 전송하고자 하는 상황을 가정해 보자.
전송에 필요한 정보량의 평균치는 p(x)에 대해 위의 식을 통해 기대값을 구함으로써 알아낼 수 있다.</p>

<p>\[ H(X) = -\sum_{x}p(x)log_{2}p(x)  \]</p>

<p>이 값이 바로 확률 변수의 x의 엔트로피다.</p>

<p>(이제부터는 엔트로피를 정의하는 데 있어서 자연 로그를 사용하도록 하겠다.)
이 장에서는 엔트로피를 확률 변수의 상태를 결정짓는 데 필요한 정보량의 평균으로 정의하였다. 다음의 관점으로 엔트로피를 이해해보자.
N개의 동일한 물체가 몇 개의 통 안에 담겨 있다고 가정해 보자. 이때 i번쨰 통 안의 ni개의 물체가 담기도록 할 것이다. 물체를 통 안에 담는 가짓수는 N!개의 방법이있다. 하지만 한 통 안에서 물체들이 어떤 순서로 놓여 있는지는 중요하지 않다. i 번째 통에는 물체를 정렬하기 위한 ni!가지 방법이 있을 것이고, 이에 따라 N개의 물체를 통에 넣는 가짓수는 다음과 같이 될 것이다.</p>

<p>\[ W = \frac{N!}{\prod_{i}n_{i}!}\]</p>

<p>위의 식을 다중도라 한다. 엔트로피는 다중도의 로그를 취해서 적절한 상수로 나눈 것이다.</p>

<p>비율 \( \frac{ni}{N} \)을 그대로 유지시킨 상태에서 N-&gt;∞을 취하여 보자. 그리고 다음 식의 스털링 근사식을 적용해보자
\[lnN! \simeq N ln N - N \]</p>

<p>그러면 다음을 얻게 된다.</p>

<p>\[ H = -lim_{N -&gt; \infty }\sum_{i}(\frac{n_{i}}{N})ln(\frac{n_{i}}{N})= -\sum_{i}p_{i}lnp_{i} (\sum_{i}n_{i} = N) \]</p>

<p>여기서 pi는 물체가 i번째 통에 속하게 될 확률이다. 물리학 용어로 통 안의 물체들의 순서를 미시 상태라 하며, ni/N으로 표현되는 통 각각이 가지고 있는 물체의 숫자 비율을 일컬어 거시 상태라 한다. 다중도 W를 거시 상태의 가중치라 일컫기도 한다.</p>

<p>각각의 통을 확률 변수 X의 상태 xi라고 해석할 수 있다. 여기서 p(X = xi) = pi다. 이 경우 확률 변수 X의 엔트로피는 다음과 같다.</p>

<p>\[ H(p) = -\sum_{i}p(x_{i})lnp(x_{i}) \]</p>

<p>이를 그림으로 확인해보자.</p>

<figure>
    <a href="/PRML/24.png" alt="image"><img src="/PRML/24.png" alt="image" /></a>
</figure>

<p>그림에서 볼 수 있는 것처럼 분포p(xi)가 몇몇 값에 뾰족하게 집중되어 있는 경우에는 상대적으로 낮은 엔트로피를 가지는 반면, 더 많은 값들 사이에 퍼져 있을 때는 높은 엔트로피를 가지게 된다.</p>

<p>(중간 내용은 완전히 이해하지 못하여 이후에 정리하여 작성하겠습니다.)</p>

:ET