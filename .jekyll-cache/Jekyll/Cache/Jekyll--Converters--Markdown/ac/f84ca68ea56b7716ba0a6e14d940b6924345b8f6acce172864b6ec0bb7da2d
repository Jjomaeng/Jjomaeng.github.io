I"u$<p>이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>지금까지 확률을 ‘반복 가능한 임의의 사건의 빈도수’라는 측면에서 살펴보았다. 이러한 해석을 고전적 또는 빈도적 관점이라 일컫는다.
이번에는 더 포괄적인 베이지안 관점에서 살펴보자. 베이지안 관점을 이용하면 확률을 이용해서 불확실성을 정량화하는 것이 가능하다.</p>

<p>베이지안 관점을 사용하면 모델 매개변수의 불확실성을 설명할 수 있고 더 나아가 모델 그 자체를 선택하는 데 있어서도 유용하다.</p>

<p>앞에서 설명한 다항식 곡선 피팅(1.1장) 예시의 매개변수 w에 적용해보자</p>
<ul>
  <li>첫 번째로 데이터를 관측하기 전의 w에 대한 우리의 가정을 사전 확률 분포 p(w)로 표현할 수 있다.</li>
  <li>관측된 데이터 D = {t1,t2,,,,,tn}은 조건부 확률 p(D|w)로써 작용하게 된다.
이 경우 베이지안 정리는 다음 형태를 띤다.</li>
</ul>

<p>\( p(w \mid D) = \frac{p(D \mid w)p(w)}{p(D)} \) 식(1)</p>

<p>자세히 설명하면</p>

<ul>
  <li>
    <p>\(p(w \mid D) \): D를 관측한 후의 w에 대한 불확실성을 사후 확률로 표현</p>
  </li>
  <li>
    <p>\(p(D \mid w) \) : 각각의 다른 매개변수 벡터 w에 대해 관측된 데이터 집합이 얼마나 그렇게 나타날 가능성이 있는지를 표현한 가능도 함수 
           (가능도 함수는 w에 대한 확률 분포가 아니며, 따라서 w에 대해 가능도 함수를 적분하여도 1이 될 필요가 없다)</p>
  </li>
  <li>
    <p>\( p(w) \): 데이터를 관측하기 전의 w에 대한 우리의 가정, 사전 확률</p>
  </li>
  <li>
    <p>\( p(D) \) : 사후 분포가 적합한 확률 분포가 되고 적분값이 1이 되도록 하기 위한 정규화 상수</p>
  </li>
</ul>

<p>가능도 함수에 대한 정의를 바탕으로 베이지안 정리를 다음처럼 적을 수가 있다.</p>

<p>사후 확률  ∝ 가능도 x 사전확률 ( 각 값은 전부 w에 대한 함수)</p>

<p>식(1)의 양쪽 변을 w에 대해 적분하면 베이지안 정리의 분모를 사전확률과 가능도 함수로 표현 가능하다</p>

<p>\[p(D) = \int p(D \mid \textbf{w})p(\textbf{w})d\textbf{w} \]</p>

<p>여기서 가능도 함수의 역할을 살펴보면 빈도적 확률 관점과 베이지안 확률 관전에서 다르다.</p>
<ul>
  <li>
    <p>빈도적 확률 관점 : w가 고정 매개변수로 여겨지며, w의 값은 어떤 형태의 ‘추정값’을 통해서 결정된다. 그리고 추정에서의 오류는
                            가능한 데이터 집합들 D의 분포를 고려함으로써 구할 수 있다.</p>
  </li>
  <li>
    <p>베이지안 확률 관점 : 오직 하나의 (실제로 관측될) 데이터 집합 D만 존재하며, 매개변수의 불확실성은 w의 확률 분포를 통해 표현한다.</p>
  </li>
</ul>

<p><u>빈도적 확률 관점</u>에서 널리 사용되는 추정값 중 하나는 바로 <b>최대가능도</b>이다.</p>

<p>최대가능도를 사용할 경우에 w의 가능도 함수 p(D|w)를 최대화하는 값으로 선택한다.
(머신러닝에서 종종 음의 로그 가능도 함숫값을 오차함수라고 일컫는다. 음의 로그 함수는 단조 감소하는 함수이기 때문에 가능도의 최댓값을 찾는 것이 오차를 최소화하는 것과 동일하다 -&gt; 자세한 내용은 CS229 강의를 정리하면서 설명할 예정이다)</p>

<p>또한, 빈도적 확률론자들이 오차를 측정하는 방법 중 하나가 부트스트랩이다.
예를 들어 설명하면, 원 데이터 집합이 N개인 데이터 포인트 X = {x1,x2,,,,xn}가 있다고 가정해보자.
X에서 N개의 데이터 포인트를 임의로 복원 추출하여 데이터 집합 Xb를 만든다. 
이 과정을 L번 반복하면 원래 데이터 집합의 표본에 해당하는 크기 N의 데이터 집합을 L개 만들 수 있다.
각각의 부트스트랩 데이터 집합에서의 예측치와 실제 매개변수 값과의 차이를 바탕으로 매개변수 추정값의 통계적 정확도를 판단할 수 있다.</p>

<p>베이지안 관점의 장점 중 하나는 사전 지식을 추론 과정에서 자연스럽게 포함시킬 수 있다.
동전 10개를 던졌는데 모두 앞면이 나왔다고 가정하자. 빈도적 관점에서는 미래의 모든 동전 던지기에서 앞면만 나올 것이라고 예측한다.대조적으로 베이지안 관점에서는 적당히 합리적인 사전 확률을 사용한다면 이렇게까지 과도한 결론이 나오지 않을 것이다.</p>

<p>하지만, 베이지안 관점에서는 사전 확률의 선택에 따라 결론이 나오기 때문에 추론 과정에서 주관이 포함될 수밖에 없고 실제로 좋지 않은 사전 분포를 바탕으로 한 베이지안 방법은 부족한 결과물을 높은 확신으로 내놓기도 한다.</p>

<h4 id="가우시안-분포">가우시안 분포</h4>

<p>이제 가장 중요한 연속 활률 분포 하나를 살펴보고자 한다. 바로 정규 분포라고도 불리는 가우시안 분포이다.</p>

<p>단일 실수 변수 x에 대해서 가우시안 분포는 다음과 같이 정의된다.</p>

<p>\[ N(x \mid \mu ,\sigma ^{2}) = \frac{1}{\sqrt{2\pi \sigma ^{2}}}exp(-\frac{(x-\mu)^{2}}{2\sigma^{2}}) \]</p>

<p>위의 식은 두 개의 매개변수 μ(평균),  \(σ^{2} \)(분산)에 의해 통제된다.</p>

<figure>
    <a href="/PRML/2.png" alt="image"><img src="/PRML/6.png" alt="image" /></a>
</figure>

<p>위의 가우시안 식으로부터 가우시안 분포가 다음의 성질은 만족함을 확인할 수 있다.</p>

<p>\[N(x \mid \mu ,\sigma ^{2})&gt; 0 \]</p>

<p>가우시안 분포가 정규화되어 있다는 것 또한 쉽게 확인할 수 있다.</p>

<p>\[ \int_{-\infty }^{\infty}N(x \mid \mu,\sigma^{2})dx = 1 \]</p>

<p>가우시안 분포를 따르는 임의의 x에 대한 함수의 기댓값을 구할 수 있다. 특히 x의 평균값은 다음과 같다</p>

<p>\[ E(x) = \int_{-\infty }^{\infty}N(x \mid \mu ,\sigma ^{2}) x dx = \mu \]</p>

<p>이와 비슷하게 x에 대한 이차 모멘트를 계산해 보자</p>

<p>\[ E(x^{2}) = \int_{-\infty }^{\infty}N(x \mid \mu ,\sigma ^{2}) x^{2} dx = \mu^{2} +\sigma^{2} \]</p>

<p>이를 통해 x의 분산을 다음과 같이 계산할 수 있다.</p>

<p>\[ var(x) = E(x^{2}) - E(x)^{2} = \sigma^{2} \]</p>

<p>분포의 최댓값을 최빈값(mode)이라 하는데, 가우시안 분포의 경우에는 최빈값과 평균값이 동일하다.</p>

<p>다음으로는 연속 변수로 이루어진 D차원 벡터 X에 대한 가우시안 분포를 살펴보도록 하자</p>

<p>\[ N(X \mid \mu,\Sigma ) = \frac{1}{(2\pi) ^{\frac{D}{2}}}\frac{1}{ \mid \Sigma \mid ^{\frac{1}{2}}}exp(-\frac{1}{2}(X-\mu)^{T}\Sigma^{-1}(X-\mu)) \]</p>

<p>이 식은 가우시안 분포의 가능도 함수에 해당한다.</p>

<p>관측된 데이터 집합을 바탕으로 확률 분포의 매개변수를 결정하는 표준적인 방법 중 하나는 가능도 함수를 최대화하는 매개변수를 찾는 것이다.</p>

<p>위의 식에 로그를 취하여 (수학적 분석, 계산을 쉽게하기 위해서 또한 언더플로우 방지) 로그가능도 함수를 다음과 같이 적을 수 있다.</p>

<p>\[ lnp(X \mid \mu,\sigma^{2} ) =-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}(x_{n} - \mu)^{2} - \frac{N}{2}ln\sigma^{2}-\frac{N}{2}ln(2\pi) \]</p>

<p>μ에 대해 이 식의 최댓값을 찾으면 다음의 최대 가능도해를 찾을 수 있다.</p>

<p>\[ mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n} \]</p>

<p>이는 바로 관찰된 값{xn}들의 평균인 표본평균이다</p>

<p>이와 비슷한 방식으로 최댓값을 \(σ^{2} \)에 대해 찾으면 분산에 대한 최대 가능도 해를 다음과 같이 찾을 수 있다.</p>

<p>\[ \sigma^{2}<em>{ML} = \frac{1}{N} \sum</em> \]</p>

<p>이는 표본 평균에 대해 계산된 표본 분산이다(가우시안 분포의 경우, μ,  \( σ^{2} \)에 대한 해가 연관이 없기 때문에 계산 순서를 변경해도 무방하다)</p>

<p>하지만, 최대가능도 방법에는 한계가 있다
최대가능도 방법이 구조적으로 분포의 분산을 과소평가하게 된다.즉 편향된다.
최대가능도 해인 μ,  \( σ^{2} \)은 데이터 집합 x1,….,xn의 함수다. 각 데이터 집합의 값에 대해 이들의 기댓값을 고려해보자</p>

<p>\[ E(\mu_{ML}) = \mu \]
\[ E(\sigma^{2}_{ML}) = (\frac{N-1}{N})\sigma^{2} \]</p>

<p>여기서 분산이 (N-1/N)만큼 과소평가된다.
최대 가능도 방법의 편향 문제는 우리가 앞에서 살펴본 다항식 곡선 피팅에서의 과적합 문제의 근본적인 원인이 된다.
또한, 데이터 포인트의 개수인 N이 커질수록 최대 가능도 해에서의 편향치는 점점 줄어든다</p>

<p>곡선 피팅에 대한 내용은 다음에 이어 설명하겠습니다.</p>

:ET