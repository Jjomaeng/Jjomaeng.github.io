I"<p>이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>이번에는 곡선 피팅 문제를 확률적 측면에서 살펴보자.</p>

<p>곡선 피팅 문제의 목표는 N개의 입력값 \( X = (x1,….xn)^{T} \) 과 해당 표적값 \( t = (t1,…tn)^{T} \)가 주어진 상황에서 새로운 입력 변수 x가 
주어졌을 때 그에 대한 타깃 변수 t를 예측해 내는 것이다.
확률 분포를 이용해서 타깃 변수의 값에 대한 불확실성을 표현할 수 있다.</p>

<p>먼저, 주어진 x값에 대한 t값이 y(x,w)를 평균으로 가지는 가우시안 분포를 가진다고 가정한다(y(x,w)에 대한 설명은 1.1 참고 )
이를 바탕으로 다음의 조건부 분포를 적을 수 있다.</p>

<p>\[ “p(t \mid x,\textbf{w},b) = N(t \mid y(x,\textbf{w}),\beta ^{-1}) \]</p>

<p>여기서 β는 정밀도 매개변수로써 분포의 표본의 역수에 해당한다.
이 식을 도식화해 확인해보면 다음과 같다.</p>

<figure>
    <a href="/PRML/7.png" alt="image"><img src="/PRML/7.png" alt="image" /></a>
</figure>

<p>이제 훈련 집합 {X,t}를 바탕으로 최대 가능도 방법을 이용해서 알려지지 않은 매개변수 w와 β를 구해보도록 하자.
데이터는 위의 식에서 독립적으로 추출했다고 가정하면(i.i.d), 가능도 함수는 다음과 같이 주어진다.</p>

<p>\[ p(\textbf{t} \mid \textbf{x},\textbf{w},b) = \prod_{n=1}^{N}N(t \mid y(x,\textbf{w}),\beta ^{-1}) \]</p>

<p>계산의 편의를 위해 로그를 취해 로그 가능도 함수를 얻으면 다음과 같다.</p>

<p>\[ ln p(\textbf{t} \mid \textbf{x},\textbf{w},b) = -\frac{\beta }{2}\sum_{n= 1}^{N}{(y(x_{n},\textbf{w})-t_{n})}^{2} + \frac{N}{2}ln\beta - \frac{N}{2}ln(2\pi) \]</p>

<p>다항식 계수(w)의 최대 가능도 해를 구해보면 w와 관련된 것은 오른쪽 식의 맨 왼쪽만 해당한다. 따라서 음의 로그이기 때문에 이를 최소화하는 것이다. 즉 가능도 함수를 최대화하려는 시도의 결과로 제곱합 오차 함수를 유도할 수 있는 것이다.</p>

<p>마찬가지로 가우시안 조건부 분포의 정밀도 매개변수 β를 결정하는 데도 최대 가능도 방법을 사용할 수 있다. 로그 가능도를 β에 대해 최대화하면 다음의 식이 도출된다.</p>

<p>\[ \frac{1}{B_{ML}} = \frac{1}{N}\sum_{n = 1}^{N}(y(x_{n},w_{ML})-t_{n})^{2} \]</p>

<p>매개변수 w와  β를 구했으니, 이제 이를 바탕으로 새로운 변수 x에 대해 예측값을 구할 수 있다.
확률모델을 사용하고 있으므로 예측값은 t에 대한 예측 분포로 표현될 것이다(점 X)
최대 가능도 매개변수들을 식(1)에 대입하면 다음을 얻을 수 있다.</p>

<p>\[ p(t \mid \textbf{t},t,\alpha,\beta)\]</p>

<p>이제 다항 계수 w에 대한 사전 분포를 도입할 것이다. 문제의 단순화를 위해서 다음 형태를 지닌 가우시안 분포를 사용할 것이다.</p>

:ET