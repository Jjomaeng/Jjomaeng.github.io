I"<p>이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>패턴 인식에서 ‘불확실성’은 측정할 때의 노이즈를 통해서도 발생하고 데이터 집합 수가 제한되어 있다는 한계점 때문에도 발생한다.
따라서, 확률론은 불확실성을 계량화하고 조작하기 위한 이론적 토대를 마련해 준다.</p>

<p>예시를 들어보자</p>

<p>이 예시에서는 X,Y라는 두 가지 확률 변수가 존재한다. X는 xi (i = 1,….,M)중 아무 값이나 취할 수 있고 Y는 yj (j = 1,…,L)중 아무 값이나 취할 수 있다. 또한, X와 Y각각에서 표본을 추출하는 시도를 N번 한다고 가정한다.
nij는 X = xi, Y = yj 인 시도 개수를 의미하며, ci는 Y 값과는 상관없이 X= xi인 시도의 숫자, rj는 X값과 상관없이 Y=yj인 시도의 숫자를 의미한다.</p>

<p>X = xi, Y = yj 일 결합 확률은 다음과 같이 표현된다.</p>

<p>\[ p(X = x_{i},Y = y_{i}) = \frac{n_{i,j}}{N} \]</p>

<p>비슷하게 Y값과 무관하게 X가 xi값을 가질 확률을 다음과 같다</p>

<p>\[p(X = x_{i}) =\frac{c_{i}}{N} \]</p>

<p>이 두 식을 이용하여 다음을 도출해 낼 수 있다.</p>

<p>\[p(X = x_{i}) = \sum_{j=1}^{L}p(X = x_{i},Y = y_{j}) \]</p>

<p>이것이 확률의 <b>합의 법칙</b>이다. 여기서 P(X = xi)는 주변 확률이라 불린다.</p>

<p>여기서 X = xi인 사례들만 고려해보자. 그들 중에서 Y = yi인 사례들의 비율을 생각해 볼 수 있고, 이를 확률 \(p(Y = y_{i} | X = x_{i}) \)로 적을 수 있으며
이를 <b>조건부 확률</b>이라고 부른다</p>

<p>\[ p(Y = y_{i} \mid X = x_{i}) = \frac{n_{i,j}}{c_{i}} \]</p>

<p>지금까지의 식을 이용해서 다음의 관계를 도출할 수 있다.</p>

<p>\[ p(X = x_{i} \mid Y = x_{j}) = \frac{n_{i,j}}{c_{i}} = \frac{n_{i,j}}{c_{i}}\cdot \frac{c_{i}}{N} = p(Y = y_{j} \mid X = x_{i})p(X=x_{i}) \]</p>

<p>이것이 바로 확률의<b> 곱의 법칙</b>이다&lt;/p&gt;</p>

<p>이제, 곱의 법칙과 합의 법칙, 대칭성 p(X,Y) = p(Y,X)로 부터 조건부 확률 간의 관계인 다음 식을 도출해 낼 수 있다.
(여기서 부터는 간단하게 확률 변수 X에서 분포를 표현할 때는 p(X)라 적고 특정 값 xi에서의 분포를 표현할 때는 p(xi)로 적기로 한다)</p>

<table>
  <tbody>
    <tr>
      <td>\[p(Y</td>
      <td>X) = \frac{p(X</td>
      <td>Y</td>
      <td>)p(Y)}{p(X)} = \frac{p(X</td>
      <td>Y</td>
      <td>)p(Y)}{\sum_{Y}p(X</td>
      <td>Y</td>
      <td>)p(Y)} \]</td>
    </tr>
  </tbody>
</table>

<p>이를 <b>베이즈 정리</b>라고 한다( 책의 전반에 걸쳐 중요한 역할을 한다)</p>

<p>여기서 분모는 왼쪽 항을 모든 Y값에 대하여 합했을 때 1이 되도록 하는 역할을 한다.
베이지안 정리는 다음과 같이 해석할 수 있다.
확률 p(A|B)를 알고 있을 때, 관계가 정반대인 확률 p(B|A)를 계산하는 것으로 여기서 P(B)는 사전 확률, p(B|A)는 사후 확률에 해당한다.
여기서, 두 확률 변수가 독립이라면, p(X,Y) = p(X)p(Y) 이다.</p>

<h4 id="확률-밀도">확률 밀도</h4>

<p>이번에는 연속적인 변수에서의 확률에 대해 알아본다.</p>

<p>만약 실수 변수 x가 (x,x+δx)구간 안의 값을 가지고 그 변수의 확률이 p(x)δx로 주어진다면, p(x)를 x의 확률 밀도라고 부른다.
이때, x가 (a,b) 구간 사이의 값을 가질 확률은 다음과 같다.</p>

<p>\[ p(x \in (a,b)) = \int_{a}^{b}p(x)dx \]</p>

<p>여기서, 확률은 양의 값을 가지고 x의 값은 실축선상에 존재해야 하기 때문에 다음의 두 조건을 만족시켜야 한다.</p>

<ol>
  <li>\( p(x) \geq 0 \)</li>
  <li>\( \int_{-\infty }^{\infty}p(x)dx = 1 \)</li>
</ol>

<h4 id="누적-분포-함수">누적 분포 함수</h4>

<p>x가 (-∞,z) 범위에 속할 확률은 <b>누적 분포 함수 </b> 로 표현된다.</p>

<p>\[ P(z) = \int_{-\infty }^{z}p(x)dx \]</p>

<p>여기서 P’(x) = p(x)이다.</p>

<p>만약 x가 이산 변수일 경우 p(x)를  <b> 확률 질량 함수 </b>라고 부르기도 한다.</p>

<h4 id="기댓값">기댓값</h4>
<p>기댓값은 확률 밀도 p(x)하에서 어떤 함수 f(x)의 평균값을 의미한다. 이산 분포의 경우 기댓값은 다음과 같이 주어진다.</p>

<p>\[ E(f) = \sum_{x}p(x)f(x) \]</p>

<p>여기서 p(x)는 각 x값에 해당하는 확률이며 이것을 가중치로 사용한 가중 평균을 구하는 것으로 해석할 수 있다.</p>

<p>연속 변수의 경우에는 해당 확률 밀도에 대해 적분을 시행해서 기댓값을 구할 수 있다.</p>

<p>\[E(f) = \int p(x)f(x)dx \]</p>

<p>공분산에 해당하는 특징은 다음과 같다.</p>

<ol>
  <li>E[aX + b] = aE[X] + b</li>
  <li>E[X + Y] = E[X] + E[Y]</li>
  <li>E[XY] = E[X]E[Y]</li>
</ol>

<h4 id="공분산">공분산</h4>

<p>공분산을 설명하기 앞서 분산부터 설명하면, f(x)의 분산은 다음과 같다.</p>

<p>\[ var(f) = E((f(x) - E(f(x)))^{2}) \]</p>

<p>이는 f(x)가 평균값 E[f(x)]로부터 전반적으로 얼마나 멀리 분포되었는지를 나타내는 값이다.</p>

<p>이를 전개하여 정리하면 다음과 같다. (식의 이해를 돕기 위하여 상수인 E[f(x)] 부분을  μ라 표현)</p>

<p>\( var(f) = E(f(x)^{2} -2\mu f(x) +\mu^{2})) \) =&gt; 공분산 특징 첫 번째 이용</p>

<p>\( = E(f(x)^{2}) -2\mu E(f(x)) +\mu^{2} \)</p>

<p>\( = E[f(x)^{2}] - E[f(x)]^{2} \)</p>

<p>여기서 변수 x 그 자체에도 고려가능하다.</p>

<p>다음으로 두 개의 확률 변수 x 와 y에 대해서 공분산은 다음과 같이 정의된다.</p>

<p>cov[x,y] = E[{x - E[x]}{y - E[y]}]
         = E[xy] - E[x]E[y]</p>

<p>이는 x값과 y값이 얼마나 함께 같이 변동하는가에 대한 지표이며 만얀 x,y가 서로 독립적일 경우 공부산값은 0으로 간다.</p>

<p>베이지안 확률에 대한 내용은 다음에 이어 설명하겠습니다.</p>

:ET