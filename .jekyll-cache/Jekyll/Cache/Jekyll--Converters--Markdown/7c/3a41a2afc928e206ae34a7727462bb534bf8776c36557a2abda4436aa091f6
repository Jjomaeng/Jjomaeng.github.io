I"V<p>이 글은 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>을 읽고 공부한 내용을 작성한 글입니다. 
모든 내용은 책에 포함되어 있는 내용을 기반으로 작성하였습니다.</p>

<p>지금까지 확률을 ‘반복 가능한 임의의 사건의 빈도수’라는 측면에서 살펴보았다. 이러한 해석을 고전적 또는 빈도적 관점이라 일컫는다.
이번에는 더 포괄적인 베이지안 관점에서 살펴보자. 베이지안 관점을 이용하면 확률을 이용해서 불확실성을 정량화하는 것이 가능하다.</p>

<p>베이지안 관점을 사용하면 모델 매개변수의 불확실성을 설명할 수 있고 더 나아가 모델 그 자체를 선택하는 데 있어서도 유용하다.</p>

<p>앞에서 설명한 다항식 곡선 피팅(1.1장) 예시의 매개변수 w에 적용해보자</p>
<ul>
  <li>첫 번째로 데이터를 관측하기 전의 w에 대한 우리의 가정을 사전 확률 분포 p(w)로 표현할 수 있다.</li>
  <li>관측된 데이터 D = {t1,t2,,,,,tn}은 조건부 확률 p(D|w)로써 작용하게 된다.
이 경우 베이지안 정리는 다음 형태를 띤다.</li>
</ul>

<p>\( p(w \mid D) = \frac{p(D \mid w)p(w)}{p(D)} \) 식(1)</p>

<p>자세히 설명하면</p>

<p>p(w|D) : D를 관측한 후의 w에 대한 불확실성을 사후 확률로 표현
p(D|w) : 각각의 다른 매개변수 벡터 w에 대해 관측된 데이터 집합이 얼마나 그렇게 나타날 가능성이 있는지를 표현한 가능도 함수 
             (가능도 함수는 w에 대한 확률 분포가 아니며, 따라서 w에 대해 가능도 함수를 적분하여도 1이 될 필요가 없다)
p(w) : 데이터를 관측하기 전의 w에 대한 우리의 가정, 사전 확률
p(D) : 사후 분포가 적합한 확률 분포가 되고 적분값이 1이 되도록 하기 위한 정규화 상수</p>

<figure>
    <a href="/PRML/2.png" alt="image"><img src="/PRML/2.png" alt="image" /></a>
</figure>

:ET